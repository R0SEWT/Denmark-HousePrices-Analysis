{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0551193",
   "metadata": {},
   "source": [
    "# Feature Engineering para Precios Inmobiliarios Dinamarca\n",
    "\n",
    "**Objetivo**: Transformación y construcción de variables predictivas para el modelado supervisado de precios de vivienda en Dinamarca.\n",
    "\n",
    "**Contenido**:\n",
    "1. Carga de datos y configuración inicial\n",
    "2. Pipeline de feature engineering modular\n",
    "3. Enriquecimiento geográfico\n",
    "4. Documentación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b8f4523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/exodia/Documentos/TFBigData\n",
      "Configuración cargada.\n"
     ]
    }
   ],
   "source": [
    "# Configuración y carga de módulos\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "import setup\n",
    "setup.set_project_root()\n",
    "\n",
    "from config import *\n",
    "from feature_engineering import enhanced_feature_engineering_pipeline\n",
    "from descriptive_analysis import load_and_validate_data\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Configuración cargada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588037d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados: 1,506,591 filas, 19 columnas\n",
      "Rango temporal: 1992-01-05 a 2024-09-30\n"
     ]
    }
   ],
   "source": [
    "# Carga de datos\n",
    "df = pd.read_parquet(CLEAN_FILE)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"Datos cargados: {df.shape[0]:,} filas, {df.shape[1]} columnas\")\n",
    "print(f\"Rango temporal: {df['date'].min().date()} a {df['date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b57cfa8",
   "metadata": {},
   "source": [
    "## Pipeline de Feature Engineering con Enriquecimiento Geográfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f61063f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando pipeline de feature engineering...\n",
      "🚀 INICIANDO PIPELINE COMPLETO DE FEATURE ENGINEERING\n",
      "============================================================\n",
      "📅 Creando variables temporales...\n",
      "✅ Variables temporales creadas: ['year', 'month', 'quarter', 'season', 'property_age', 'decade_built', 'month_sin', 'month_cos', 'quarter_sin', 'quarter_cos']\n",
      "💰 Creando variables de precio...\n",
      "✅ Variables de precio creadas: ['log_price', 'price_per_sqm', 'price_category', 'price_zscore']\n",
      "🏠 Creando variables de tamaño...\n",
      "✅ Variables de tamaño creadas: ['rooms_category', 'size_category', 'sqm_per_room', 'rooms_sqm_ratio']\n",
      "🔤 Iniciando codificación de variables categóricas...\n",
      "✅ Variables de precio creadas: ['log_price', 'price_per_sqm', 'price_category', 'price_zscore']\n",
      "🏠 Creando variables de tamaño...\n",
      "✅ Variables de tamaño creadas: ['rooms_category', 'size_category', 'sqm_per_room', 'rooms_sqm_ratio']\n",
      "🔤 Iniciando codificación de variables categóricas...\n",
      "\n",
      "📊 Análisis de cardinalidad:\n",
      "region: 4 categorías únicas\n",
      "house_type: 5 categorías únicas\n",
      "sales_type: 5 categorías únicas\n",
      "season: 4 categorías únicas\n",
      "price_category: 4 categorías únicas\n",
      "rooms_category: 4 categorías únicas\n",
      "size_category: 3 categorías únicas\n",
      "\n",
      "🔢 Aplicando One-Hot Encoding...\n",
      "\n",
      "📊 Análisis de cardinalidad:\n",
      "region: 4 categorías únicas\n",
      "house_type: 5 categorías únicas\n",
      "sales_type: 5 categorías únicas\n",
      "season: 4 categorías únicas\n",
      "price_category: 4 categorías únicas\n",
      "rooms_category: 4 categorías únicas\n",
      "size_category: 3 categorías únicas\n",
      "\n",
      "🔢 Aplicando One-Hot Encoding...\n",
      "✅ house_type: 4 variables dummy creadas\n",
      "✅ sales_type: 4 variables dummy creadas\n",
      "✅ house_type: 4 variables dummy creadas\n",
      "✅ sales_type: 4 variables dummy creadas\n",
      "✅ season: 3 variables dummy creadas\n",
      "✅ price_category: 3 variables dummy creadas\n",
      "✅ season: 3 variables dummy creadas\n",
      "✅ price_category: 3 variables dummy creadas\n",
      "✅ rooms_category: 3 variables dummy creadas\n",
      "✅ size_category: 2 variables dummy creadas\n",
      "\n",
      "🎯 Aplicando Target Encoding para 'region'...\n",
      "✅ rooms_category: 3 variables dummy creadas\n",
      "✅ size_category: 2 variables dummy creadas\n",
      "\n",
      "🎯 Aplicando Target Encoding para 'region'...\n",
      "✅ Target encoding aplicado: media global = 1915291\n",
      "\n",
      "📊 Aplicando Frequency Encoding...\n",
      "✅ Frequency encoding aplicado a 'region'\n",
      "\n",
      "📋 Resumen de codificación:\n",
      "Dataset: 1,506,591 filas x 61 columnas\n",
      "⚖️ Iniciando normalización y escalado...\n",
      "✅ Target encoding aplicado: media global = 1915291\n",
      "\n",
      "📊 Aplicando Frequency Encoding...\n",
      "✅ Frequency encoding aplicado a 'region'\n",
      "\n",
      "📋 Resumen de codificación:\n",
      "Dataset: 1,506,591 filas x 61 columnas\n",
      "⚖️ Iniciando normalización y escalado...\n",
      "Variables numéricas a escalar: 13\n",
      "StandardScaler aplicado a: 6 variables\n",
      "Variables numéricas a escalar: 13\n",
      "StandardScaler aplicado a: 6 variables\n",
      "RobustScaler aplicado a: 7 variables\n",
      "MinMaxScaler aplicado a: 4 variables\n",
      "✅ Escalado completado: 1,506,591 filas x 61 columnas\n",
      "🚀 Creando Feature Engineering Avanzado...\n",
      "RobustScaler aplicado a: 7 variables\n",
      "MinMaxScaler aplicado a: 4 variables\n",
      "✅ Escalado completado: 1,506,591 filas x 61 columnas\n",
      "🚀 Creando Feature Engineering Avanzado...\n",
      "\n",
      "🔗 Creando variables de interacción...\n",
      "✅ sqm × region_target_encoded\n",
      "✅ price_per_sqm × region_target_encoded\n",
      "✅ property_age × house_type_Villa\n",
      "✅ sqm_per_room²\n",
      "✅ no_rooms × sqm\n",
      "\n",
      "💹 Creando variables macroeconómicas...\n",
      "✅ time_trend\n",
      "✅ crisis_period (años: [2008, 2009, 2020, 2021])\n",
      "\n",
      "🔗 Creando variables de interacción...\n",
      "✅ sqm × region_target_encoded\n",
      "✅ price_per_sqm × region_target_encoded\n",
      "✅ property_age × house_type_Villa\n",
      "✅ sqm_per_room²\n",
      "✅ no_rooms × sqm\n",
      "\n",
      "💹 Creando variables macroeconómicas...\n",
      "✅ time_trend\n",
      "✅ crisis_period (años: [2008, 2009, 2020, 2021])\n",
      "✅ market_phase → ['phase_boom_2000s', 'phase_covid_era', 'phase_crisis_post2008', 'phase_growth_90s', 'phase_recovery_2010s']\n",
      "\n",
      "🌍 Creando variables geográficas...\n",
      "✅ is_premium\n",
      "✅ market_phase → ['phase_boom_2000s', 'phase_covid_era', 'phase_crisis_post2008', 'phase_growth_90s', 'phase_recovery_2010s']\n",
      "\n",
      "🌍 Creando variables geográficas...\n",
      "✅ is_premium\n",
      "✅ price_deviation_from_median\n",
      "\n",
      "📋 Features avanzados completados: 1,506,591 filas x 78 columnas\n",
      "🎯 Preparando dataset final para modelado...\n",
      "Features candidatas: 53\n",
      "\n",
      "🧹 Limpieza de datos...\n",
      "✅ price_deviation_from_median\n",
      "\n",
      "📋 Features avanzados completados: 1,506,591 filas x 78 columnas\n",
      "🎯 Preparando dataset final para modelado...\n",
      "Features candidatas: 53\n",
      "\n",
      "🧹 Limpieza de datos...\n",
      "\n",
      "🎯 Feature selection...\n",
      "Usando muestra de 50,000 observaciones\n",
      "\n",
      "🎯 Feature selection...\n",
      "Usando muestra de 50,000 observaciones\n",
      "✅ Seleccionadas 30 features de 53\n",
      "📊 Dataset final: 1,506,591 filas x 30 features\n",
      "✅ Seleccionadas 30 features de 53\n",
      "📊 Dataset final: 1,506,591 filas x 30 features\n",
      "⚠️ Columnas duplicadas encontradas, eliminando duplicados...\n",
      "Columnas duplicadas: ['year']\n",
      "✅ Columnas duplicadas eliminadas. Nuevo shape: (1506591, 31)\n",
      "📅 Creando división temporal train/test...\n",
      "⚠️ Columnas duplicadas encontradas, eliminando duplicados...\n",
      "Columnas duplicadas: ['year']\n",
      "✅ Columnas duplicadas eliminadas. Nuevo shape: (1506591, 31)\n",
      "📅 Creando división temporal train/test...\n",
      "📈 Train: 893,112 obs. (59.3%) - 1992-2017\n",
      "📊 Test: 613,479 obs. (40.7%) - 2018-2024\n",
      "💾 Guardando artefactos de feature engineering...\n",
      "📈 Train: 893,112 obs. (59.3%) - 1992-2017\n",
      "📊 Test: 613,479 obs. (40.7%) - 2018-2024\n",
      "💾 Guardando artefactos de feature engineering...\n",
      "✅ Artefactos guardados en: /home/exodia/Documentos/TFBigData/data/processed\n",
      "  📄 feature_engineered_complete.parquet: 47.0 MB\n",
      "  📄 modeling_dataset.parquet: 47.0 MB\n",
      "  📄 train_data.parquet: 28.5 MB\n",
      "  📄 test_data.parquet: 17.1 MB\n",
      "  📄 scalers.pkl: 0.0 MB\n",
      "  📄 selected_features.txt: 0.0 MB\n",
      "  📄 feature_engineering_metadata.json: 0.0 MB\n",
      "  📄 feature_engineering_summary.md: 0.0 MB\n",
      "\n",
      "🎉 PIPELINE COMPLETADO EXITOSAMENTE!\n",
      "📊 Dataset final: 30 features seleccionadas\n",
      "📁 Archivos guardados en: /home/exodia/Documentos/TFBigData/data/processed\n",
      "\n",
      "Pipeline completado.\n",
      "Dataset final: 1,506,591 filas x 31 columnas\n",
      "Features seleccionadas: 30\n",
      "Archivos guardados en: /home/exodia/Documentos/TFBigData/data/processed\n",
      "✅ Artefactos guardados en: /home/exodia/Documentos/TFBigData/data/processed\n",
      "  📄 feature_engineered_complete.parquet: 47.0 MB\n",
      "  📄 modeling_dataset.parquet: 47.0 MB\n",
      "  📄 train_data.parquet: 28.5 MB\n",
      "  📄 test_data.parquet: 17.1 MB\n",
      "  📄 scalers.pkl: 0.0 MB\n",
      "  📄 selected_features.txt: 0.0 MB\n",
      "  📄 feature_engineering_metadata.json: 0.0 MB\n",
      "  📄 feature_engineering_summary.md: 0.0 MB\n",
      "\n",
      "🎉 PIPELINE COMPLETADO EXITOSAMENTE!\n",
      "📊 Dataset final: 30 features seleccionadas\n",
      "📁 Archivos guardados en: /home/exodia/Documentos/TFBigData/data/processed\n",
      "\n",
      "Pipeline completado.\n",
      "Dataset final: 1,506,591 filas x 31 columnas\n",
      "Features seleccionadas: 30\n",
      "Archivos guardados en: /home/exodia/Documentos/TFBigData/data/processed\n"
     ]
    }
   ],
   "source": [
    "# Pipeline completo (sin enriquecimiento geográfico temporalmente)\n",
    "print(\"Ejecutando pipeline de feature engineering...\")\n",
    "\n",
    "output_dir = DATA_DIR / \"processed\"\n",
    "\n",
    "# Usar el pipeline básico primero\n",
    "from feature_engineering import run_complete_feature_engineering_pipeline\n",
    "\n",
    "results = run_complete_feature_engineering_pipeline(\n",
    "    df=df,\n",
    "    target_col=TARGET,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "# Extraer resultados\n",
    "df_final = results['final_dataset']\n",
    "selected_features = results['selected_features']\n",
    "metadata = results['metadata']\n",
    "saved_files = results['saved_files']\n",
    "\n",
    "print(f\"\\nPipeline completado.\")\n",
    "print(f\"Dataset final: {df_final.shape[0]:,} filas x {df_final.shape[1]} columnas\")\n",
    "print(f\"Features seleccionadas: {len(selected_features)}\")\n",
    "\n",
    "# Agregar enriquecimiento geográfico simple aquí directamente\n",
    "if 'region' in df_final.columns:\n",
    "    print(\"Agregando características geográficas...\")\n",
    "    \n",
    "    # Mapeo simple de densidad urbana por región\n",
    "    urban_density_map = {\n",
    "        'Copenhagen': 5, 'Aarhus': 4, 'Odense': 3, 'Aalborg': 3,\n",
    "        'Frederiksberg': 5, 'Esbjerg': 2, 'Randers': 2, 'Kolding': 2\n",
    "    }\n",
    "    \n",
    "    df_final['urban_density'] = df_final['region'].map(urban_density_map).fillna(1)\n",
    "    df_final['location_type'] = df_final['urban_density'].apply(\n",
    "        lambda x: 'Urban' if x >= 4 else 'Suburban' if x >= 2 else 'Rural'\n",
    "    )\n",
    "    \n",
    "    print(\"Características geográficas agregadas: urban_density, location_type\")\n",
    "\n",
    "print(f\"Archivos guardados en: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f86427",
   "metadata": {},
   "source": [
    "## Documentación de Artefactos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa9bd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo                                       | Descripción\n",
      "--------------------------------------------------------------------------------\n",
      "feature_engineered_complete.parquet           | Dataset con todas las features generadas\n",
      "modeling_dataset.parquet                      | Dataset final para modelado\n",
      "train_data.parquet                            | Conjunto de entrenamiento\n",
      "test_data.parquet                             | Conjunto de prueba\n",
      "scalers.pkl                                   | Escaladores ajustados\n",
      "selected_features.txt                         | Lista de features seleccionadas\n",
      "feature_engineering_metadata.json             | Metadatos del proceso\n",
      "feature_engineering_summary.md                | Archivo auxiliar\n",
      "\n",
      "Documentación guardada: feature_engineering_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Documentación de archivos generados\n",
    "print(f\"{'Archivo':<45} | {'Descripción'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "file_descriptions = {\n",
    "    \"feature_engineered_complete.parquet\": \"Dataset con todas las features generadas\",\n",
    "    \"modeling_dataset.parquet\": \"Dataset final para modelado\",\n",
    "    \"train_data.parquet\": \"Conjunto de entrenamiento\",\n",
    "    \"test_data.parquet\": \"Conjunto de prueba\",\n",
    "    \"selected_features.txt\": \"Lista de features seleccionadas\",\n",
    "    \"scalers.pkl\": \"Escaladores ajustados\",\n",
    "    \"feature_engineering_metadata.json\": \"Metadatos del proceso\",\n",
    "    \"feature_engineered_with_geography.parquet\": \"Dataset con enriquecimiento geográfico\"\n",
    "}\n",
    "\n",
    "for name, path_obj in saved_files.items():\n",
    "    if hasattr(path_obj, 'exists') and path_obj.exists():\n",
    "        desc = file_descriptions.get(path_obj.name, \"Archivo auxiliar\")\n",
    "        print(f\"{path_obj.name:<45} | {desc}\")\n",
    "\n",
    "# Guardar documentación final\n",
    "doc_path = output_dir / \"feature_engineering_summary.json\"\n",
    "summary = {\n",
    "    \"fecha\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"dataset_shape\": f\"{df_final.shape[0]} x {df_final.shape[1]}\",\n",
    "    \"features_count\": len(selected_features),\n",
    "    \"geographic_enrichment\": 'geographic_features' in results,\n",
    "    \"archivos_generados\": {name: str(path) for name, path in saved_files.items()}\n",
    "}\n",
    "\n",
    "with open(doc_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nDocumentación guardada: {doc_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd1372f",
   "metadata": {},
   "source": [
    "## Proceso Completado\n",
    "\n",
    "El pipeline de feature engineering ha sido ejecutado exitosamente con enriquecimiento geográfico incluido. Los datos están listos para el modelado supervisado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFBigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
