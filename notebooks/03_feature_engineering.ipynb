{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0551193",
   "metadata": {},
   "source": [
    "# Feature Engineering para Precios Inmobiliarios Dinamarca\n",
    "\n",
    "**Objetivo**: Transformaci√≥n y construcci√≥n de variables predictivas para el modelado supervisado de precios de vivienda en Dinamarca.\n",
    "\n",
    "**Contenido**:\n",
    "1. Carga de datos y configuraci√≥n inicial\n",
    "2. Pipeline de feature engineering modular\n",
    "3. Enriquecimiento geogr√°fico\n",
    "4. Documentaci√≥n de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b8f4523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/exodia/Documentos/TFBigData\n",
      "Configuraci√≥n cargada.\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n y carga de m√≥dulos\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "import setup\n",
    "setup.set_project_root()\n",
    "\n",
    "from config import *\n",
    "from feature_engineering import enhanced_feature_engineering_pipeline\n",
    "from descriptive_analysis import load_and_validate_data\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Configuraci√≥n cargada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588037d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados: 1,506,591 filas, 19 columnas\n",
      "Rango temporal: 1992-01-05 a 2024-09-30\n"
     ]
    }
   ],
   "source": [
    "# Carga de datos\n",
    "df = pd.read_parquet(CLEAN_FILE)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"Datos cargados: {df.shape[0]:,} filas, {df.shape[1]} columnas\")\n",
    "print(f\"Rango temporal: {df['date'].min().date()} a {df['date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b57cfa8",
   "metadata": {},
   "source": [
    "## Pipeline de Feature Engineering con Enriquecimiento Geogr√°fico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f61063f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando pipeline de feature engineering...\n",
      "üöÄ INICIANDO PIPELINE COMPLETO DE FEATURE ENGINEERING\n",
      "============================================================\n",
      "üìÖ Creando variables temporales...\n",
      "‚úÖ Variables temporales creadas: ['year', 'month', 'quarter', 'season', 'property_age', 'decade_built', 'month_sin', 'month_cos', 'quarter_sin', 'quarter_cos']\n",
      "üí∞ Creando variables de precio...\n",
      "‚úÖ Variables de precio creadas: ['log_price', 'price_per_sqm', 'price_category', 'price_zscore']\n",
      "üè† Creando variables de tama√±o...\n",
      "‚úÖ Variables de tama√±o creadas: ['rooms_category', 'size_category', 'sqm_per_room', 'rooms_sqm_ratio']\n",
      "üî§ Iniciando codificaci√≥n de variables categ√≥ricas...\n",
      "‚úÖ Variables de precio creadas: ['log_price', 'price_per_sqm', 'price_category', 'price_zscore']\n",
      "üè† Creando variables de tama√±o...\n",
      "‚úÖ Variables de tama√±o creadas: ['rooms_category', 'size_category', 'sqm_per_room', 'rooms_sqm_ratio']\n",
      "üî§ Iniciando codificaci√≥n de variables categ√≥ricas...\n",
      "\n",
      "üìä An√°lisis de cardinalidad:\n",
      "region: 4 categor√≠as √∫nicas\n",
      "house_type: 5 categor√≠as √∫nicas\n",
      "sales_type: 5 categor√≠as √∫nicas\n",
      "season: 4 categor√≠as √∫nicas\n",
      "price_category: 4 categor√≠as √∫nicas\n",
      "rooms_category: 4 categor√≠as √∫nicas\n",
      "size_category: 3 categor√≠as √∫nicas\n",
      "\n",
      "üî¢ Aplicando One-Hot Encoding...\n",
      "\n",
      "üìä An√°lisis de cardinalidad:\n",
      "region: 4 categor√≠as √∫nicas\n",
      "house_type: 5 categor√≠as √∫nicas\n",
      "sales_type: 5 categor√≠as √∫nicas\n",
      "season: 4 categor√≠as √∫nicas\n",
      "price_category: 4 categor√≠as √∫nicas\n",
      "rooms_category: 4 categor√≠as √∫nicas\n",
      "size_category: 3 categor√≠as √∫nicas\n",
      "\n",
      "üî¢ Aplicando One-Hot Encoding...\n",
      "‚úÖ house_type: 4 variables dummy creadas\n",
      "‚úÖ sales_type: 4 variables dummy creadas\n",
      "‚úÖ house_type: 4 variables dummy creadas\n",
      "‚úÖ sales_type: 4 variables dummy creadas\n",
      "‚úÖ season: 3 variables dummy creadas\n",
      "‚úÖ price_category: 3 variables dummy creadas\n",
      "‚úÖ season: 3 variables dummy creadas\n",
      "‚úÖ price_category: 3 variables dummy creadas\n",
      "‚úÖ rooms_category: 3 variables dummy creadas\n",
      "‚úÖ size_category: 2 variables dummy creadas\n",
      "\n",
      "üéØ Aplicando Target Encoding para 'region'...\n",
      "‚úÖ rooms_category: 3 variables dummy creadas\n",
      "‚úÖ size_category: 2 variables dummy creadas\n",
      "\n",
      "üéØ Aplicando Target Encoding para 'region'...\n",
      "‚úÖ Target encoding aplicado: media global = 1915291\n",
      "\n",
      "üìä Aplicando Frequency Encoding...\n",
      "‚úÖ Frequency encoding aplicado a 'region'\n",
      "\n",
      "üìã Resumen de codificaci√≥n:\n",
      "Dataset: 1,506,591 filas x 61 columnas\n",
      "‚öñÔ∏è Iniciando normalizaci√≥n y escalado...\n",
      "‚úÖ Target encoding aplicado: media global = 1915291\n",
      "\n",
      "üìä Aplicando Frequency Encoding...\n",
      "‚úÖ Frequency encoding aplicado a 'region'\n",
      "\n",
      "üìã Resumen de codificaci√≥n:\n",
      "Dataset: 1,506,591 filas x 61 columnas\n",
      "‚öñÔ∏è Iniciando normalizaci√≥n y escalado...\n",
      "Variables num√©ricas a escalar: 13\n",
      "StandardScaler aplicado a: 6 variables\n",
      "Variables num√©ricas a escalar: 13\n",
      "StandardScaler aplicado a: 6 variables\n",
      "RobustScaler aplicado a: 7 variables\n",
      "MinMaxScaler aplicado a: 4 variables\n",
      "‚úÖ Escalado completado: 1,506,591 filas x 61 columnas\n",
      "üöÄ Creando Feature Engineering Avanzado...\n",
      "RobustScaler aplicado a: 7 variables\n",
      "MinMaxScaler aplicado a: 4 variables\n",
      "‚úÖ Escalado completado: 1,506,591 filas x 61 columnas\n",
      "üöÄ Creando Feature Engineering Avanzado...\n",
      "\n",
      "üîó Creando variables de interacci√≥n...\n",
      "‚úÖ sqm √ó region_target_encoded\n",
      "‚úÖ price_per_sqm √ó region_target_encoded\n",
      "‚úÖ property_age √ó house_type_Villa\n",
      "‚úÖ sqm_per_room¬≤\n",
      "‚úÖ no_rooms √ó sqm\n",
      "\n",
      "üíπ Creando variables macroecon√≥micas...\n",
      "‚úÖ time_trend\n",
      "‚úÖ crisis_period (a√±os: [2008, 2009, 2020, 2021])\n",
      "\n",
      "üîó Creando variables de interacci√≥n...\n",
      "‚úÖ sqm √ó region_target_encoded\n",
      "‚úÖ price_per_sqm √ó region_target_encoded\n",
      "‚úÖ property_age √ó house_type_Villa\n",
      "‚úÖ sqm_per_room¬≤\n",
      "‚úÖ no_rooms √ó sqm\n",
      "\n",
      "üíπ Creando variables macroecon√≥micas...\n",
      "‚úÖ time_trend\n",
      "‚úÖ crisis_period (a√±os: [2008, 2009, 2020, 2021])\n",
      "‚úÖ market_phase ‚Üí ['phase_boom_2000s', 'phase_covid_era', 'phase_crisis_post2008', 'phase_growth_90s', 'phase_recovery_2010s']\n",
      "\n",
      "üåç Creando variables geogr√°ficas...\n",
      "‚úÖ is_premium\n",
      "‚úÖ market_phase ‚Üí ['phase_boom_2000s', 'phase_covid_era', 'phase_crisis_post2008', 'phase_growth_90s', 'phase_recovery_2010s']\n",
      "\n",
      "üåç Creando variables geogr√°ficas...\n",
      "‚úÖ is_premium\n",
      "‚úÖ price_deviation_from_median\n",
      "\n",
      "üìã Features avanzados completados: 1,506,591 filas x 78 columnas\n",
      "üéØ Preparando dataset final para modelado...\n",
      "Features candidatas: 53\n",
      "\n",
      "üßπ Limpieza de datos...\n",
      "‚úÖ price_deviation_from_median\n",
      "\n",
      "üìã Features avanzados completados: 1,506,591 filas x 78 columnas\n",
      "üéØ Preparando dataset final para modelado...\n",
      "Features candidatas: 53\n",
      "\n",
      "üßπ Limpieza de datos...\n",
      "\n",
      "üéØ Feature selection...\n",
      "Usando muestra de 50,000 observaciones\n",
      "\n",
      "üéØ Feature selection...\n",
      "Usando muestra de 50,000 observaciones\n",
      "‚úÖ Seleccionadas 30 features de 53\n",
      "üìä Dataset final: 1,506,591 filas x 30 features\n",
      "‚úÖ Seleccionadas 30 features de 53\n",
      "üìä Dataset final: 1,506,591 filas x 30 features\n",
      "‚ö†Ô∏è Columnas duplicadas encontradas, eliminando duplicados...\n",
      "Columnas duplicadas: ['year']\n",
      "‚úÖ Columnas duplicadas eliminadas. Nuevo shape: (1506591, 31)\n",
      "üìÖ Creando divisi√≥n temporal train/test...\n",
      "‚ö†Ô∏è Columnas duplicadas encontradas, eliminando duplicados...\n",
      "Columnas duplicadas: ['year']\n",
      "‚úÖ Columnas duplicadas eliminadas. Nuevo shape: (1506591, 31)\n",
      "üìÖ Creando divisi√≥n temporal train/test...\n",
      "üìà Train: 893,112 obs. (59.3%) - 1992-2017\n",
      "üìä Test: 613,479 obs. (40.7%) - 2018-2024\n",
      "üíæ Guardando artefactos de feature engineering...\n",
      "üìà Train: 893,112 obs. (59.3%) - 1992-2017\n",
      "üìä Test: 613,479 obs. (40.7%) - 2018-2024\n",
      "üíæ Guardando artefactos de feature engineering...\n",
      "‚úÖ Artefactos guardados en: /home/exodia/Documentos/TFBigData/data/processed\n",
      "  üìÑ feature_engineered_complete.parquet: 47.0 MB\n",
      "  üìÑ modeling_dataset.parquet: 47.0 MB\n",
      "  üìÑ train_data.parquet: 28.5 MB\n",
      "  üìÑ test_data.parquet: 17.1 MB\n",
      "  üìÑ scalers.pkl: 0.0 MB\n",
      "  üìÑ selected_features.txt: 0.0 MB\n",
      "  üìÑ feature_engineering_metadata.json: 0.0 MB\n",
      "  üìÑ feature_engineering_summary.md: 0.0 MB\n",
      "\n",
      "üéâ PIPELINE COMPLETADO EXITOSAMENTE!\n",
      "üìä Dataset final: 30 features seleccionadas\n",
      "üìÅ Archivos guardados en: /home/exodia/Documentos/TFBigData/data/processed\n",
      "\n",
      "Pipeline completado.\n",
      "Dataset final: 1,506,591 filas x 31 columnas\n",
      "Features seleccionadas: 30\n",
      "Archivos guardados en: /home/exodia/Documentos/TFBigData/data/processed\n",
      "‚úÖ Artefactos guardados en: /home/exodia/Documentos/TFBigData/data/processed\n",
      "  üìÑ feature_engineered_complete.parquet: 47.0 MB\n",
      "  üìÑ modeling_dataset.parquet: 47.0 MB\n",
      "  üìÑ train_data.parquet: 28.5 MB\n",
      "  üìÑ test_data.parquet: 17.1 MB\n",
      "  üìÑ scalers.pkl: 0.0 MB\n",
      "  üìÑ selected_features.txt: 0.0 MB\n",
      "  üìÑ feature_engineering_metadata.json: 0.0 MB\n",
      "  üìÑ feature_engineering_summary.md: 0.0 MB\n",
      "\n",
      "üéâ PIPELINE COMPLETADO EXITOSAMENTE!\n",
      "üìä Dataset final: 30 features seleccionadas\n",
      "üìÅ Archivos guardados en: /home/exodia/Documentos/TFBigData/data/processed\n",
      "\n",
      "Pipeline completado.\n",
      "Dataset final: 1,506,591 filas x 31 columnas\n",
      "Features seleccionadas: 30\n",
      "Archivos guardados en: /home/exodia/Documentos/TFBigData/data/processed\n"
     ]
    }
   ],
   "source": [
    "# Pipeline completo (sin enriquecimiento geogr√°fico temporalmente)\n",
    "print(\"Ejecutando pipeline de feature engineering...\")\n",
    "\n",
    "output_dir = DATA_DIR / \"processed\"\n",
    "\n",
    "# Usar el pipeline b√°sico primero\n",
    "from feature_engineering import run_complete_feature_engineering_pipeline\n",
    "\n",
    "results = run_complete_feature_engineering_pipeline(\n",
    "    df=df,\n",
    "    target_col=TARGET,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "# Extraer resultados\n",
    "df_final = results['final_dataset']\n",
    "selected_features = results['selected_features']\n",
    "metadata = results['metadata']\n",
    "saved_files = results['saved_files']\n",
    "\n",
    "print(f\"\\nPipeline completado.\")\n",
    "print(f\"Dataset final: {df_final.shape[0]:,} filas x {df_final.shape[1]} columnas\")\n",
    "print(f\"Features seleccionadas: {len(selected_features)}\")\n",
    "\n",
    "# Agregar enriquecimiento geogr√°fico simple aqu√≠ directamente\n",
    "if 'region' in df_final.columns:\n",
    "    print(\"Agregando caracter√≠sticas geogr√°ficas...\")\n",
    "    \n",
    "    # Mapeo simple de densidad urbana por regi√≥n\n",
    "    urban_density_map = {\n",
    "        'Copenhagen': 5, 'Aarhus': 4, 'Odense': 3, 'Aalborg': 3,\n",
    "        'Frederiksberg': 5, 'Esbjerg': 2, 'Randers': 2, 'Kolding': 2\n",
    "    }\n",
    "    \n",
    "    df_final['urban_density'] = df_final['region'].map(urban_density_map).fillna(1)\n",
    "    df_final['location_type'] = df_final['urban_density'].apply(\n",
    "        lambda x: 'Urban' if x >= 4 else 'Suburban' if x >= 2 else 'Rural'\n",
    "    )\n",
    "    \n",
    "    print(\"Caracter√≠sticas geogr√°ficas agregadas: urban_density, location_type\")\n",
    "\n",
    "print(f\"Archivos guardados en: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f86427",
   "metadata": {},
   "source": [
    "## Documentaci√≥n de Artefactos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa9bd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo                                       | Descripci√≥n\n",
      "--------------------------------------------------------------------------------\n",
      "feature_engineered_complete.parquet           | Dataset con todas las features generadas\n",
      "modeling_dataset.parquet                      | Dataset final para modelado\n",
      "train_data.parquet                            | Conjunto de entrenamiento\n",
      "test_data.parquet                             | Conjunto de prueba\n",
      "scalers.pkl                                   | Escaladores ajustados\n",
      "selected_features.txt                         | Lista de features seleccionadas\n",
      "feature_engineering_metadata.json             | Metadatos del proceso\n",
      "feature_engineering_summary.md                | Archivo auxiliar\n",
      "\n",
      "Documentaci√≥n guardada: feature_engineering_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Documentaci√≥n de archivos generados\n",
    "print(f\"{'Archivo':<45} | {'Descripci√≥n'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "file_descriptions = {\n",
    "    \"feature_engineered_complete.parquet\": \"Dataset con todas las features generadas\",\n",
    "    \"modeling_dataset.parquet\": \"Dataset final para modelado\",\n",
    "    \"train_data.parquet\": \"Conjunto de entrenamiento\",\n",
    "    \"test_data.parquet\": \"Conjunto de prueba\",\n",
    "    \"selected_features.txt\": \"Lista de features seleccionadas\",\n",
    "    \"scalers.pkl\": \"Escaladores ajustados\",\n",
    "    \"feature_engineering_metadata.json\": \"Metadatos del proceso\",\n",
    "    \"feature_engineered_with_geography.parquet\": \"Dataset con enriquecimiento geogr√°fico\"\n",
    "}\n",
    "\n",
    "for name, path_obj in saved_files.items():\n",
    "    if hasattr(path_obj, 'exists') and path_obj.exists():\n",
    "        desc = file_descriptions.get(path_obj.name, \"Archivo auxiliar\")\n",
    "        print(f\"{path_obj.name:<45} | {desc}\")\n",
    "\n",
    "# Guardar documentaci√≥n final\n",
    "doc_path = output_dir / \"feature_engineering_summary.json\"\n",
    "summary = {\n",
    "    \"fecha\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"dataset_shape\": f\"{df_final.shape[0]} x {df_final.shape[1]}\",\n",
    "    \"features_count\": len(selected_features),\n",
    "    \"geographic_enrichment\": 'geographic_features' in results,\n",
    "    \"archivos_generados\": {name: str(path) for name, path in saved_files.items()}\n",
    "}\n",
    "\n",
    "with open(doc_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nDocumentaci√≥n guardada: {doc_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd1372f",
   "metadata": {},
   "source": [
    "## Proceso Completado\n",
    "\n",
    "El pipeline de feature engineering ha sido ejecutado exitosamente con enriquecimiento geogr√°fico incluido. Los datos est√°n listos para el modelado supervisado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFBigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
