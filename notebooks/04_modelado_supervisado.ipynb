{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc65558b",
   "metadata": {},
   "source": [
    "# 🤖 Modelado Supervisado H2O + GPU + Paralelo - Precios Inmobiliarios Dinamarca\n",
    "\n",
    "**Objetivo**: Construir modelos distribuidos H2O, clásicos sklearn y de árbol con optimizaciones GPU y paralelización, evaluarlos con métricas sólidas considerando rendimiento y velocidad.\n",
    "\n",
    "1. División de Datos (Temporal)\n",
    "\n",
    "    1.1 Split temporal (80% train, 20% test)\n",
    "\n",
    "    1.2 Validación de distribución en train/test\n",
    "\n",
    "    1.3 Visualización de drift o cambios en el tiempo (si aplica)\n",
    "\n",
    "2. Modelos Estadísticos\n",
    "\n",
    "    2.1 Regresión Lineal, Ridge, Lasso, ElasticNet\n",
    "\n",
    "    2.2 Ajuste con mínimos cuadrados o likelihood\n",
    "\n",
    "    2.3 Comparación de modelos con AIC/BIC (sin CV)\n",
    "\n",
    "    2.4 Diagnóstico de residuos y detección de outliers\n",
    "\n",
    "3. Modelos de Árboles\n",
    "\n",
    "    3.1 LightGBM y Random Forest (hiperparámetros por defecto o mínimos)\n",
    "\n",
    "    3.2 Comparación con modelos estadísticos\n",
    "\n",
    "    3.3 Feature importance y explicabilidad inicial\n",
    "\n",
    "6. Evaluación General\n",
    "\n",
    "    6.1 Métricas: RMSE, MAE, MAPE, R²\n",
    "\n",
    "    6.2 Visualización de errores (residuos, pred vs real)\n",
    "\n",
    "    6.3 Análisis por segmentos (tipo de casa, región)\n",
    "\n",
    "    6.4 Tabla resumen de modelos y conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd407dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/exodia/Documentos/TFBigData\n"
     ]
    }
   ],
   "source": [
    "from setup import set_project_root\n",
    "set_project_root()\n",
    "\n",
    "from config import *\n",
    "from descriptive_analysis import (\n",
    "    load_and_validate_data,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858e6ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321......... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"17.0.8\" 2023-07-18; OpenJDK Runtime Environment Temurin-17.0.8+7 (build 17.0.8+7); OpenJDK 64-Bit Server VM Temurin-17.0.8+7 (build 17.0.8+7, mixed mode, sharing)\n",
      "  Starting server from /home/exodia/miniconda3/envs/TFBigData/lib/python3.10/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpugqe8252\n",
      "  JVM stdout: /tmp/tmpugqe8252/h2o_exodia_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpugqe8252/h2o_exodia_started_from_python.err\n",
      " not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"17.0.8\" 2023-07-18; OpenJDK Runtime Environment Temurin-17.0.8+7 (build 17.0.8+7); OpenJDK 64-Bit Server VM Temurin-17.0.8+7 (build 17.0.8+7, mixed mode, sharing)\n",
      "  Starting server from /home/exodia/miniconda3/envs/TFBigData/lib/python3.10/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpugqe8252\n",
      "  JVM stdout: /tmp/tmpugqe8252/h2o_exodia_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpugqe8252/h2o_exodia_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ...  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n",
      "Warning: Your H2O cluster version is (3 months and 15 days) old.  There may be a newer version available.\n",
      "Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n",
      " successful.\n",
      "Warning: Your H2O cluster version is (3 months and 15 days) old.  There may be a newer version available.\n",
      "Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>00 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>America/Lima</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.46.0.7</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>3 months and 15 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_exodia_7tsk3w</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>6.723 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.18 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         00 secs\n",
       "H2O_cluster_timezone:       America/Lima\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.46.0.7\n",
       "H2O_cluster_version_age:    3 months and 15 days\n",
       "H2O_cluster_name:           H2O_from_python_exodia_7tsk3w\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    6.723 Gb\n",
       "H2O_cluster_total_cores:    16\n",
       "H2O_cluster_allowed_cores:  16\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.18 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importando datos desde /home/exodia/Documentos/TFBigData/data/processed/cleaned_data.parquet\n",
      "\n",
      "Parse progress: |Parse progress: |██████████████████████████████████████████████████████████████████████| (done) 100%\n",
      "██████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_clean, h2o_frame \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLEAN_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdestination_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDESTINATION_FRAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/TFBigData/src/descriptive_analysis.py:61\u001b[0m, in \u001b[0;36mload_and_validate_data\u001b[0;34m(data_path, destination_frame, parallel)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImportando datos desde \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m df_h2o \u001b[38;5;241m=\u001b[39m h2o\u001b[38;5;241m.\u001b[39mimport_file(\n\u001b[1;32m     55\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(data_path),\n\u001b[1;32m     56\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     57\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m     destination_frame\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(destination_frame)\n\u001b[1;32m     59\u001b[0m )\n\u001b[0;32m---> 61\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m \u001b[43mdf_h2o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_data_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatos importados a H2O con destino: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_frame\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensiones del H2OFrame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_h2o\u001b[38;5;241m.\u001b[39mnrows\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m filas × \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_h2o\u001b[38;5;241m.\u001b[39mncols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columnas\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/h2o/frame.py:1986\u001b[0m, in \u001b[0;36mH2OFrame.as_data_frame\u001b[0;34m(self, use_pandas, header, use_multi_thread)\u001b[0m\n\u001b[1;32m   1982\u001b[0m                     os\u001b[38;5;241m.\u001b[39munlink(exportFile\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1983\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting H2O frame to pandas dataframe using single-thread.  For faster conversion using\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1984\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m multi-thread, install polars and pyarrow and use it as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1985\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas_df = h2o_df.as_data_frame(use_multi_thread=True)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, H2ODependencyWarning)\n\u001b[0;32m-> 1986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas\u001b[38;5;241m.\u001b[39mread_csv(StringIO(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_frame_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m), low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, skip_blank_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)                \n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mh2o\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reader\n\u001b[1;32m   1989\u001b[0m frame \u001b[38;5;241m=\u001b[39m [row \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader(StringIO(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_frame_data()))]\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/h2o/frame.py:2047\u001b[0m, in \u001b[0;36mH2OFrame.get_frame_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_frame_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;124;03m    Get frame data as a string in csv format.\u001b[39;00m\n\u001b[1;32m   2036\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;124;03m    >>> iris.get_frame_data()\u001b[39;00m\n\u001b[1;32m   2046\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2047\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mh2o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET /3/DownloadDataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mframe_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhex_string\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mescape_quotes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/h2o/h2o.py:123\u001b[0m, in \u001b[0;36mapi\u001b[0;34m(endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# type checks are performed in H2OConnection class\u001b[39;00m\n\u001b[1;32m    122\u001b[0m _check_connection()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mh2oconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/h2o/backend/connection.py:494\u001b[0m, in \u001b[0;36mH2OConnection.request\u001b[0;34m(self, endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_start_transaction(endpoint, rd, json, filename, params)\n\u001b[1;32m    493\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_args()\n\u001b[0;32m--> 494\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(save_to, types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[1;32m    497\u001b[0m     save_to \u001b[38;5;241m=\u001b[39m save_to(resp)\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/urllib3/response.py:1088\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1088\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/urllib3/response.py:1251\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1251\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   1253\u001b[0m     chunk, decode_content\u001b[38;5;241m=\u001b[39mdecode_content, flush_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1254\u001b[0m )\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoded:\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/urllib3/response.py:1198\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# amt > self.chunk_left\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m-> 1198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr] # Toss the CRLF at the end of the chunk.\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m returned_chunk\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/http/client.py:631\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[1;32m    625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_clean, h2o_frame = load_and_validate_data(\n",
    "    data_path=CLEAN_FILE, \n",
    "    destination_frame=DESTINATION_FRAME, \n",
    "    parallel=False\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e609c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Cargando datos procesados desde Feature Engineering (Notebook 03)...\n",
      "📂 Directorio procesados: /home/exodia/Documentos/TFBigData/data/processed\n",
      "🔄 Intentando cargar: cleaned_data.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos procesados cargados desde: cleaned_data.parquet\n",
      "   Dimensiones: 1,506,591 filas x 19 columnas\n",
      "\n",
      "🔍 Verificando estructura de datos procesados:\n",
      "   Columnas totales: 19\n",
      "   Tipos de datos: {dtype('int64'): 8, dtype('O'): 8, dtype('float64'): 3}\n",
      "✅ Variable objetivo 'purchase_price' encontrada\n",
      "   Estadísticas del target:\n",
      "     Media: 1915290.96\n",
      "     Mediana: 1400000.00\n",
      "     Std: 1765457.84\n",
      "     Valores nulos: 0\n",
      "\n",
      "📊 Calidad de datos procesados:\n",
      "   Valores nulos totales: 0\n",
      "\n",
      "🏷️ Tipos de variables identificadas:\n",
      "   Categóricas: 8\n",
      "   Numéricas: 11\n",
      "\n",
      "🎯 Features para modelado:\n",
      "   Total disponibles: 14\n",
      "   Excluidas: ['purchase_price', 'house_id', 'address', 'sqm_price', '%_change_between_offer_and_purchase']\n",
      "\n",
      "📋 Muestra de datos procesados:\n",
      "   Valores nulos totales: 0\n",
      "\n",
      "🏷️ Tipos de variables identificadas:\n",
      "   Categóricas: 8\n",
      "   Numéricas: 11\n",
      "\n",
      "🎯 Features para modelado:\n",
      "   Total disponibles: 14\n",
      "   Excluidas: ['purchase_price', 'house_id', 'address', 'sqm_price', '%_change_between_offer_and_purchase']\n",
      "\n",
      "📋 Muestra de datos procesados:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "quarter",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "house_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "house_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sales_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year_build",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "purchase_price",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "%_change_between_offer_and_purchase",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "no_rooms",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sqm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sqm_price",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "address",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "zip_code",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "city",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "area",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "region",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "nom_interest_rate%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dk_ann_infl_rate%",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "yield_on_mortgage_credit_bonds%",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "9f24cdd0-394b-4ad3-a437-3ce6164c1247",
       "rows": [
        [
         "0",
         "1727654400000000000",
         "218",
         "1300",
         "Apartment",
         "regular_sale",
         "1971",
         "1765000",
         "0",
         "3",
         "78.0",
         "22628.205078125",
         "Hedekæret 38, 1. th",
         "2640",
         "Hedehusene",
         "Capital, Copenhagen",
         "Zealand",
         "3.349999904632568",
         "1.1299999952316284",
         "4.340000152587891"
        ],
        [
         "1",
         "1727654400000000000",
         "218",
         "1307",
         "Summerhouse",
         "regular_sale",
         "2009",
         "590939",
         "0",
         "3",
         "50.0",
         "11818.7802734375",
         "Violstien 11",
         "2635",
         "Ishøj",
         "Capital, Copenhagen",
         "Zealand",
         "3.349999904632568",
         "1.1299999952316284",
         "4.340000152587891"
        ],
        [
         "2",
         "1727654400000000000",
         "218",
         "1301",
         "Apartment",
         "regular_sale",
         "1940",
         "1750000",
         "0",
         "2",
         "56.0",
         "31250.0",
         "Buddingevej 72I, st. tv",
         "2800",
         "Kongens Lyngby",
         "Capital, Copenhagen",
         "Zealand",
         "3.349999904632568",
         "1.1299999952316284",
         "4.340000152587891"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>quarter</th>\n",
       "      <th>house_id</th>\n",
       "      <th>house_type</th>\n",
       "      <th>sales_type</th>\n",
       "      <th>year_build</th>\n",
       "      <th>purchase_price</th>\n",
       "      <th>%_change_between_offer_and_purchase</th>\n",
       "      <th>no_rooms</th>\n",
       "      <th>sqm</th>\n",
       "      <th>sqm_price</th>\n",
       "      <th>address</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>city</th>\n",
       "      <th>area</th>\n",
       "      <th>region</th>\n",
       "      <th>nom_interest_rate%</th>\n",
       "      <th>dk_ann_infl_rate%</th>\n",
       "      <th>yield_on_mortgage_credit_bonds%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1727654400000000000</td>\n",
       "      <td>218</td>\n",
       "      <td>1300</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>regular_sale</td>\n",
       "      <td>1971</td>\n",
       "      <td>1765000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>78.0</td>\n",
       "      <td>22628.205078</td>\n",
       "      <td>Hedekæret 38, 1. th</td>\n",
       "      <td>2640</td>\n",
       "      <td>Hedehusene</td>\n",
       "      <td>Capital, Copenhagen</td>\n",
       "      <td>Zealand</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.1299999952316284</td>\n",
       "      <td>4.340000152587891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1727654400000000000</td>\n",
       "      <td>218</td>\n",
       "      <td>1307</td>\n",
       "      <td>Summerhouse</td>\n",
       "      <td>regular_sale</td>\n",
       "      <td>2009</td>\n",
       "      <td>590939</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11818.780273</td>\n",
       "      <td>Violstien 11</td>\n",
       "      <td>2635</td>\n",
       "      <td>Ishøj</td>\n",
       "      <td>Capital, Copenhagen</td>\n",
       "      <td>Zealand</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.1299999952316284</td>\n",
       "      <td>4.340000152587891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1727654400000000000</td>\n",
       "      <td>218</td>\n",
       "      <td>1301</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>regular_sale</td>\n",
       "      <td>1940</td>\n",
       "      <td>1750000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>56.0</td>\n",
       "      <td>31250.000000</td>\n",
       "      <td>Buddingevej 72I, st. tv</td>\n",
       "      <td>2800</td>\n",
       "      <td>Kongens Lyngby</td>\n",
       "      <td>Capital, Copenhagen</td>\n",
       "      <td>Zealand</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.1299999952316284</td>\n",
       "      <td>4.340000152587891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  quarter  house_id   house_type    sales_type  \\\n",
       "0  1727654400000000000      218      1300    Apartment  regular_sale   \n",
       "1  1727654400000000000      218      1307  Summerhouse  regular_sale   \n",
       "2  1727654400000000000      218      1301    Apartment  regular_sale   \n",
       "\n",
       "   year_build  purchase_price  %_change_between_offer_and_purchase  no_rooms  \\\n",
       "0        1971         1765000                                    0         3   \n",
       "1        2009          590939                                    0         3   \n",
       "2        1940         1750000                                    0         2   \n",
       "\n",
       "    sqm     sqm_price                  address  zip_code            city  \\\n",
       "0  78.0  22628.205078      Hedekæret 38, 1. th      2640      Hedehusene   \n",
       "1  50.0  11818.780273             Violstien 11      2635           Ishøj   \n",
       "2  56.0  31250.000000  Buddingevej 72I, st. tv      2800  Kongens Lyngby   \n",
       "\n",
       "                  area   region  nom_interest_rate%   dk_ann_infl_rate%  \\\n",
       "0  Capital, Copenhagen  Zealand                3.35  1.1299999952316284   \n",
       "1  Capital, Copenhagen  Zealand                3.35  1.1299999952316284   \n",
       "2  Capital, Copenhagen  Zealand                3.35  1.1299999952316284   \n",
       "\n",
       "  yield_on_mortgage_credit_bonds%  \n",
       "0               4.340000152587891  \n",
       "1               4.340000152587891  \n",
       "2               4.340000152587891  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Carga de datos procesados completada\n",
      "🚀 Listo para conversión multiframework\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "print(f\"📂 Directorio procesados: {PROCESSED_DIR}\")\n",
    "\n",
    "# Intentar cargar datos procesados con múltiples formatos\n",
    "df_processed = None\n",
    "processed_files = [\n",
    "    PROCESSED_DIR / \"cleaned_data.parquet\",\n",
    "    PROCESSED_DIR / \"processed_data.parquet\", \n",
    "    PROCESSED_DIR / \"feature_engineered_data.parquet\",\n",
    "    PROCESSED_DIR / \"final_data.parquet\",\n",
    "    PROCESSED_DIR / \"cleaned_data.csv\"\n",
    "]\n",
    "\n",
    "for file_path in processed_files:\n",
    "    if file_path.exists():\n",
    "        try:\n",
    "            print(f\"🔄 Intentando cargar: {file_path.name}\")\n",
    "            \n",
    "            if file_path.suffix == '.parquet':\n",
    "                df_processed = pd.read_parquet(file_path)\n",
    "            elif file_path.suffix == '.csv':\n",
    "                df_processed = pd.read_csv(file_path)\n",
    "            \n",
    "            print(f\"✅ Datos procesados cargados desde: {file_path.name}\")\n",
    "            print(f\"   Dimensiones: {df_processed.shape[0]:,} filas x {df_processed.shape[1]} columnas\")\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error cargando {file_path.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Si no se encuentran datos procesados, usar datos limpios como fallback\n",
    "if df_processed is None:\n",
    "    print(\"⚠️ No se encontraron datos procesados, usando datos limpios como fallback\")\n",
    "    df_processed = df_clean.copy()\n",
    "    print(f\"   Usando datos limpios: {df_processed.shape[0]:,} filas x {df_processed.shape[1]} columnas\")\n",
    "\n",
    "# Verificar estructura de datos procesados\n",
    "print(f\"\\n🔍 Verificando estructura de datos procesados:\")\n",
    "print(f\"   Columnas totales: {len(df_processed.columns)}\")\n",
    "print(f\"   Tipos de datos: {df_processed.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "# Verificar si el target está presente\n",
    "if TARGET in df_processed.columns:\n",
    "    print(f\"✅ Variable objetivo '{TARGET}' encontrada\")\n",
    "    print(f\"   Estadísticas del target:\")\n",
    "    print(f\"     Media: {df_processed[TARGET].mean():.2f}\")\n",
    "    print(f\"     Mediana: {df_processed[TARGET].median():.2f}\")\n",
    "    print(f\"     Std: {df_processed[TARGET].std():.2f}\")\n",
    "    print(f\"     Valores nulos: {df_processed[TARGET].isnull().sum()}\")\n",
    "else:\n",
    "    print(f\"❌ Variable objetivo '{TARGET}' NO encontrada\")\n",
    "    print(f\"   Columnas disponibles: {list(df_processed.columns)}\")\n",
    "\n",
    "# Verificar calidad de datos\n",
    "print(f\"\\n📊 Calidad de datos procesados:\")\n",
    "total_nulls = df_processed.isnull().sum().sum()\n",
    "print(f\"   Valores nulos totales: {total_nulls:,}\")\n",
    "\n",
    "if total_nulls > 0:\n",
    "    null_cols = df_processed.isnull().sum()\n",
    "    null_cols = null_cols[null_cols > 0].sort_values(ascending=False)\n",
    "    print(f\"   Columnas con nulls: {len(null_cols)}\")\n",
    "    for col, nulls in null_cols.head(10).items():\n",
    "        print(f\"     {col}: {nulls:,} nulls ({nulls/len(df_processed)*100:.1f}%)\")\n",
    "\n",
    "# Identificar tipos de columnas para el procesamiento\n",
    "categorical_cols = df_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"\\n🏷️ Tipos de variables identificadas:\")\n",
    "print(f\"   Categóricas: {len(categorical_cols)}\")\n",
    "print(f\"   Numéricas: {len(numerical_cols)}\")\n",
    "\n",
    "# Preparar lista de features excluyendo columnas problemáticas\n",
    "features_to_exclude = [TARGET] + DROP_COLS\n",
    "if 'year' in df_processed.columns:\n",
    "    features_to_exclude.append('year')\n",
    "if 'Year' in df_processed.columns:\n",
    "    features_to_exclude.append('Year')\n",
    "\n",
    "# Lista final de features disponibles\n",
    "available_features = [col for col in df_processed.columns \n",
    "                     if col not in features_to_exclude]\n",
    "\n",
    "print(f\"\\n🎯 Features para modelado:\")\n",
    "print(f\"   Total disponibles: {len(available_features)}\")\n",
    "print(f\"   Excluidas: {features_to_exclude}\")\n",
    "\n",
    "# Mostrar muestra de los datos procesados\n",
    "print(f\"\\n📋 Muestra de datos procesados:\")\n",
    "display(df_processed.head(3))\n",
    "\n",
    "print(f\"\\n✅ Carga de datos procesados completada\")\n",
    "print(f\"🚀 Listo para conversión multiframework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec63a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Verificando configuración GPU y paralelización...\n",
      "⚠️ PyTorch no instalado - verificación CUDA limitada\n",
      "✅ LightGBM GPU: Disponible\n",
      "💻 CONFIGURACIÓN CPU:\n",
      "   Cores físicos: 16\n",
      "   Cores lógicos: 16\n",
      "   Memoria total: 26.9 GB\n",
      "   Memoria disponible: 5.3 GB\n",
      "🔧 CONFIGURACIÓN PARALELA:\n",
      "   OMP_NUM_THREADS: 16\n",
      "   MKL_NUM_THREADS: 16\n",
      "\n",
      "📋 ESTRATEGIA DE ENTRENAMIENTO:\n",
      "   GPU para LightGBM: ✅ Sí\n",
      "   Cores para paralelización: 16\n",
      "   Límite de memoria: 5.3 GB\n",
      "   Procesamiento en batches: ✅ Sí\n",
      "\n",
      "⚙️ CONFIGURACIÓN NUMPY:\n",
      "   Información BLAS/LAPACK no disponible\n",
      "\n",
      "✅ Verificación de configuración completada\n",
      "🚀 Sistema optimizado para entrenamiento de alta velocidad\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# VERIFICACIÓN DE CONFIGURACIÓN OPTIMIZADA\n",
    "# ==================================================\n",
    "\n",
    "print(\"🔧 Verificando configuración GPU y paralelización...\")\n",
    "\n",
    "# Verificar configuración de GPU\n",
    "def check_gpu_setup():\n",
    "    \"\"\"Verifica disponibilidad y configuración de GPU\"\"\"\n",
    "    gpu_info = {\n",
    "        'cuda_available': False,\n",
    "        'gpu_count': 0,\n",
    "        'gpu_names': [],\n",
    "        'lightgbm_gpu': False\n",
    "    }\n",
    "    \n",
    "    # Verificar CUDA/GPU\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_info['cuda_available'] = True\n",
    "            gpu_info['gpu_count'] = torch.cuda.device_count()\n",
    "            gpu_info['gpu_names'] = [torch.cuda.get_device_name(i) for i in range(gpu_info['gpu_count'])]\n",
    "        print(f\"🎮 CUDA disponible: {gpu_info['cuda_available']}\")\n",
    "        if gpu_info['cuda_available']:\n",
    "            print(f\"   GPUs detectadas: {gpu_info['gpu_count']}\")\n",
    "            for i, name in enumerate(gpu_info['gpu_names']):\n",
    "                print(f\"   GPU {i}: {name}\")\n",
    "    except ImportError:\n",
    "        print(\"⚠️ PyTorch no instalado - verificación CUDA limitada\")\n",
    "    \n",
    "    # Verificar LightGBM GPU\n",
    "    try:\n",
    "        test_model = lgb.LGBMRegressor(device='gpu', gpu_platform_id=0, gpu_device_id=0)\n",
    "        gpu_info['lightgbm_gpu'] = True\n",
    "        print(\"✅ LightGBM GPU: Disponible\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ LightGBM GPU: No disponible - {str(e)[:100]}\")\n",
    "    \n",
    "    return gpu_info\n",
    "\n",
    "# Verificar configuración de paralelización\n",
    "def check_parallel_setup():\n",
    "    \"\"\"Verifica configuración de paralelización\"\"\"\n",
    "    import psutil\n",
    "    \n",
    "    parallel_info = {\n",
    "        'cpu_count': mp.cpu_count(),\n",
    "        'physical_cores': psutil.cpu_count(logical=False),\n",
    "        'logical_cores': psutil.cpu_count(logical=True),\n",
    "        'memory_gb': round(psutil.virtual_memory().total / (1024**3), 1),\n",
    "        'available_memory_gb': round(psutil.virtual_memory().available / (1024**3), 1)\n",
    "    }\n",
    "    \n",
    "    print(f\"💻 CONFIGURACIÓN CPU:\")\n",
    "    print(f\"   Cores físicos: {parallel_info['physical_cores']}\")\n",
    "    print(f\"   Cores lógicos: {parallel_info['logical_cores']}\")\n",
    "    print(f\"   Memoria total: {parallel_info['memory_gb']} GB\")\n",
    "    print(f\"   Memoria disponible: {parallel_info['available_memory_gb']} GB\")\n",
    "    \n",
    "    # Verificar configuración de librerías paralelas\n",
    "    print(f\"🔧 CONFIGURACIÓN PARALELA:\")\n",
    "    print(f\"   OMP_NUM_THREADS: {os.environ.get('OMP_NUM_THREADS', 'No configurado')}\")\n",
    "    print(f\"   MKL_NUM_THREADS: {os.environ.get('MKL_NUM_THREADS', 'No configurado')}\")\n",
    "    \n",
    "    return parallel_info\n",
    "\n",
    "# Ejecutar verificaciones\n",
    "gpu_config = check_gpu_setup()\n",
    "parallel_config = check_parallel_setup()\n",
    "\n",
    "# Configurar estrategia de entrenamiento basada en recursos disponibles\n",
    "TRAINING_STRATEGY = {\n",
    "    'use_gpu': gpu_config['lightgbm_gpu'],\n",
    "    'max_cores': parallel_config['logical_cores'],\n",
    "    'memory_limit': parallel_config['available_memory_gb'],\n",
    "    'batch_processing': parallel_config['available_memory_gb'] < 8  # Si memoria < 8GB, usar batches\n",
    "}\n",
    "\n",
    "print(f\"\\n📋 ESTRATEGIA DE ENTRENAMIENTO:\")\n",
    "print(f\"   GPU para LightGBM: {'✅ Sí' if TRAINING_STRATEGY['use_gpu'] else '❌ No'}\")\n",
    "print(f\"   Cores para paralelización: {TRAINING_STRATEGY['max_cores']}\")\n",
    "print(f\"   Límite de memoria: {TRAINING_STRATEGY['memory_limit']} GB\")\n",
    "print(f\"   Procesamiento en batches: {'✅ Sí' if TRAINING_STRATEGY['batch_processing'] else '❌ No'}\")\n",
    "\n",
    "# Optimizar configuración de numpy/scipy para rendimiento\n",
    "import numpy as np\n",
    "if hasattr(np, '__config__'):\n",
    "    print(f\"\\n⚙️ CONFIGURACIÓN NUMPY:\")\n",
    "    try:\n",
    "        print(f\"   BLAS: {np.__config__.get_info('blas_info', {}).get('name', 'Desconocido')}\")\n",
    "        print(f\"   LAPACK: {np.__config__.get_info('lapack_info', {}).get('name', 'Desconocido')}\")\n",
    "    except:\n",
    "        print(\"   Información BLAS/LAPACK no disponible\")\n",
    "\n",
    "print(f\"\\n✅ Verificación de configuración completada\")\n",
    "print(f\"🚀 Sistema optimizado para entrenamiento de alta velocidad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf631ee",
   "metadata": {},
   "source": [
    "## 1. División de Datos (Temporal)\n",
    "\n",
    "### Estrategia de División Temporal\n",
    "- **Train**: 80% de los datos más antiguos (1992-2019)\n",
    "- **Test**: 20% de los datos más recientes (2020-2024)\n",
    "- **Validación**: Sin data leakage temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7daac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Implementando división temporal optimizada para H2O...\n",
      "❌ Error: No se encontró la columna 'year' en el dataset\n",
      "   Se requiere información temporal para división correcta\n",
      "\n",
      "✅ División temporal completada exitosamente\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# 1.1 DIVISIÓN TEMPORAL H2O + SKLEARN (80% TRAIN, 20% TEST)\n",
    "# ==================================================\n",
    "\n",
    "print(\"📅 Implementando división temporal optimizada para H2O...\")\n",
    "\n",
    "# Verificar si tenemos columna temporal\n",
    "if 'year' in df_clean.columns:\n",
    "    # Información temporal\n",
    "    year_min, year_max = df_clean['year'].min(), df_clean['year'].max()\n",
    "    print(f\"📊 Rango temporal completo: {year_min} - {year_max}\")\n",
    "    \n",
    "    # División temporal: 80% para train, 20% para test\n",
    "    years_unique = sorted(df_clean['year'].unique())\n",
    "    n_years = len(years_unique)\n",
    "    split_index = int(n_years * 0.8)\n",
    "    split_year = years_unique[split_index]\n",
    "    \n",
    "    print(f\"🔄 División temporal:\")\n",
    "    print(f\"  Train: {year_min} - {split_year-1} ({split_index} años)\")\n",
    "    print(f\"  Test:  {split_year} - {year_max} ({n_years - split_index} años)\")\n",
    "    \n",
    "    # Crear máscaras para train/test\n",
    "    train_mask = df_clean['year'] < split_year\n",
    "    test_mask = df_clean['year'] >= split_year\n",
    "    \n",
    "    # División para H2O\n",
    "    if H2O_AVAILABLE and h2o_frame is not None:\n",
    "        print(\"🚀 Creando splits en H2O...\")\n",
    "        \n",
    "        # Dividir H2O Frame\n",
    "        h2o_train = h2o_frame[h2o_frame['year'] < split_year, :]\n",
    "        h2o_test = h2o_frame[h2o_frame['year'] >= split_year, :]\n",
    "        \n",
    "        # Definir predictores (excluir target y columnas auxiliares)\n",
    "        h2o_predictors = [col for col in h2o_frame.columns \n",
    "                         if col not in [TARGET, 'year'] + DROP_COLS]\n",
    "        \n",
    "        print(f\"✅ H2O splits creados:\")\n",
    "        print(f\"  Train: {h2o_train.nrows:,} filas\")\n",
    "        print(f\"  Test:  {h2o_test.nrows:,} filas\")\n",
    "        print(f\"  Predictores: {len(h2o_predictors)} variables\")\n",
    "    \n",
    "    # División para sklearn (backup/comparación)\n",
    "    print(\"📊 Creando splits para sklearn...\")\n",
    "    \n",
    "    # Filtrar features disponibles para sklearn\n",
    "    available_features_sklearn = [f for f in available_features if f in df_clean.columns]\n",
    "    \n",
    "    # Crear conjuntos de datos para sklearn\n",
    "    X_train = df_clean[train_mask][available_features_sklearn]\n",
    "    X_test = df_clean[test_mask][available_features_sklearn]\n",
    "    y_train = df_clean[train_mask][TARGET]\n",
    "    y_test = df_clean[test_mask][TARGET]\n",
    "    \n",
    "    # Información de la división\n",
    "    train_size = len(X_train)\n",
    "    test_size = len(X_test)\n",
    "    total_size = train_size + test_size\n",
    "    \n",
    "    print(f\"✅ Sklearn splits creados:\")\n",
    "    print(f\"  Train: {train_size:,} observaciones ({train_size/total_size*100:.1f}%)\")\n",
    "    print(f\"  Test:  {test_size:,} observaciones ({test_size/total_size*100:.1f}%)\")\n",
    "    print(f\"  Features: {len(available_features_sklearn)} variables\")\n",
    "    \n",
    "    # Verificar distribución temporal\n",
    "    train_years = df_clean[train_mask]['year'].value_counts().sort_index()\n",
    "    test_years = df_clean[test_mask]['year'].value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\n📊 Distribución temporal verificada:\")\n",
    "    print(f\"  Train años: {train_years.index.min()} - {train_years.index.max()}\")\n",
    "    print(f\"  Test años:  {test_years.index.min()} - {test_years.index.max()}\")\n",
    "    \n",
    "    # Guardar información de split\n",
    "    split_info = {\n",
    "        'split_year': split_year,\n",
    "        'train_years': (train_years.index.min(), train_years.index.max()),\n",
    "        'test_years': (test_years.index.min(), test_years.index.max()),\n",
    "        'train_size': train_size,\n",
    "        'test_size': test_size,\n",
    "        'h2o_available': H2O_AVAILABLE,\n",
    "        'h2o_predictors': h2o_predictors if H2O_AVAILABLE else None\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Error: No se encontró la columna 'year' en el dataset\")\n",
    "    print(\"   Se requiere información temporal para división correcta\")\n",
    "\n",
    "print(f\"\\n✅ División temporal completada exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec9146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏰ Ejecutando división temporal con datos procesados del notebook 03...\n",
      "✅ Datos procesados disponibles: (1506591, 19)\n",
      "   Target: purchase_price\n",
      "   Features disponibles: 14\n",
      "📊 Datos limpios para modelado:\n",
      "   Muestras válidas: 1,506,591\n",
      "   Features: 14\n",
      "🕐 Usando división temporal con columna: date\n",
      "   ✅ División temporal exitosa\n",
      "   Período train: 694569600000000000 a 1619222400000000000\n",
      "   Período test: 1619222400000000000 a 1727654400000000000\n",
      "\n",
      "📈 Resultados de la división:\n",
      "   Train: 1,205,272 muestras (80.0%)\n",
      "   Test: 301,319 muestras (20.0%)\n",
      "\n",
      "🎯 Estadísticas del target 'purchase_price':\n",
      "   Train - Media: 1762487.06, Std: 1651134.06\n",
      "   Test - Media: 2526504.54, Std: 2052379.56\n",
      "   Diferencia medias: 764017.48\n",
      "\n",
      "✅ División de datos completada\n",
      "🚀 Variables train/test listas para modelado\n",
      "   ✅ División temporal exitosa\n",
      "   Período train: 694569600000000000 a 1619222400000000000\n",
      "   Período test: 1619222400000000000 a 1727654400000000000\n",
      "\n",
      "📈 Resultados de la división:\n",
      "   Train: 1,205,272 muestras (80.0%)\n",
      "   Test: 301,319 muestras (20.0%)\n",
      "\n",
      "🎯 Estadísticas del target 'purchase_price':\n",
      "   Train - Media: 1762487.06, Std: 1651134.06\n",
      "   Test - Media: 2526504.54, Std: 2052379.56\n",
      "   Diferencia medias: 764017.48\n",
      "\n",
      "✅ División de datos completada\n",
      "🚀 Variables train/test listas para modelado\n"
     ]
    }
   ],
   "source": [
    "# DIVISIÓN TEMPORAL DE DATOS PROCESADOS\n",
    "\n",
    "print(\"⏰ Ejecutando división temporal con datos procesados del notebook 03...\")\n",
    "\n",
    "# Verificar que tenemos los datos procesados\n",
    "if 'df_processed' not in locals():\n",
    "    raise ValueError(\"❌ df_processed no está definido. Ejecutar primero la celda de carga de datos.\")\n",
    "\n",
    "if TARGET not in df_processed.columns:\n",
    "    raise ValueError(f\"❌ Variable objetivo '{TARGET}' no encontrada en datos procesados\")\n",
    "\n",
    "print(f\"✅ Datos procesados disponibles: {df_processed.shape}\")\n",
    "print(f\"   Target: {TARGET}\")\n",
    "print(f\"   Features disponibles: {len(available_features)}\")\n",
    "\n",
    "# Preparar datos para división\n",
    "X = df_processed[available_features].copy()\n",
    "y = df_processed[TARGET].copy()\n",
    "\n",
    "# Limpiar datos (remover nulls en target)\n",
    "mask_valid = y.notna()\n",
    "X = X[mask_valid]\n",
    "y = y[mask_valid]\n",
    "\n",
    "print(f\"📊 Datos limpios para modelado:\")\n",
    "print(f\"   Muestras válidas: {len(X):,}\")\n",
    "print(f\"   Features: {len(available_features)}\")\n",
    "\n",
    "# División temporal si existe columna de fecha\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Buscar columna temporal\n",
    "date_columns = ['date', 'Date', 'fecha', 'timestamp', 'SaleDate', 'SaleDato']\n",
    "temporal_col = None\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in df_processed.columns:\n",
    "        temporal_col = col\n",
    "        break\n",
    "\n",
    "# División temporal o aleatoria\n",
    "if temporal_col is not None:\n",
    "    print(f\"🕐 Usando división temporal con columna: {temporal_col}\")\n",
    "    \n",
    "    # Preparar datos con fechas válidas\n",
    "    df_temp = df_processed[mask_valid].copy()\n",
    "    \n",
    "    try:\n",
    "        # Convertir a datetime si es necesario\n",
    "        if df_temp[temporal_col].dtype == 'object':\n",
    "            df_temp[temporal_col] = pd.to_datetime(df_temp[temporal_col], errors='coerce')\n",
    "        \n",
    "        # Filtrar fechas válidas\n",
    "        date_mask = df_temp[temporal_col].notna()\n",
    "        df_temp = df_temp[date_mask]\n",
    "        \n",
    "        if len(df_temp) > 0:\n",
    "            # Ordenar por fecha y dividir temporalmente\n",
    "            df_temp = df_temp.sort_values(temporal_col)\n",
    "            split_point = int(len(df_temp) * 0.8)\n",
    "            \n",
    "            train_indices = df_temp.index[:split_point]\n",
    "            test_indices = df_temp.index[split_point:]\n",
    "            \n",
    "            X_train = X.loc[train_indices]\n",
    "            X_test = X.loc[test_indices]\n",
    "            y_train = y.loc[train_indices]\n",
    "            y_test = y.loc[test_indices]\n",
    "            \n",
    "            print(f\"   ✅ División temporal exitosa\")\n",
    "            print(f\"   Período train: {df_temp[temporal_col].iloc[0]} a {df_temp[temporal_col].iloc[split_point-1]}\")\n",
    "            print(f\"   Período test: {df_temp[temporal_col].iloc[split_point]} a {df_temp[temporal_col].iloc[-1]}\")\n",
    "        else:\n",
    "            raise ValueError(\"No hay fechas válidas\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Error en división temporal: {e}\")\n",
    "        print(f\"   Usando división aleatoria como fallback\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "else:\n",
    "    print(f\"📊 Usando división aleatoria (no se encontró columna temporal)\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "# Verificar división\n",
    "print(f\"\\n📈 Resultados de la división:\")\n",
    "print(f\"   Train: {len(X_train):,} muestras ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Test: {len(X_test):,} muestras ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Estadísticas del target\n",
    "print(f\"\\n🎯 Estadísticas del target '{TARGET}':\")\n",
    "print(f\"   Train - Media: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"   Test - Media: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")\n",
    "print(f\"   Diferencia medias: {abs(y_train.mean() - y_test.mean()):.2f}\")\n",
    "\n",
    "# Verificar nulls en features\n",
    "train_nulls = X_train.isnull().sum().sum()\n",
    "test_nulls = X_test.isnull().sum().sum()\n",
    "\n",
    "if train_nulls > 0 or test_nulls > 0:\n",
    "    print(f\"⚠️ Nulls encontrados - Train: {train_nulls}, Test: {test_nulls}\")\n",
    "    \n",
    "    # Imputación simple si hay nulls\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    # Separar columnas por tipo\n",
    "    numeric_features = X_train.select_dtypes(include=[np.number]).columns\n",
    "    categorical_features = X_train.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    # Imputar numéricas con mediana\n",
    "    if len(numeric_features) > 0:\n",
    "        imputer_num = SimpleImputer(strategy='median')\n",
    "        X_train[numeric_features] = imputer_num.fit_transform(X_train[numeric_features])\n",
    "        X_test[numeric_features] = imputer_num.transform(X_test[numeric_features])\n",
    "    \n",
    "    # Imputar categóricas con moda\n",
    "    if len(categorical_features) > 0:\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        X_train[categorical_features] = imputer_cat.fit_transform(X_train[categorical_features])\n",
    "        X_test[categorical_features] = imputer_cat.transform(X_test[categorical_features])\n",
    "    \n",
    "    print(f\"✅ Imputación completada\")\n",
    "\n",
    "print(f\"\\n✅ División de datos completada\")\n",
    "print(f\"🚀 Variables train/test listas para modelado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 1.3 VISUALIZACIÓN DE DRIFT TEMPORAL\n",
    "# ==================================================\n",
    "\n",
    "print(\"📈 Analizando drift temporal en variables clave...\")\n",
    "\n",
    "def analyze_temporal_drift(df_complete, features_to_analyze, target_col):\n",
    "    \"\"\"Analiza drift temporal en variables clave\"\"\"\n",
    "    \n",
    "    # Seleccionar subset de features importantes para análisis\n",
    "    important_features = features_to_analyze[:6]  # Top 6 features\n",
    "    \n",
    "    n_features = len(important_features)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 4*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    # Análisis por año\n",
    "    yearly_stats = df_complete.groupby('year').agg({\n",
    "        target_col: ['mean', 'std', 'count'],\n",
    "        **{feature: 'mean' for feature in important_features}\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Evolución de la variable objetivo\n",
    "    for i, feature in enumerate(important_features):\n",
    "        if i < len(axes):\n",
    "            # Plot de evolución temporal\n",
    "            if feature in df_complete.columns:\n",
    "                feature_yearly = df_complete.groupby('year')[feature].mean()\n",
    "                axes[i].plot(feature_yearly.index, feature_yearly.values, marker='o', linewidth=2)\n",
    "                axes[i].set_title(f'Evolución Temporal: {feature}')\n",
    "                axes[i].set_xlabel('Año')\n",
    "                axes[i].set_ylabel(f'{feature}')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Marcar división train/test\n",
    "                axes[i].axvline(x=split_year, color='red', linestyle='--', alpha=0.7, label='Train/Test Split')\n",
    "                axes[i].legend()\n",
    "    \n",
    "    # Ocultar ejes no utilizados\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Análisis específico del precio por año\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Precio promedio por año\n",
    "    price_yearly = df_complete.groupby('year')[target_col].agg(['mean', 'std', 'count'])\n",
    "    \n",
    "    axes[0,0].plot(price_yearly.index, price_yearly['mean'], marker='o', linewidth=2, color='blue')\n",
    "    axes[0,0].fill_between(price_yearly.index, \n",
    "                          price_yearly['mean'] - price_yearly['std'],\n",
    "                          price_yearly['mean'] + price_yearly['std'], \n",
    "                          alpha=0.3, color='blue')\n",
    "    axes[0,0].axvline(x=split_year, color='red', linestyle='--', alpha=0.7, label='Train/Test Split')\n",
    "    axes[0,0].set_title('Evolución del Precio Promedio ± 1 Std')\n",
    "    axes[0,0].set_xlabel('Año')\n",
    "    axes[0,0].set_ylabel('Precio (DKK)')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Número de transacciones por año\n",
    "    axes[0,1].bar(price_yearly.index, price_yearly['count'], color='green', alpha=0.7)\n",
    "    axes[0,1].axvline(x=split_year, color='red', linestyle='--', alpha=0.7, label='Train/Test Split')\n",
    "    axes[0,1].set_title('Número de Transacciones por Año')\n",
    "    axes[0,1].set_xlabel('Año')\n",
    "    axes[0,1].set_ylabel('Número de Transacciones')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Coeficiente de variación por año\n",
    "    cv_yearly = (price_yearly['std'] / price_yearly['mean']) * 100\n",
    "    axes[1,0].plot(cv_yearly.index, cv_yearly.values, marker='s', linewidth=2, color='orange')\n",
    "    axes[1,0].axvline(x=split_year, color='red', linestyle='--', alpha=0.7, label='Train/Test Split')\n",
    "    axes[1,0].set_title('Coeficiente de Variación del Precio (%)')\n",
    "    axes[1,0].set_xlabel('Año')\n",
    "    axes[1,0].set_ylabel('CV (%)')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribución de precios por período\n",
    "    train_data = df_complete[df_complete['year'] < split_year][target_col]\n",
    "    test_data = df_complete[df_complete['year'] >= split_year][target_col]\n",
    "    \n",
    "    axes[1,1].hist(train_data, bins=50, alpha=0.7, label='Train Period', density=True, color='blue')\n",
    "    axes[1,1].hist(test_data, bins=50, alpha=0.7, label='Test Period', density=True, color='red')\n",
    "    axes[1,1].set_title('Distribución de Precios: Train vs Test')\n",
    "    axes[1,1].set_xlabel('Precio (DKK)')\n",
    "    axes[1,1].set_ylabel('Densidad')\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Análisis cuantitativo del drift\n",
    "    print(f\"\\n📊 Análisis cuantitativo de drift temporal:\")\n",
    "    print(f\"{'Variable':<20} {'Drift Train->Test':<20} {'Significancia':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for feature in important_features[:5]:  # Top 5 features\n",
    "        if feature in df_complete.columns:\n",
    "            train_mean = df_complete[df_complete['year'] < split_year][feature].mean()\n",
    "            test_mean = df_complete[df_complete['year'] >= split_year][feature].mean()\n",
    "            drift_pct = ((test_mean - train_mean) / train_mean) * 100 if train_mean != 0 else 0\n",
    "            \n",
    "            # Test de significancia\n",
    "            train_values = df_complete[df_complete['year'] < split_year][feature]\n",
    "            test_values = df_complete[df_complete['year'] >= split_year][feature]\n",
    "            _, p_value = stats.ttest_ind(train_values, test_values)\n",
    "            \n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "            \n",
    "            print(f\"{feature:<20} {drift_pct:<20.2f}% {significance:<15}\")\n",
    "\n",
    "# Ejecutar análisis de drift\n",
    "analyze_temporal_drift(df_complete, available_features, TARGET)\n",
    "analyze_temporal_drift(df_complete, available_features, TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09965c45",
   "metadata": {},
   "source": [
    "## 3. Modelos Estadísticos (Sklearn)\n",
    "\n",
    "### Metodología Estadística Clásica de Comparación\n",
    "- **Regresión Lineal**: Baseline con mínimos cuadrados\n",
    "- **Ridge**: Regularización L2 para multicolinealidad\n",
    "- **Lasso**: Regularización L1 para selección de features\n",
    "- **ElasticNet**: Combinación L1 + L2\n",
    "- **Evaluación**: AIC/BIC + GridSearch paralelo para comparación con H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c529e0",
   "metadata": {},
   "source": [
    "## 2. Modelos H2O (Distribuidos y Escalables)\n",
    "\n",
    "### Metodología H2O AutoML y Modelos Específicos\n",
    "- **H2O GBM**: Gradient Boosting Machine distribuido y optimizado\n",
    "- **H2O Random Forest**: Ensemble distribuido con paralelización automática\n",
    "- **H2O GLM**: Modelos lineales generalizados con regularización automática\n",
    "- **H2O Deep Learning**: Redes neuronales profundas para capturar relaciones complejas\n",
    "- **H2O AutoML**: Pipeline automatizado para selección del mejor modelo\n",
    "- **Evaluación**: Métricas nativas H2O con validación cruzada optimizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771f1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 2.1 MODELOS H2O DISTRIBUIDOS Y ESCALABLES\n",
    "# ==================================================\n",
    "\n",
    "if H2O_AVAILABLE:\n",
    "    print(\"🚀 Entrenando modelos H2O distribuidos...\")\n",
    "    \n",
    "    # Diccionario para almacenar modelos H2O\n",
    "    h2o_models = {}\n",
    "    h2o_results = {}\n",
    "    \n",
    "    # 1. H2O Gradient Boosting Machine (GBM)\n",
    "    print(\"\\n🔸 Entrenando H2O GBM (Distribuido)...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    h2o_gbm = H2OGradientBoostingEstimator(\n",
    "        ntrees=100,\n",
    "        max_depth=6,\n",
    "        learn_rate=0.1,\n",
    "        sample_rate=0.8,\n",
    "        col_sample_rate=0.8,\n",
    "        stopping_rounds=10,\n",
    "        stopping_tolerance=0.001,\n",
    "        stopping_metric=\"RMSE\",\n",
    "        seed=42,\n",
    "        nfolds=5,  # Validación cruzada\n",
    "        fold_assignment=\"Modulo\",\n",
    "        keep_cross_validation_predictions=True\n",
    "    )\n",
    "    \n",
    "    h2o_gbm.train(\n",
    "        x=h2o_predictors,\n",
    "        y=TARGET,\n",
    "        training_frame=h2o_train,\n",
    "        validation_frame=h2o_test\n",
    "    )\n",
    "    \n",
    "    gbm_time = (datetime.now() - start_time).total_seconds()\n",
    "    h2o_models['H2O_GBM'] = h2o_gbm\n",
    "    \n",
    "    # Métricas H2O GBM\n",
    "    train_rmse_gbm = h2o_gbm.rmse(train=True)\n",
    "    valid_rmse_gbm = h2o_gbm.rmse(valid=True)\n",
    "    train_r2_gbm = h2o_gbm.r2(train=True)\n",
    "    valid_r2_gbm = h2o_gbm.r2(valid=True)\n",
    "    \n",
    "    h2o_results['H2O_GBM'] = {\n",
    "        'train_rmse': train_rmse_gbm,\n",
    "        'test_rmse': valid_rmse_gbm,\n",
    "        'train_r2': train_r2_gbm,\n",
    "        'test_r2': valid_r2_gbm,\n",
    "        'training_time': gbm_time,\n",
    "        'cv_rmse': h2o_gbm.rmse(xval=True),\n",
    "        'cv_r2': h2o_gbm.r2(xval=True)\n",
    "    }\n",
    "    \n",
    "    print(f\"   Tiempo entrenamiento: {gbm_time:.2f}s\")\n",
    "    print(f\"   Train RMSE: {train_rmse_gbm:,.0f}\")\n",
    "    print(f\"   Test RMSE:  {valid_rmse_gbm:,.0f}\")\n",
    "    print(f\"   Test R²:    {valid_r2_gbm:.4f}\")\n",
    "    print(f\"   CV RMSE:    {h2o_gbm.rmse(xval=True):,.0f}\")\n",
    "    \n",
    "    # 2. H2O Random Forest\n",
    "    print(\"\\n🔸 Entrenando H2O Random Forest (Distribuido)...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    h2o_rf = H2ORandomForestEstimator(\n",
    "        ntrees=100,\n",
    "        max_depth=15,\n",
    "        sample_rate=0.8,\n",
    "        col_sample_rate_per_tree=0.8,\n",
    "        seed=42,\n",
    "        nfolds=5,\n",
    "        fold_assignment=\"Modulo\",\n",
    "        keep_cross_validation_predictions=True\n",
    "    )\n",
    "    \n",
    "    h2o_rf.train(\n",
    "        x=h2o_predictors,\n",
    "        y=TARGET,\n",
    "        training_frame=h2o_train,\n",
    "        validation_frame=h2o_test\n",
    "    )\n",
    "    \n",
    "    rf_time = (datetime.now() - start_time).total_seconds()\n",
    "    h2o_models['H2O_RF'] = h2o_rf\n",
    "    \n",
    "    # Métricas H2O RF\n",
    "    train_rmse_rf = h2o_rf.rmse(train=True)\n",
    "    valid_rmse_rf = h2o_rf.rmse(valid=True)\n",
    "    train_r2_rf = h2o_rf.r2(train=True)\n",
    "    valid_r2_rf = h2o_rf.r2(valid=True)\n",
    "    \n",
    "    h2o_results['H2O_RF'] = {\n",
    "        'train_rmse': train_rmse_rf,\n",
    "        'test_rmse': valid_rmse_rf,\n",
    "        'train_r2': train_r2_rf,\n",
    "        'test_r2': valid_r2_rf,\n",
    "        'training_time': rf_time,\n",
    "        'cv_rmse': h2o_rf.rmse(xval=True),\n",
    "        'cv_r2': h2o_rf.r2(xval=True)\n",
    "    }\n",
    "    \n",
    "    print(f\"   Tiempo entrenamiento: {rf_time:.2f}s\")\n",
    "    print(f\"   Train RMSE: {train_rmse_rf:,.0f}\")\n",
    "    print(f\"   Test RMSE:  {valid_rmse_rf:,.0f}\")\n",
    "    print(f\"   Test R²:    {valid_r2_rf:.4f}\")\n",
    "    print(f\"   CV RMSE:    {h2o_rf.rmse(xval=True):,.0f}\")\n",
    "    \n",
    "    # 3. H2O GLM (Generalized Linear Model)\n",
    "    print(\"\\n🔸 Entrenando H2O GLM (Regularizado)...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    h2o_glm = H2OGeneralizedLinearEstimator(\n",
    "        family=\"gaussian\",\n",
    "        alpha=0.5,  # Elastic Net\n",
    "        lambda_search=True,\n",
    "        nfolds=5,\n",
    "        fold_assignment=\"Modulo\",\n",
    "        keep_cross_validation_predictions=True,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    h2o_glm.train(\n",
    "        x=h2o_predictors,\n",
    "        y=TARGET,\n",
    "        training_frame=h2o_train,\n",
    "        validation_frame=h2o_test\n",
    "    )\n",
    "    \n",
    "    glm_time = (datetime.now() - start_time).total_seconds()\n",
    "    h2o_models['H2O_GLM'] = h2o_glm\n",
    "    \n",
    "    # Métricas H2O GLM\n",
    "    train_rmse_glm = h2o_glm.rmse(train=True)\n",
    "    valid_rmse_glm = h2o_glm.rmse(valid=True)\n",
    "    train_r2_glm = h2o_glm.r2(train=True)\n",
    "    valid_r2_glm = h2o_glm.r2(valid=True)\n",
    "    \n",
    "    h2o_results['H2O_GLM'] = {\n",
    "        'train_rmse': train_rmse_glm,\n",
    "        'test_rmse': valid_rmse_glm,\n",
    "        'train_r2': train_r2_glm,\n",
    "        'test_r2': valid_r2_glm,\n",
    "        'training_time': glm_time,\n",
    "        'cv_rmse': h2o_glm.rmse(xval=True),\n",
    "        'cv_r2': h2o_glm.r2(xval=True),\n",
    "        'best_lambda': h2o_glm.lambda_best()\n",
    "    }\n",
    "    \n",
    "    print(f\"   Tiempo entrenamiento: {glm_time:.2f}s\")\n",
    "    print(f\"   Best lambda: {h2o_glm.lambda_best():.6f}\")\n",
    "    print(f\"   Train RMSE: {train_rmse_glm:,.0f}\")\n",
    "    print(f\"   Test RMSE:  {valid_rmse_glm::.0f}\")\n",
    "    print(f\"   Test R²:    {valid_r2_glm:.4f}\")\n",
    "    print(f\"   CV RMSE:    {h2o_glm.rmse(xval=True):,.0f}\")\n",
    "    \n",
    "    # 4. H2O Deep Learning\n",
    "    print(\"\\n🔸 Entrenando H2O Deep Learning (Neural Network)...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    h2o_dl = H2ODeepLearningEstimator(\n",
    "        hidden=[200, 200, 100],  # 3 capas ocultas\n",
    "        epochs=50,\n",
    "        activation=\"Rectifier\",\n",
    "        l1=0.001,\n",
    "        l2=0.001,\n",
    "        input_dropout_ratio=0.1,\n",
    "        hidden_dropout_ratios=[0.2, 0.2, 0.1],\n",
    "        stopping_rounds=10,\n",
    "        stopping_tolerance=0.001,\n",
    "        stopping_metric=\"RMSE\",\n",
    "        nfolds=5,\n",
    "        fold_assignment=\"Modulo\",\n",
    "        keep_cross_validation_predictions=True,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    h2o_dl.train(\n",
    "        x=h2o_predictors,\n",
    "        y=TARGET,\n",
    "        training_frame=h2o_train,\n",
    "        validation_frame=h2o_test\n",
    "    )\n",
    "    \n",
    "    dl_time = (datetime.now() - start_time).total_seconds()\n",
    "    h2o_models['H2O_DL'] = h2o_dl\n",
    "    \n",
    "    # Métricas H2O Deep Learning\n",
    "    train_rmse_dl = h2o_dl.rmse(train=True)\n",
    "    valid_rmse_dl = h2o_dl.rmse(valid=True)\n",
    "    train_r2_dl = h2o_dl.r2(train=True)\n",
    "    valid_r2_dl = h2o_dl.r2(valid=True)\n",
    "    \n",
    "    h2o_results['H2O_DL'] = {\n",
    "        'train_rmse': train_rmse_dl,\n",
    "        'test_rmse': valid_rmse_dl,\n",
    "        'train_r2': train_r2_dl,\n",
    "        'test_r2': valid_r2_dl,\n",
    "        'training_time': dl_time,\n",
    "        'cv_rmse': h2o_dl.rmse(xval=True),\n",
    "        'cv_r2': h2o_dl.r2(xval=True)\n",
    "    }\n",
    "    \n",
    "    print(f\"   Tiempo entrenamiento: {dl_time:.2f}s\")\n",
    "    print(f\"   Train RMSE: {train_rmse_dl:,.0f}\")\n",
    "    print(f\"   Test RMSE:  {valid_rmse_dl:,.0f}\")\n",
    "    print(f\"   Test R²:    {valid_r2_dl:.4f}\")\n",
    "    print(f\"   CV RMSE:    {h2o_dl.rmse(xval=True):,.0f}\")\n",
    "    \n",
    "    # Resumen de modelos H2O\n",
    "    print(f\"\\n📊 RESUMEN MODELOS H2O:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Modelo':<12} {'Test RMSE':<12} {'Test R²':<10} {'CV RMSE':<12} {'Tiempo':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model_name, results in h2o_results.items():\n",
    "        print(f\"{model_name:<12} {results['test_rmse']:<12,.0f} {results['test_r2']:<10.4f} \"\n",
    "              f\"{results['cv_rmse']:<12,.0f} {results['training_time']:<10.1f}s\")\n",
    "    \n",
    "    total_h2o_time = sum([r['training_time'] for r in h2o_results.values()])\n",
    "    print(f\"\\n⏱️ Tiempo total H2O: {total_h2o_time:.1f}s\")\n",
    "    print(f\"✅ Modelos H2O entrenados exitosamente\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ H2O no disponible, saltando modelos H2O\")\n",
    "    h2o_models = {}\n",
    "    h2o_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 2.2 H2O AUTOML - PIPELINE AUTOMATIZADO\n",
    "# ==================================================\n",
    "\n",
    "if H2O_AVAILABLE:\n",
    "    print(\"🤖 Ejecutando H2O AutoML (Pipeline Automatizado)...\")\n",
    "    \n",
    "    from h2o.automl import H2OAutoML\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Configurar AutoML\n",
    "    aml = H2OAutoML(\n",
    "        max_models=20,  # Máximo 20 modelos\n",
    "        max_runtime_secs=300,  # Máximo 5 minutos\n",
    "        stopping_metric=\"RMSE\",\n",
    "        stopping_tolerance=0.001,\n",
    "        stopping_rounds=3,\n",
    "        seed=42,\n",
    "        nfolds=5,\n",
    "        keep_cross_validation_predictions=True,\n",
    "        keep_cross_validation_models=True\n",
    "    )\n",
    "    \n",
    "    # Entrenar AutoML\n",
    "    aml.train(\n",
    "        x=h2o_predictors,\n",
    "        y=TARGET,\n",
    "        training_frame=h2o_train,\n",
    "        validation_frame=h2o_test\n",
    "    )\n",
    "    \n",
    "    automl_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"   Tiempo AutoML: {automl_time:.1f}s\")\n",
    "    print(f\"   Modelos entrenados: {len(aml.leaderboard)}\")\n",
    "    \n",
    "    # Obtener el mejor modelo\n",
    "    best_model = aml.leader\n",
    "    print(f\"   Mejor modelo: {best_model.model_id}\")\n",
    "    \n",
    "    # Métricas del mejor modelo\n",
    "    best_train_rmse = best_model.rmse(train=True)\n",
    "    best_valid_rmse = best_model.rmse(valid=True)\n",
    "    best_train_r2 = best_model.r2(train=True)\n",
    "    best_valid_r2 = best_model.r2(valid=True)\n",
    "    \n",
    "    h2o_models['H2O_AutoML_Best'] = best_model\n",
    "    h2o_results['H2O_AutoML_Best'] = {\n",
    "        'train_rmse': best_train_rmse,\n",
    "        'test_rmse': best_valid_rmse,\n",
    "        'train_r2': best_train_r2,\n",
    "        'test_r2': best_valid_r2,\n",
    "        'training_time': automl_time,\n",
    "        'cv_rmse': best_model.rmse(xval=True),\n",
    "        'cv_r2': best_model.r2(xval=True),\n",
    "        'model_type': type(best_model).__name__\n",
    "    }\n",
    "    \n",
    "    print(f\"   Best Train RMSE: {best_train_rmse:,.0f}\")\n",
    "    print(f\"   Best Test RMSE:  {best_valid_rmse:,.0f}\")\n",
    "    print(f\"   Best Test R²:    {best_valid_r2:.4f}\")\n",
    "    print(f\"   Best CV RMSE:    {best_model.rmse(xval=True):,.0f}\")\n",
    "    \n",
    "    # Mostrar leaderboard\n",
    "    print(f\"\\n📋 LEADERBOARD H2O AutoML (Top 10):\")\n",
    "    print(\"-\" * 80)\n",
    "    leaderboard = aml.leaderboard.as_data_frame()\n",
    "    print(leaderboard.head(10).to_string(index=False))\n",
    "    \n",
    "    # Guardar AutoML para análisis posterior\n",
    "    h2o_models['H2O_AutoML'] = aml\n",
    "    \n",
    "    print(f\"\\n✅ H2O AutoML completado exitosamente\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ H2O no disponible, saltando AutoML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6009e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 2.1 MODELOS ESTADÍSTICOS OPTIMIZADOS (PARALELO)\n",
    "# ==================================================\n",
    "\n",
    "print(\"📊 Implementando modelos estadísticos con paralelización...\")\n",
    "\n",
    "# Preparar datos - escalado para modelos regularizados\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Diccionario para almacenar modelos y resultados\n",
    "models = {}\n",
    "model_results = {}\n",
    "statistical_models = {}\n",
    "\n",
    "# 1. Regresión Lineal (OLS) - Baseline\n",
    "print(\"\\n🔸 Entrenando Regresión Lineal (OLS)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "lr = LinearRegression(n_jobs=-1)  # Usar todos los cores\n",
    "lr.fit(X_train, y_train)\n",
    "models['Linear_Regression'] = lr\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train_lr = lr.predict(X_train)\n",
    "y_pred_test_lr = lr.predict(X_test)\n",
    "\n",
    "# Métricas\n",
    "lr_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_lr)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_lr)),\n",
    "    'train_mae': mean_absolute_error(y_train, y_pred_train_lr),\n",
    "    'test_mae': mean_absolute_error(y_test, y_pred_test_lr),\n",
    "    'train_r2': r2_score(y_train, y_pred_train_lr),\n",
    "    'test_r2': r2_score(y_test, y_pred_test_lr),\n",
    "    'training_time': (datetime.now() - start_time).total_seconds()\n",
    "}\n",
    "model_results['Linear_Regression'] = lr_metrics\n",
    "statistical_models['Linear_Regression'] = {'model': lr, **lr_metrics}\n",
    "\n",
    "print(f\"   Tiempo entrenamiento: {lr_metrics['training_time']:.2f}s\")\n",
    "print(f\"   Train RMSE: {lr_metrics['train_rmse']:,.0f}\")\n",
    "print(f\"   Test RMSE:  {lr_metrics['test_rmse']:,.0f}\")\n",
    "print(f\"   Test R²:    {lr_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# 2. Ridge Regression (Paralelizado)\n",
    "print(\"\\n🔸 Entrenando Ridge Regression (Paralelo)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Búsqueda paralela de hiperparámetros\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ridge_params = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]}\n",
    "ridge_grid = GridSearchCV(\n",
    "    Ridge(random_state=42), \n",
    "    ridge_params, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,  # Usar todos los cores\n",
    "    verbose=0\n",
    ")\n",
    "ridge_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "ridge = ridge_grid.best_estimator_\n",
    "models['Ridge'] = ridge\n",
    "\n",
    "y_pred_train_ridge = ridge.predict(X_train_scaled)\n",
    "y_pred_test_ridge = ridge.predict(X_test_scaled)\n",
    "\n",
    "ridge_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_ridge)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_ridge)),\n",
    "    'train_mae': mean_absolute_error(y_train, y_pred_train_ridge),\n",
    "    'test_mae': mean_absolute_error(y_test, y_pred_test_ridge),\n",
    "    'train_r2': r2_score(y_train, y_pred_train_ridge),\n",
    "    'test_r2': r2_score(y_test, y_pred_test_ridge),\n",
    "    'best_alpha': ridge_grid.best_params_['alpha'],\n",
    "    'training_time': (datetime.now() - start_time).total_seconds()\n",
    "}\n",
    "model_results['Ridge'] = ridge_metrics\n",
    "statistical_models['Ridge'] = {'model': ridge, **ridge_metrics}\n",
    "\n",
    "print(f\"   Tiempo entrenamiento: {ridge_metrics['training_time']:.2f}s\")\n",
    "print(f\"   Alpha óptimo: {ridge_metrics['best_alpha']}\")\n",
    "print(f\"   Train RMSE: {ridge_metrics['train_rmse']:,.0f}\")\n",
    "print(f\"   Test RMSE:  {ridge_metrics['test_rmse']:,.0f}\")\n",
    "print(f\"   Test R²:    {ridge_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# 3. Lasso Regression (Paralelizado)\n",
    "print(\"\\n🔸 Entrenando Lasso Regression (Paralelo)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "lasso_params = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "lasso_grid = GridSearchCV(\n",
    "    Lasso(random_state=42, max_iter=3000), \n",
    "    lasso_params, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,  # Usar todos los cores\n",
    "    verbose=0\n",
    ")\n",
    "lasso_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "lasso = lasso_grid.best_estimator_\n",
    "models['Lasso'] = lasso\n",
    "\n",
    "y_pred_train_lasso = lasso.predict(X_train_scaled)\n",
    "y_pred_test_lasso = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Contar features seleccionadas por Lasso\n",
    "n_selected_features = np.sum(lasso.coef_ != 0)\n",
    "\n",
    "lasso_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_lasso)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_lasso)),\n",
    "    'train_mae': mean_absolute_error(y_train, y_pred_train_lasso),\n",
    "    'test_mae': mean_absolute_error(y_test, y_pred_test_lasso),\n",
    "    'train_r2': r2_score(y_train, y_pred_train_lasso),\n",
    "    'test_r2': r2_score(y_test, y_pred_test_lasso),\n",
    "    'best_alpha': lasso_grid.best_params_['alpha'],\n",
    "    'n_features': n_selected_features,\n",
    "    'training_time': (datetime.now() - start_time).total_seconds()\n",
    "}\n",
    "model_results['Lasso'] = lasso_metrics\n",
    "statistical_models['Lasso'] = {'model': lasso, **lasso_metrics}\n",
    "\n",
    "print(f\"   Tiempo entrenamiento: {lasso_metrics['training_time']:.2f}s\")\n",
    "print(f\"   Alpha óptimo: {lasso_metrics['best_alpha']}\")\n",
    "print(f\"   Features seleccionadas: {n_selected_features}/{len(available_features)}\")\n",
    "print(f\"   Train RMSE: {lasso_metrics['train_rmse']:,.0f}\")\n",
    "print(f\"   Test RMSE:  {lasso_metrics['test_rmse']:,.0f}\")\n",
    "print(f\"   Test R²:    {lasso_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# 4. ElasticNet (Paralelizado)\n",
    "print(\"\\n🔸 Entrenando ElasticNet (Paralelo)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "elastic_params = {\n",
    "    'alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "elastic_grid = GridSearchCV(\n",
    "    ElasticNet(random_state=42, max_iter=3000), \n",
    "    elastic_params, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,  # Usar todos los cores\n",
    "    verbose=0\n",
    ")\n",
    "elastic_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "elastic = elastic_grid.best_estimator_\n",
    "models['ElasticNet'] = elastic\n",
    "\n",
    "y_pred_train_elastic = elastic.predict(X_train_scaled)\n",
    "y_pred_test_elastic = elastic.predict(X_test_scaled)\n",
    "\n",
    "n_selected_features_elastic = np.sum(elastic.coef_ != 0)\n",
    "\n",
    "elastic_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_elastic)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_elastic)),\n",
    "    'train_mae': mean_absolute_error(y_train, y_pred_train_elastic),\n",
    "    'test_mae': mean_absolute_error(y_test, y_pred_test_elastic),\n",
    "    'train_r2': r2_score(y_train, y_pred_train_elastic),\n",
    "    'test_r2': r2_score(y_test, y_pred_test_elastic),\n",
    "    'best_alpha': elastic_grid.best_params_['alpha'],\n",
    "    'best_l1_ratio': elastic_grid.best_params_['l1_ratio'],\n",
    "    'n_features': n_selected_features_elastic,\n",
    "    'training_time': (datetime.now() - start_time).total_seconds()\n",
    "}\n",
    "model_results['ElasticNet'] = elastic_metrics\n",
    "statistical_models['ElasticNet'] = {'model': elastic, **elastic_metrics}\n",
    "\n",
    "print(f\"   Tiempo entrenamiento: {elastic_metrics['training_time']:.2f}s\")\n",
    "print(f\"   Alpha óptimo: {elastic_metrics['best_alpha']}\")\n",
    "print(f\"   L1 ratio óptimo: {elastic_metrics['best_l1_ratio']}\")\n",
    "print(f\"   Features seleccionadas: {n_selected_features_elastic}/{len(available_features)}\")\n",
    "print(f\"   Train RMSE: {elastic_metrics['train_rmse']:,.0f}\")\n",
    "print(f\"   Test RMSE:  {elastic_metrics['test_rmse']:,.0f}\")\n",
    "print(f\"   Test R²:    {elastic_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# Guardar modelos estadísticos\n",
    "model_results['statistical_models'] = statistical_models\n",
    "\n",
    "print(f\"\\n✅ Modelos estadísticos entrenados correctamente con paralelización\")\n",
    "print(f\"⏱️ Tiempo total modelos estadísticos: {sum([m['training_time'] for m in model_results.values() if 'training_time' in m]):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 2.2 COMPARACIÓN CON AIC/BIC (SIN CV)\n",
    "# ==================================================\n",
    "\n",
    "print(\"🔍 Calculando AIC/BIC para comparación de modelos...\")\n",
    "\n",
    "def calculate_aic_bic(y_true, y_pred, n_params, n_obs):\n",
    "    \"\"\"Calcula AIC y BIC para un modelo\"\"\"\n",
    "    # Suma de cuadrados de residuos\n",
    "    ssr = np.sum((y_true - y_pred) ** 2)\n",
    "    \n",
    "    # Log-likelihood (asumiendo errores normales)\n",
    "    sigma2 = ssr / n_obs\n",
    "    log_likelihood = -0.5 * n_obs * (np.log(2 * np.pi) + np.log(sigma2) + 1)\n",
    "    \n",
    "    # AIC y BIC\n",
    "    aic = 2 * n_params - 2 * log_likelihood\n",
    "    bic = np.log(n_obs) * n_params - 2 * log_likelihood\n",
    "    \n",
    "    return aic, bic\n",
    "\n",
    "# Calcular AIC/BIC para cada modelo estadístico\n",
    "n_obs = len(y_train)\n",
    "model_comparison = []\n",
    "\n",
    "# Linear Regression\n",
    "n_params_lr = len(available_features) + 1  # features + intercept\n",
    "aic_lr, bic_lr = calculate_aic_bic(y_train, y_pred_train_lr, n_params_lr, n_obs)\n",
    "\n",
    "model_comparison.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'AIC': aic_lr,\n",
    "    'BIC': bic_lr,\n",
    "    'N_Params': n_params_lr,\n",
    "    'Test_R2': lr_metrics['test_r2'],\n",
    "    'Test_RMSE': lr_metrics['test_rmse']\n",
    "})\n",
    "\n",
    "# Ridge (todos los parámetros excepto regularización)\n",
    "n_params_ridge = len(available_features) + 1\n",
    "aic_ridge, bic_ridge = calculate_aic_bic(y_train, y_pred_train_ridge, n_params_ridge, n_obs)\n",
    "\n",
    "model_comparison.append({\n",
    "    'Model': 'Ridge',\n",
    "    'AIC': aic_ridge,\n",
    "    'BIC': bic_ridge,\n",
    "    'N_Params': n_params_ridge,\n",
    "    'Test_R2': ridge_metrics['test_r2'],\n",
    "    'Test_RMSE': ridge_metrics['test_rmse']\n",
    "})\n",
    "\n",
    "# Lasso (solo parámetros no-cero)\n",
    "n_params_lasso = n_selected_features + 1\n",
    "aic_lasso, bic_lasso = calculate_aic_bic(y_train, y_pred_train_lasso, n_params_lasso, n_obs)\n",
    "\n",
    "model_comparison.append({\n",
    "    'Model': 'Lasso',\n",
    "    'AIC': aic_lasso,\n",
    "    'BIC': bic_lasso,\n",
    "    'N_Params': n_params_lasso,\n",
    "    'Test_R2': lasso_metrics['test_r2'],\n",
    "    'Test_RMSE': lasso_metrics['test_rmse']\n",
    "})\n",
    "\n",
    "# ElasticNet (solo parámetros no-cero)\n",
    "n_params_elastic = n_selected_features_elastic + 1\n",
    "aic_elastic, bic_elastic = calculate_aic_bic(y_train, y_pred_train_elastic, n_params_elastic, n_obs)\n",
    "\n",
    "model_comparison.append({\n",
    "    'Model': 'ElasticNet',\n",
    "    'AIC': aic_elastic,\n",
    "    'BIC': bic_elastic,\n",
    "    'N_Params': n_params_elastic,\n",
    "    'Test_R2': elastic_metrics['test_r2'],\n",
    "    'Test_RMSE': elastic_metrics['test_rmse']\n",
    "})\n",
    "\n",
    "# Crear DataFrame para comparación\n",
    "comparison_df = pd.DataFrame(model_comparison)\n",
    "comparison_df = comparison_df.sort_values('AIC').reset_index(drop=True)\n",
    "\n",
    "print(\"📊 Comparación de modelos estadísticos (ordenado por AIC):\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Modelo':<15} {'AIC':<12} {'BIC':<12} {'N_Params':<10} {'Test_R²':<10} {'Test_RMSE':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    print(f\"{row['Model']:<15} {row['AIC']:<12,.0f} {row['BIC']:<12,.0f} {row['N_Params']:<10} {row['Test_R2']:<10.4f} {row['Test_RMSE']:<12,.0f}\")\n",
    "\n",
    "# Identificar mejor modelo por cada criterio\n",
    "best_aic = comparison_df.loc[comparison_df['AIC'].idxmin(), 'Model']\n",
    "best_bic = comparison_df.loc[comparison_df['BIC'].idxmin(), 'Model']\n",
    "best_r2 = comparison_df.loc[comparison_df['Test_R2'].idxmax(), 'Model']\n",
    "best_rmse = comparison_df.loc[comparison_df['Test_RMSE'].idxmin(), 'Model']\n",
    "\n",
    "print(f\"\\n🏆 Mejores modelos por criterio:\")\n",
    "print(f\"   Mejor AIC:    {best_aic}\")\n",
    "print(f\"   Mejor BIC:    {best_bic}\")\n",
    "print(f\"   Mejor R²:     {best_r2}\")\n",
    "print(f\"   Mejor RMSE:   {best_rmse}\")\n",
    "\n",
    "# Visualización de comparación\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# AIC vs BIC\n",
    "axes[0].scatter(comparison_df['AIC'], comparison_df['BIC'], s=100, alpha=0.7)\n",
    "for i, row in comparison_df.iterrows():\n",
    "    axes[0].annotate(row['Model'], (row['AIC'], row['BIC']), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "axes[0].set_xlabel('AIC')\n",
    "axes[0].set_ylabel('BIC')\n",
    "axes[0].set_title('AIC vs BIC')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Número de parámetros vs Performance\n",
    "axes[1].scatter(comparison_df['N_Params'], comparison_df['Test_R2'], s=100, alpha=0.7, color='green')\n",
    "for i, row in comparison_df.iterrows():\n",
    "    axes[1].annotate(row['Model'], (row['N_Params'], row['Test_R2']), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "axes[1].set_xlabel('Número de Parámetros')\n",
    "axes[1].set_ylabel('Test R²')\n",
    "axes[1].set_title('Complejidad vs Performance')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Ranking de modelos\n",
    "ranking_criteria = ['AIC', 'BIC', 'Test_R2', 'Test_RMSE']\n",
    "model_names = comparison_df['Model'].tolist()\n",
    "rankings = []\n",
    "\n",
    "for criterion in ranking_criteria:\n",
    "    if criterion in ['AIC', 'BIC', 'Test_RMSE']:\n",
    "        # Menor es mejor\n",
    "        sorted_df = comparison_df.sort_values(criterion)\n",
    "    else:\n",
    "        # Mayor es mejor\n",
    "        sorted_df = comparison_df.sort_values(criterion, ascending=False)\n",
    "    \n",
    "    ranking = {model: idx+1 for idx, model in enumerate(sorted_df['Model'])}\n",
    "    rankings.append([ranking[model] for model in model_names])\n",
    "\n",
    "# Heatmap de rankings\n",
    "ranking_matrix = np.array(rankings).T\n",
    "im = axes[2].imshow(ranking_matrix, cmap='RdYlGn_r', aspect='auto')\n",
    "axes[2].set_xticks(range(len(ranking_criteria)))\n",
    "axes[2].set_xticklabels(ranking_criteria)\n",
    "axes[2].set_yticks(range(len(model_names)))\n",
    "axes[2].set_yticklabels(model_names)\n",
    "axes[2].set_title('Ranking de Modelos (1=mejor)')\n",
    "\n",
    "# Añadir valores al heatmap\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(len(ranking_criteria)):\n",
    "        axes[2].text(j, i, int(ranking_matrix[i, j]), ha='center', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Guardar resultados\n",
    "model_results['comparison_df'] = comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e10e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 2.3 DIAGNÓSTICO DE RESIDUOS Y DETECCIÓN DE OUTLIERS\n",
    "# ==================================================\n",
    "\n",
    "print(\"🔍 Realizando diagnóstico de residuos para el mejor modelo...\")\n",
    "\n",
    "# Usar el mejor modelo según AIC (Linear Regression en este caso)\n",
    "best_model_name = best_aic\n",
    "print(f\"Analizando residuos para: {best_model_name}\")\n",
    "\n",
    "# Obtener predicciones del mejor modelo\n",
    "if best_model_name == 'Linear_Regression':\n",
    "    y_pred_train_best = y_pred_train_lr\n",
    "    y_pred_test_best = y_pred_test_lr\n",
    "elif best_model_name == 'Ridge':\n",
    "    y_pred_train_best = y_pred_train_ridge\n",
    "    y_pred_test_best = y_pred_test_ridge\n",
    "elif best_model_name == 'Lasso':\n",
    "    y_pred_train_best = y_pred_train_lasso\n",
    "    y_pred_test_best = y_pred_test_lasso\n",
    "else:  # ElasticNet\n",
    "    y_pred_train_best = y_pred_train_elastic\n",
    "    y_pred_test_best = y_pred_test_elastic\n",
    "\n",
    "# Calcular residuos\n",
    "residuals_train = y_train - y_pred_train_best\n",
    "residuals_test = y_test - y_pred_test_best\n",
    "\n",
    "# Residuos estandarizados\n",
    "residuals_train_std = residuals_train / np.std(residuals_train)\n",
    "residuals_test_std = residuals_test / np.std(residuals_test)\n",
    "\n",
    "def diagnostic_plots(y_true, y_pred, residuals, residuals_std, title_prefix):\n",
    "    \"\"\"Crea gráficos de diagnóstico de residuos\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Residuos vs Fitted Values\n",
    "    axes[0,0].scatter(y_pred, residuals, alpha=0.5)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[0,0].set_xlabel('Valores Predichos')\n",
    "    axes[0,0].set_ylabel('Residuos')\n",
    "    axes[0,0].set_title(f'{title_prefix}: Residuos vs Predicciones')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Q-Q Plot de residuos\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title(f'{title_prefix}: Q-Q Plot Residuos')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Histograma de residuos\n",
    "    axes[0,2].hist(residuals, bins=50, density=True, alpha=0.7, color='skyblue')\n",
    "    axes[0,2].set_xlabel('Residuos')\n",
    "    axes[0,2].set_ylabel('Densidad')\n",
    "    axes[0,2].set_title(f'{title_prefix}: Distribución de Residuos')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot (Residuos estandarizados vs Fitted)\n",
    "    axes[1,0].scatter(y_pred, np.abs(residuals_std), alpha=0.5)\n",
    "    axes[1,0].set_xlabel('Valores Predichos')\n",
    "    axes[1,0].set_ylabel('|Residuos Estandarizados|')\n",
    "    axes[1,0].set_title(f'{title_prefix}: Scale-Location Plot')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Residuos vs Orden (para detectar autocorrelación)\n",
    "    axes[1,1].plot(range(len(residuals)), residuals, alpha=0.7)\n",
    "    axes[1,1].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[1,1].set_xlabel('Orden de Observación')\n",
    "    axes[1,1].set_ylabel('Residuos')\n",
    "    axes[1,1].set_title(f'{title_prefix}: Residuos vs Orden')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Predicted vs Actual\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    axes[1,2].scatter(y_true, y_pred, alpha=0.5)\n",
    "    axes[1,2].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    axes[1,2].set_xlabel('Valores Reales')\n",
    "    axes[1,2].set_ylabel('Valores Predichos')\n",
    "    axes[1,2].set_title(f'{title_prefix}: Predicho vs Real')\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Diagnósticos para conjunto de entrenamiento\n",
    "print(\"\\n📊 Diagnósticos para conjunto de ENTRENAMIENTO:\")\n",
    "diagnostic_plots(y_train, y_pred_train_best, residuals_train, residuals_train_std, \"Train\")\n",
    "\n",
    "# Diagnósticos para conjunto de prueba\n",
    "print(\"\\n📊 Diagnósticos para conjunto de PRUEBA:\")\n",
    "diagnostic_plots(y_test, y_pred_test_best, residuals_test, residuals_test_std, \"Test\")\n",
    "\n",
    "# Tests estadísticos de los residuos\n",
    "print(\"\\n🧪 Tests estadísticos de residuos:\")\n",
    "\n",
    "# Test de normalidad (Shapiro-Wilk en muestra pequeña)\n",
    "from scipy.stats import shapiro, jarque_bera\n",
    "\n",
    "# Tomar muestra para test de normalidad (Shapiro-Wilk max 5000 observaciones)\n",
    "sample_size = min(5000, len(residuals_train))\n",
    "residuals_sample = np.random.choice(residuals_train, sample_size, replace=False)\n",
    "\n",
    "shapiro_stat, shapiro_p = shapiro(residuals_sample)\n",
    "jb_stat, jb_p = jarque_bera(residuals_train)\n",
    "\n",
    "print(f\"Test de normalidad:\")\n",
    "print(f\"  Shapiro-Wilk (muestra {sample_size}): stat={shapiro_stat:.4f}, p-value={shapiro_p:.4f}\")\n",
    "print(f\"  Jarque-Bera: stat={jb_stat:.4f}, p-value={jb_p:.4f}\")\n",
    "\n",
    "# Test de heterocedasticidad (Breusch-Pagan)\n",
    "if best_model_name == 'Linear_Regression':\n",
    "    # Usar statsmodels para test más riguroso\n",
    "    X_train_sm = sm.add_constant(X_train)\n",
    "    model_sm = sm.OLS(y_train, X_train_sm).fit()\n",
    "    \n",
    "    # Test de Breusch-Pagan\n",
    "    from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "    bp_stat, bp_p, _, _ = het_breuschpagan(model_sm.resid, X_train_sm)\n",
    "    \n",
    "    print(f\"\\nTest de heterocedasticidad:\")\n",
    "    print(f\"  Breusch-Pagan: stat={bp_stat:.4f}, p-value={bp_p:.4f}\")\n",
    "    \n",
    "    if bp_p < 0.05:\n",
    "        print(\"  ⚠️ Evidencia de heterocedasticidad\")\n",
    "    else:\n",
    "        print(\"  ✅ No hay evidencia de heterocedasticidad\")\n",
    "\n",
    "# Detección de outliers\n",
    "print(f\"\\n🎯 Detección de outliers:\")\n",
    "\n",
    "# Outliers basados en residuos estandarizados (|z| > 3)\n",
    "outliers_train = np.where(np.abs(residuals_train_std) > 3)[0]\n",
    "outliers_test = np.where(np.abs(residuals_test_std) > 3)[0]\n",
    "\n",
    "print(f\"Outliers por residuos estandarizados (|z| > 3):\")\n",
    "print(f\"  Train: {len(outliers_train)} outliers ({len(outliers_train)/len(y_train)*100:.2f}%)\")\n",
    "print(f\"  Test:  {len(outliers_test)} outliers ({len(outliers_test)/len(y_test)*100:.2f}%)\")\n",
    "\n",
    "# Estadísticas de los outliers\n",
    "if len(outliers_train) > 0:\n",
    "    outlier_residuals = residuals_train_std[outliers_train]\n",
    "    print(f\"\\nEstadísticas de outliers (train):\")\n",
    "    print(f\"  Residuo estandarizado máximo: {np.max(np.abs(outlier_residuals)):.2f}\")\n",
    "    print(f\"  Residuo estandarizado promedio: {np.mean(np.abs(outlier_residuals)):.2f}\")\n",
    "\n",
    "# Análisis de leverage y influencia (solo para Linear Regression)\n",
    "if best_model_name == 'Linear_Regression':\n",
    "    print(f\"\\n📈 Análisis de leverage e influencia:\")\n",
    "    \n",
    "    # Calcular leverage, residuos estudentizados y distancia de Cook\n",
    "    influence = OLSInfluence(model_sm)\n",
    "    leverage = influence.hat_matrix_diag\n",
    "    studentized_residuals = influence.resid_studentized_external\n",
    "    cooks_distance = influence.cooks_distance[0]\n",
    "    \n",
    "    # Umbrales\n",
    "    leverage_threshold = 2 * (len(available_features) + 1) / len(y_train)\n",
    "    cooks_threshold = 4 / len(y_train)\n",
    "    \n",
    "    # Identificar observaciones influyentes\n",
    "    high_leverage = np.where(leverage > leverage_threshold)[0]\n",
    "    high_cooks = np.where(cooks_distance > cooks_threshold)[0]\n",
    "    high_studentized = np.where(np.abs(studentized_residuals) > 3)[0]\n",
    "    \n",
    "    print(f\"  High leverage (>{leverage_threshold:.4f}): {len(high_leverage)} obs ({len(high_leverage)/len(y_train)*100:.2f}%)\")\n",
    "    print(f\"  High Cook's distance (>{cooks_threshold:.4f}): {len(high_cooks)} obs ({len(high_cooks)/len(y_train)*100:.2f}%)\")\n",
    "    print(f\"  High studentized residuals (>3): {len(high_studentized)} obs ({len(high_studentized)/len(y_train)*100:.2f}%)\")\n",
    "    \n",
    "    # Observaciones problemáticas (combinan múltiples criterios)\n",
    "    problematic = np.intersect1d(high_leverage, high_cooks)\n",
    "    if len(problematic) > 0:\n",
    "        print(f\"  ⚠️ Observaciones problemáticas (alto leverage Y Cook's distance): {len(problematic)}\")\n",
    "    else:\n",
    "        print(f\"  ✅ No hay observaciones extremadamente problemáticas\")\n",
    "\n",
    "print(f\"\\n✅ Diagnóstico de residuos completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6614ba8",
   "metadata": {},
   "source": [
    "## 3. Modelos de Árboles\n",
    "\n",
    "### Metodología de Modelos Basados en Árboles\n",
    "- **LightGBM**: Gradient boosting eficiente con parámetros por defecto\n",
    "- **Random Forest**: Ensemble de árboles con configuración básica\n",
    "- **Comparación**: Performance vs modelos estadísticos\n",
    "- **Interpretabilidad**: Feature importance y explicabilidad inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7840a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 3.1 MODELOS DE ÁRBOLES OPTIMIZADOS (GPU + PARALELO)\n",
    "# ==================================================\n",
    "\n",
    "print(\"🌳 Entrenando modelos basados en árboles con GPU y paralelización...\")\n",
    "\n",
    "# 1. LightGBM con GPU (si disponible)\n",
    "print(\"\\n🔸 Entrenando LightGBM (GPU Optimizado)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Parámetros optimizados para GPU y paralelización\n",
    "if GPU_AVAILABLE:\n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'num_leaves': 63,  # Más hojas para GPU\n",
    "        'learning_rate': 0.1,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'max_depth': 7,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'lambda_l1': 0.1,\n",
    "        'lambda_l2': 0.1,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        'force_col_wise': True,  # Optimización para GPU\n",
    "        'gpu_use_dp': False  # Usar single precision para velocidad\n",
    "    }\n",
    "    print(\"   🚀 Usando GPU para entrenamiento\")\n",
    "else:\n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'device': 'cpu',\n",
    "        'num_threads': mp.cpu_count(),  # Usar todos los cores\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        'force_col_wise': True\n",
    "    }\n",
    "    print(f\"   ⚡ Usando CPU con {mp.cpu_count()} threads\")\n",
    "\n",
    "# Crear datasets de LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Entrenar modelo con early stopping optimizado\n",
    "lgb_model = lgb.train(\n",
    "    lgb_params,\n",
    "    train_data,\n",
    "    num_boost_round=500,  # Más iteraciones para mejor rendimiento\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=20),\n",
    "        lgb.log_evaluation(50)  # Log cada 50 iteraciones\n",
    "    ]\n",
    ")\n",
    "\n",
    "training_time_lgb = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train_lgb = lgb_model.predict(X_train, num_iteration=lgb_model.best_iteration)\n",
    "y_pred_test_lgb = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "# Métricas LightGBM\n",
    "lgb_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_lgb)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_lgb)),\n",
    "    'train_mae': mean_absolute_error(y_train, y_pred_train_lgb),\n",
    "    'test_mae': mean_absolute_error(y_test, y_pred_test_lgb),\n",
    "    'train_r2': r2_score(y_train, y_pred_train_lgb),\n",
    "    'test_r2': r2_score(y_test, y_pred_test_lgb),\n",
    "    'best_iteration': lgb_model.best_iteration,\n",
    "    'num_features': lgb_model.num_feature(),\n",
    "    'training_time': training_time_lgb,\n",
    "    'device': 'GPU' if GPU_AVAILABLE else 'CPU'\n",
    "}\n",
    "\n",
    "models['LightGBM'] = lgb_model\n",
    "model_results['LightGBM'] = lgb_metrics\n",
    "\n",
    "print(f\"   Tiempo entrenamiento: {training_time_lgb:.2f}s\")\n",
    "print(f\"   Mejores iteraciones: {lgb_model.best_iteration}\")\n",
    "print(f\"   Device usado: {lgb_metrics['device']}\")\n",
    "print(f\"   Train RMSE: {lgb_metrics['train_rmse']:,.0f}\")\n",
    "print(f\"   Test RMSE:  {lgb_metrics['test_rmse']:,.0f}\")\n",
    "print(f\"   Test R²:    {lgb_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# 2. Random Forest con paralelización optimizada\n",
    "print(\"\\n🔸 Entrenando Random Forest (Paralelo Optimizado)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Parámetros optimizados para paralelización\n",
    "rf_params = {\n",
    "    'n_estimators': 200,  # Más árboles para mejor rendimiento\n",
    "    'max_depth': 15,  # Más profundidad\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features': 'sqrt',\n",
    "    'bootstrap': True,\n",
    "    'oob_score': True,  # Out-of-bag score para validación\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,  # Usar todos los cores\n",
    "    'verbose': 1  # Mostrar progreso\n",
    "}\n",
    "\n",
    "rf_model = RandomForestRegressor(**rf_params)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "training_time_rf = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train_rf = rf_model.predict(X_train)\n",
    "y_pred_test_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Métricas Random Forest\n",
    "rf_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_rf)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_rf)),\n",
    "    'train_mae': mean_absolute_error(y_train, y_pred_train_rf),\n",
    "    'test_mae': mean_absolute_error(y_test, y_pred_test_rf),\n",
    "    'train_r2': r2_score(y_train, y_pred_train_rf),\n",
    "    'test_r2': r2_score(y_test, y_pred_test_rf),\n",
    "    'oob_score': rf_model.oob_score_,\n",
    "    'n_estimators': rf_params['n_estimators'],\n",
    "    'max_depth': rf_params['max_depth'],\n",
    "    'training_time': training_time_rf,\n",
    "    'n_cores': mp.cpu_count()\n",
    "}\n",
    "\n",
    "models['Random_Forest'] = rf_model\n",
    "model_results['Random_Forest'] = rf_metrics\n",
    "\n",
    "print(f\"   Tiempo entrenamiento: {training_time_rf:.2f}s\")\n",
    "print(f\"   Número de árboles: {rf_params['n_estimators']}\")\n",
    "print(f\"   Cores utilizados: {mp.cpu_count()}\")\n",
    "print(f\"   OOB Score: {rf_metrics['oob_score']:.4f}\")\n",
    "print(f\"   Train RMSE: {rf_metrics['train_rmse']:,.0f}\")\n",
    "print(f\"   Test RMSE:  {rf_metrics['test_rmse']:,.0f}\")\n",
    "print(f\"   Test R²:    {rf_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# Comparación de velocidad de entrenamiento\n",
    "print(f\"\\n⚡ COMPARACIÓN DE VELOCIDAD:\")\n",
    "print(f\"   LightGBM ({lgb_metrics['device']}): {training_time_lgb:.2f}s\")\n",
    "print(f\"   Random Forest ({mp.cpu_count()} cores): {training_time_rf:.2f}s\")\n",
    "print(f\"   Speedup LGB vs RF: {training_time_rf/training_time_lgb:.1f}x\")\n",
    "\n",
    "print(f\"\\n✅ Modelos de árboles entrenados correctamente con optimizaciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebc74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 3.2 FEATURE IMPORTANCE Y EXPLICABILIDAD INICIAL\n",
    "# ==================================================\n",
    "\n",
    "print(\"🔍 Analizando feature importance de modelos de árboles...\")\n",
    "\n",
    "def plot_feature_importance(model, model_name, feature_names, top_n=15):\n",
    "    \"\"\"Grafica feature importance para modelos de árboles\"\"\"\n",
    "    \n",
    "    if model_name == 'LightGBM':\n",
    "        # Feature importance de LightGBM\n",
    "        importance = model.feature_importance(importance_type='gain')\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "    elif model_name == 'Random_Forest':\n",
    "        # Feature importance de Random Forest\n",
    "        importance = model.feature_importances_\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top features\n",
    "    top_features = feature_imp.head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(top_features)), top_features['importance'], color='lightcoral', alpha=0.8)\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top {top_n} Features - {model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_imp\n",
    "\n",
    "# Analizar feature importance para LightGBM\n",
    "print(f\"\\n📊 Feature Importance - LightGBM:\")\n",
    "lgb_feature_imp = plot_feature_importance(lgb_model, 'LightGBM', available_features)\n",
    "\n",
    "# Mostrar top 10 features de LightGBM\n",
    "print(f\"Top 10 Features más importantes (LightGBM):\")\n",
    "for i, (_, row) in enumerate(lgb_feature_imp.head(10).iterrows(), 1):\n",
    "    print(f\"  {i:2d}. {row['feature']:<25}: {row['importance']:.2f}\")\n",
    "\n",
    "# Analizar feature importance para Random Forest\n",
    "print(f\"\\n📊 Feature Importance - Random Forest:\")\n",
    "rf_feature_imp = plot_feature_importance(rf_model, 'Random_Forest', available_features)\n",
    "\n",
    "# Mostrar top 10 features de Random Forest\n",
    "print(f\"Top 10 Features más importantes (Random Forest):\")\n",
    "for i, (_, row) in enumerate(rf_feature_imp.head(10).iterrows(), 1):\n",
    "    print(f\"  {i:2d}. {row['feature']:<25}: {row['importance']:.4f}\")\n",
    "\n",
    "# Comparación de feature importance entre modelos de árboles\n",
    "print(f\"\\n🔄 Comparación de Feature Importance entre modelos:\")\n",
    "\n",
    "# Merge de importancias\n",
    "comparison_imp = lgb_feature_imp[['feature', 'importance']].rename(columns={'importance': 'lgb_importance'})\n",
    "comparison_imp = comparison_imp.merge(\n",
    "    rf_feature_imp[['feature', 'importance']].rename(columns={'importance': 'rf_importance'}),\n",
    "    on='feature'\n",
    ")\n",
    "\n",
    "# Normalizar para comparación\n",
    "comparison_imp['lgb_importance_norm'] = comparison_imp['lgb_importance'] / comparison_imp['lgb_importance'].max()\n",
    "comparison_imp['rf_importance_norm'] = comparison_imp['rf_importance'] / comparison_imp['rf_importance'].max()\n",
    "\n",
    "# Calcular correlación entre importancias\n",
    "correlation = comparison_imp['lgb_importance_norm'].corr(comparison_imp['rf_importance_norm'])\n",
    "print(f\"Correlación entre importancias LightGBM-RF: {correlation:.4f}\")\n",
    "\n",
    "# Plot comparativo de importancias\n",
    "top_features_comparison = comparison_imp.head(15)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Scatter plot de importancias\n",
    "axes[0].scatter(comparison_imp['lgb_importance_norm'], comparison_imp['rf_importance_norm'], alpha=0.6)\n",
    "axes[0].plot([0, 1], [0, 1], 'r--', alpha=0.8)\n",
    "axes[0].set_xlabel('LightGBM Importance (normalized)')\n",
    "axes[0].set_ylabel('Random Forest Importance (normalized)')\n",
    "axes[0].set_title(f'Comparación Feature Importance\\n(Correlación: {correlation:.3f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Top features side by side\n",
    "x_pos = np.arange(len(top_features_comparison))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].barh(x_pos - width/2, top_features_comparison['lgb_importance_norm'], width, \n",
    "            label='LightGBM', alpha=0.8, color='lightblue')\n",
    "axes[1].barh(x_pos + width/2, top_features_comparison['rf_importance_norm'], width,\n",
    "            label='Random Forest', alpha=0.8, color='lightcoral')\n",
    "\n",
    "axes[1].set_yticks(x_pos)\n",
    "axes[1].set_yticklabels(top_features_comparison['feature'])\n",
    "axes[1].set_xlabel('Normalized Importance')\n",
    "axes[1].set_title('Top 15 Features: LightGBM vs Random Forest')\n",
    "axes[1].legend()\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Features que aparecen en top 10 de ambos modelos\n",
    "lgb_top10 = set(lgb_feature_imp.head(10)['feature'])\n",
    "rf_top10 = set(rf_feature_imp.head(10)['feature'])\n",
    "common_top10 = lgb_top10.intersection(rf_top10)\n",
    "\n",
    "print(f\"\\n🎯 Features en Top 10 de ambos modelos ({len(common_top10)}):\")\n",
    "for feature in sorted(common_top10):\n",
    "    lgb_rank = lgb_feature_imp[lgb_feature_imp['feature'] == feature].index[0] + 1\n",
    "    rf_rank = rf_feature_imp[rf_feature_imp['feature'] == feature].index[0] + 1\n",
    "    print(f\"  • {feature} (LGB: #{lgb_rank}, RF: #{rf_rank})\")\n",
    "\n",
    "# Guardar feature importance\n",
    "model_results['lgb_feature_importance'] = lgb_feature_imp\n",
    "model_results['rf_feature_importance'] = rf_feature_imp\n",
    "model_results['feature_importance_correlation'] = correlation\n",
    "\n",
    "print(f\"\\n✅ Análisis de feature importance completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c4bd5",
   "metadata": {},
   "source": [
    "## 4. Evaluación Comparativa y Selección de Modelos\n",
    "\n",
    "### 4.1 Comparación Global de Modelos\n",
    "\n",
    "En esta sección realizaremos una evaluación comparativa sistemática de todos los modelos desarrollados, considerando múltiples criterios de evaluación y el contexto específico del problema de predicción de precios inmobiliarios.\n",
    "\n",
    "**Metodología de Evaluación:**\n",
    "- **Métricas de Rendimiento**: RMSE, MAE, R² para evaluación cuantitativa\n",
    "- **Criterios Estadísticos**: AIC/BIC para modelos lineales (penalización por complejidad)\n",
    "- **Análisis de Residuos**: Distribución, heterocedasticidad, outliers\n",
    "- **Estabilidad Temporal**: Robustez en datos de test (simulando nuevas observaciones)\n",
    "- **Interpretabilidad**: Facilidad de explicación para stakeholders inmobiliarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7962768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 5.1 EVALUACIÓN COMPARATIVA H2O + SKLEARN + GPU\n",
    "# ==================================================\n",
    "\n",
    "print(\"📊 Evaluación comparativa completa: H2O + Sklearn + GPU...\")\n",
    "\n",
    "# Función para calcular métricas completas en paralelo\n",
    "def evaluate_model_comprehensive(y_true, y_pred, model_name, n_params=None, training_time=None, cv_metrics=None):\n",
    "    \"\"\"Calcula métricas completas para evaluación de modelos\"\"\"\n",
    "    \n",
    "    # Métricas básicas calculadas en paralelo donde sea posible\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Métricas adicionales\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    residuals = y_true - y_pred\n",
    "    std_residuals = np.std(residuals)\n",
    "    \n",
    "    # Error relativo al rango\n",
    "    price_range = np.max(y_true) - np.min(y_true)\n",
    "    rmse_relative = (rmse / price_range) * 100\n",
    "    \n",
    "    # Crear diccionario de métricas\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape,\n",
    "        'std_residuals': std_residuals,\n",
    "        'rmse_relative': rmse_relative,\n",
    "        'n_params': n_params,\n",
    "        'training_time': training_time,\n",
    "        'framework': 'H2O' if model_name.startswith('H2O') else 'Sklearn'\n",
    "    }\n",
    "    \n",
    "    # Añadir métricas de validación cruzada si están disponibles\n",
    "    if cv_metrics:\n",
    "        metrics.update(cv_metrics)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluar todos los modelos usando paralelización donde sea posible\n",
    "print(\"🎯 Evaluando todos los modelos (H2O + Sklearn + GPU)...\")\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "# === MODELOS H2O ===\n",
    "if H2O_AVAILABLE and h2o_results:\n",
    "    print(\"🚀 Evaluando modelos H2O...\")\n",
    "    \n",
    "    for model_name, model_info in h2o_results.items():\n",
    "        # Para H2O usamos métricas ya calculadas\n",
    "        metrics = {\n",
    "            'model': model_name,\n",
    "            'rmse': model_info['test_rmse'],\n",
    "            'mae': model_info['test_rmse'] * 0.7,  # Aproximación MAE ≈ 0.7 * RMSE\n",
    "            'r2': model_info['test_r2'],\n",
    "            'mape': (model_info['test_rmse'] / y_test.mean()) * 100,  # Aproximación\n",
    "            'training_time': model_info['training_time'],\n",
    "            'cv_rmse': model_info.get('cv_rmse', None),\n",
    "            'cv_r2': model_info.get('cv_r2', None),\n",
    "            'framework': 'H2O',\n",
    "            'distributed': True\n",
    "        }\n",
    "        \n",
    "        evaluation_results.append(metrics)\n",
    "\n",
    "# === MODELOS SKLEARN ESTADÍSTICOS ===\n",
    "if 'statistical_models' in model_results:\n",
    "    print(\"📊 Evaluando modelos Sklearn...\")\n",
    "    \n",
    "    for model_name, model_info in model_results['statistical_models'].items():\n",
    "        model = model_info['model']\n",
    "        \n",
    "        # Predicciones optimizadas\n",
    "        if hasattr(model, 'n_jobs'):\n",
    "            model.n_jobs = -1  # Asegurar paralelización\n",
    "        \n",
    "        if model_name in ['Ridge', 'Lasso', 'ElasticNet']:\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Número de parámetros para AIC/BIC\n",
    "        if hasattr(model, 'coef_'):\n",
    "            if model_name in ['Lasso', 'ElasticNet']:\n",
    "                n_params = np.sum(model.coef_ != 0) + 1  # Solo parámetros no-cero + intercept\n",
    "            else:\n",
    "                n_params = len(model.coef_) + 1  # Todos los parámetros + intercept\n",
    "        else:\n",
    "            n_params = X_test.shape[1] + 1  # Features + intercept\n",
    "        \n",
    "        metrics = evaluate_model_comprehensive(\n",
    "            y_test, y_pred, model_name, n_params, \n",
    "            model_info.get('training_time', None)\n",
    "        )\n",
    "        \n",
    "        evaluation_results.append(metrics)\n",
    "\n",
    "# === MODELOS DE ÁRBOLES ===\n",
    "tree_models_info = {\n",
    "    'LightGBM': (lgb_model, lambda m: m.predict(X_test, num_iteration=m.best_iteration)),\n",
    "    'Random_Forest': (rf_model, lambda m: m.predict(X_test))\n",
    "}\n",
    "\n",
    "for model_name, (model, predict_fn) in tree_models_info.items():\n",
    "    if model_name in model_results:\n",
    "        model_info = model_results[model_name]\n",
    "        y_pred = predict_fn(model)\n",
    "        \n",
    "        if model_name == 'LightGBM':\n",
    "            n_params = model.best_iteration * len(available_features_sklearn)\n",
    "        else:\n",
    "            n_params = model.n_estimators * len(available_features_sklearn)\n",
    "        \n",
    "        metrics = evaluate_model_comprehensive(\n",
    "            y_test, y_pred, model_name, n_params, \n",
    "            model_info.get('training_time', None)\n",
    "        )\n",
    "        \n",
    "        # Añadir métricas específicas\n",
    "        if model_name == 'LightGBM':\n",
    "            metrics['device'] = model_info.get('device', 'CPU')\n",
    "            metrics['best_iteration'] = model_info.get('best_iteration', 0)\n",
    "        else:\n",
    "            metrics['oob_score'] = model_info.get('oob_score', None)\n",
    "            metrics['n_cores'] = model_info.get('n_cores', 1)\n",
    "        \n",
    "        evaluation_results.append(metrics)\n",
    "\n",
    "# Crear DataFrame de resultados optimizado\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Mostrar tabla comparativa completa\n",
    "print(\"\\n📋 RESUMEN COMPARATIVO COMPLETO (H2O + SKLEARN + GPU):\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Formatear tabla para mejor visualización\n",
    "display_cols = ['model', 'framework', 'rmse', 'r2', 'training_time']\n",
    "results_display = results_df[display_cols].copy()\n",
    "\n",
    "results_display['rmse'] = results_display['rmse'].round(0)\n",
    "results_display['r2'] = results_display['r2'].round(4)\n",
    "results_display['training_time'] = results_display['training_time'].round(2)\n",
    "\n",
    "# Ordenar por R² descendente\n",
    "results_display = results_display.sort_values('r2', ascending=False)\n",
    "print(results_display.to_string(index=False))\n",
    "\n",
    "# Análisis por framework\n",
    "print(f\"\\n🏆 ANÁLISIS POR FRAMEWORK:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "framework_analysis = results_df.groupby('framework').agg({\n",
    "    'rmse': ['mean', 'min'],\n",
    "    'r2': ['mean', 'max'],\n",
    "    'training_time': ['mean', 'sum']\n",
    "}).round(3)\n",
    "\n",
    "print(framework_analysis)\n",
    "\n",
    "# Top 3 modelos globales\n",
    "print(f\"\\n🥇 TOP 3 MODELOS GLOBALES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "top_models = results_df.nlargest(3, 'r2')\n",
    "for i, (_, model) in enumerate(top_models.iterrows(), 1):\n",
    "    print(f\"  {i}. {model['model']} ({model['framework']})\")\n",
    "    print(f\"     R² = {model['r2']:.4f}, RMSE = {model['rmse']:,.0f}\")\n",
    "    print(f\"     Tiempo = {model['training_time']:.1f}s\")\n",
    "\n",
    "# Comparación de velocidad por framework\n",
    "h2o_time = results_df[results_df['framework'] == 'H2O']['training_time'].sum()\n",
    "sklearn_time = results_df[results_df['framework'] == 'Sklearn']['training_time'].sum()\n",
    "\n",
    "print(f\"\\n⚡ COMPARACIÓN DE VELOCIDAD:\")\n",
    "print(f\"   H2O total: {h2o_time:.1f}s\")\n",
    "print(f\"   Sklearn total: {sklearn_time:.1f}s\")\n",
    "if h2o_time > 0 and sklearn_time > 0:\n",
    "    speedup = sklearn_time / h2o_time\n",
    "    print(f\"   Speedup H2O vs Sklearn: {speedup:.1f}x\")\n",
    "\n",
    "# Identificar mejor modelo overall\n",
    "best_model_idx = results_df['r2'].idxmax()\n",
    "best_model_info = results_df.loc[best_model_idx]\n",
    "\n",
    "print(f\"\\n🎯 MEJOR MODELO OVERALL: {best_model_info['model']} ({best_model_info['framework']})\")\n",
    "print(f\"   R² = {best_model_info['r2']:.4f}\")\n",
    "print(f\"   RMSE = {best_model_info['rmse']:,.0f}\")\n",
    "print(f\"   Tiempo = {best_model_info['training_time']:.1f}s\")\n",
    "\n",
    "# Guardar resultados completos\n",
    "model_results['comprehensive_evaluation'] = results_df\n",
    "model_results['best_overall_model'] = best_model_info['model']\n",
    "model_results['framework_analysis'] = framework_analysis\n",
    "\n",
    "print(f\"\\n✅ Evaluación comparativa completa finalizada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a23509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 4.2 VISUALIZACIÓN COMPARATIVA CON MÉTRICAS DE RENDIMIENTO\n",
    "# ==================================================\n",
    "\n",
    "print(\"📈 Generando visualizaciones comparativas con análisis de rendimiento...\")\n",
    "\n",
    "# 1. Dashboard de métricas principales incluyendo velocidad\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# RMSE comparison\n",
    "bars1 = axes[0,0].bar(results_df['model'], results_df['rmse'], color='lightcoral', alpha=0.8)\n",
    "axes[0,0].set_title('RMSE por Modelo', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_ylabel('RMSE (DKK)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "# Añadir valores en las barras\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:,.0f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# R² comparison\n",
    "bars2 = axes[0,1].bar(results_df['model'], results_df['r2'], color='lightblue', alpha=0.8)\n",
    "axes[0,1].set_title('R² por Modelo', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_ylabel('R²')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[0,1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Training Time comparison\n",
    "bars3 = axes[0,2].bar(results_df['model'], results_df['training_time'], color='gold', alpha=0.8)\n",
    "axes[0,2].set_title('Tiempo de Entrenamiento', fontsize=14, fontweight='bold')\n",
    "axes[0,2].set_ylabel('Tiempo (segundos)')\n",
    "axes[0,2].tick_params(axis='x', rotation=45)\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    axes[0,2].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.1f}s', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# MAE comparison\n",
    "bars4 = axes[1,0].bar(results_df['model'], results_df['mae'], color='lightgreen', alpha=0.8)\n",
    "axes[1,0].set_title('MAE por Modelo', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_ylabel('MAE (DKK)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "for bar in bars4:\n",
    "    height = bar.get_height()\n",
    "    axes[1,0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:,.0f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Efficiency Score (R²/Time)\n",
    "efficiency_vals = results_df['r2'] / results_df['training_time']\n",
    "bars5 = axes[1,1].bar(results_df['model'], efficiency_vals, color='purple', alpha=0.8)\n",
    "axes[1,1].set_title('Eficiencia (R²/Tiempo)', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_ylabel('Eficiencia')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "for bar in bars5:\n",
    "    height = bar.get_height()\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Scatter: Performance vs Speed\n",
    "axes[1,2].scatter(results_df['training_time'], results_df['r2'], \n",
    "                  s=results_df['rmse']/100, alpha=0.7, c=range(len(results_df)), \n",
    "                  cmap='viridis')\n",
    "for i, row in results_df.iterrows():\n",
    "    axes[1,2].annotate(row['model'], (row['training_time'], row['r2']), \n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[1,2].set_xlabel('Tiempo de Entrenamiento (s)')\n",
    "axes[1,2].set_ylabel('R²')\n",
    "axes[1,2].set_title('Performance vs Velocidad\\n(tamaño = RMSE)')\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Dashboard Comparativo - Precisión y Rendimiento', fontsize=18, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Análisis de trade-off Performance-Velocidad\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pareto Front: R² vs Training Time\n",
    "axes[0].scatter(results_df['training_time'], results_df['r2'], s=100, alpha=0.7)\n",
    "for i, row in results_df.iterrows():\n",
    "    axes[0].annotate(row['model'], (row['training_time'], row['r2']), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "# Identificar frontera de Pareto\n",
    "pareto_indices = []\n",
    "for i, row_i in results_df.iterrows():\n",
    "    is_pareto = True\n",
    "    for j, row_j in results_df.iterrows():\n",
    "        if i != j:\n",
    "            # Un punto está dominado si otro tiene mejor R² Y menor tiempo\n",
    "            if row_j['r2'] >= row_i['r2'] and row_j['training_time'] <= row_i['training_time']:\n",
    "                if row_j['r2'] > row_i['r2'] or row_j['training_time'] < row_i['training_time']:\n",
    "                    is_pareto = False\n",
    "                    break\n",
    "    if is_pareto:\n",
    "        pareto_indices.append(i)\n",
    "\n",
    "# Dibujar frontera de Pareto\n",
    "pareto_points = results_df.iloc[pareto_indices].sort_values('training_time')\n",
    "axes[0].plot(pareto_points['training_time'], pareto_points['r2'], 'r--', alpha=0.7, linewidth=2, label='Frontera de Pareto')\n",
    "axes[0].scatter(pareto_points['training_time'], pareto_points['r2'], c='red', s=150, alpha=0.8, marker='*')\n",
    "\n",
    "axes[0].set_xlabel('Tiempo de Entrenamiento (s)')\n",
    "axes[0].set_ylabel('R²')\n",
    "axes[0].set_title('Trade-off Performance vs Velocidad')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Radar chart para comparación multidimensional\n",
    "categories = ['R²', 'Velocidad\\n(1/tiempo)', 'Precisión\\n(1/RMSE)', 'Estabilidad\\n(1/MAE)']\n",
    "N = len(categories)\n",
    "\n",
    "# Normalizar métricas para radar chart\n",
    "r2_norm = (results_df['r2'] - results_df['r2'].min()) / (results_df['r2'].max() - results_df['r2'].min())\n",
    "speed_norm = (1/results_df['training_time'] - 1/results_df['training_time'].max()) / (1/results_df['training_time'].min() - 1/results_df['training_time'].max())\n",
    "rmse_norm = (1/results_df['rmse'] - 1/results_df['rmse'].max()) / (1/results_df['rmse'].min() - 1/results_df['rmse'].max())\n",
    "mae_norm = (1/results_df['mae'] - 1/results_df['mae'].max()) / (1/results_df['mae'].min() - 1/results_df['mae'].max())\n",
    "\n",
    "# Ángulos para radar chart\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Plot top 3 modelos en radar chart\n",
    "top_3_models = ranking_scores[:3]\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "for i, model_info in enumerate(top_3_models):\n",
    "    model_name = model_info['model']\n",
    "    model_idx = results_df[results_df['model'] == model_name].index[0]\n",
    "    \n",
    "    values = [r2_norm.iloc[model_idx], speed_norm.iloc[model_idx], \n",
    "              rmse_norm.iloc[model_idx], mae_norm.iloc[model_idx]]\n",
    "    values += values[:1]\n",
    "    \n",
    "    axes[1].plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i])\n",
    "    axes[1].fill(angles, values, alpha=0.25, color=colors[i])\n",
    "\n",
    "axes[1].set_xticks(angles[:-1])\n",
    "axes[1].set_xticklabels(categories)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_title('Comparación Multidimensional\\n(Top 3 Modelos)')\n",
    "axes[1].legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Resumen de optimizaciones aplicadas\n",
    "print(f\"\\n📊 RESUMEN DE OPTIMIZACIONES APLICADAS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "optimization_summary = {\n",
    "    'GPU_LightGBM': 'Sí' if GPU_AVAILABLE else 'No disponible',\n",
    "    'Paralelización_RF': f'{mp.cpu_count()} cores',\n",
    "    'GridSearch_Paralelo': 'Todos los modelos',\n",
    "    'Memoria_Optimizada': f'{parallel_config[\"available_memory_gb\"]} GB disponible',\n",
    "    'Tiempo_Total': f'{sum([m[\"training_time\"] for m in evaluation_results]):.1f}s'\n",
    "}\n",
    "\n",
    "for key, value in optimization_summary.items():\n",
    "    print(f\"  {key.replace('_', ' ')}: {value}\")\n",
    "\n",
    "# Comparación de speedup teórico vs real\n",
    "theoretical_speedup = mp.cpu_count()\n",
    "actual_times = {m['model']: m['training_time'] for m in evaluation_results}\n",
    "\n",
    "print(f\"\\n⚡ ANÁLISIS DE SPEEDUP:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Speedup teórico máximo: {theoretical_speedup}x\")\n",
    "print(f\"Modelo más rápido: {min(actual_times, key=actual_times.get)} ({min(actual_times.values()):.1f}s)\")\n",
    "print(f\"Modelo más lento: {max(actual_times, key=actual_times.get)} ({max(actual_times.values()):.1f}s)\")\n",
    "print(f\"Speedup real LightGBM vs RF: {actual_times.get('Random_Forest', 1)/actual_times.get('LightGBM', 1):.1f}x\")\n",
    "\n",
    "print(f\"\\n✅ Visualización optimizada completada con métricas de rendimiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c58ec1",
   "metadata": {},
   "source": [
    "# CARGA DE DATOS PROCESADOS DESDE FEATURE ENGINEERING\n",
    "\n",
    "print(\"📂 Cargando datos procesados desde notebook 03 (ingeniería de características)...\")\n",
    "\n",
    "# Usar configuración para rutas de datos procesados\n",
    "processed_data_path = DATA_DIR / \"processed\"\n",
    "\n",
    "# Buscar archivos de datos procesados específicos\n",
    "possible_files = [\n",
    "    \"feature_engineered_complete.parquet\",\n",
    "    \"modeling_dataset.parquet\", \n",
    "    \"cleaned_data.parquet\",\n",
    "    \"processed_data.parquet\"\n",
    "]\n",
    "\n",
    "df_processed = None\n",
    "loaded_file = None\n",
    "\n",
    "# Intentar cargar archivos en orden de prioridad\n",
    "for filename in possible_files:\n",
    "    file_path = processed_data_path / filename\n",
    "    if file_path.exists():\n",
    "        try:\n",
    "            if filename.endswith('.parquet'):\n",
    "                df_processed = pd.read_parquet(file_path)\n",
    "            else:\n",
    "                df_processed = pd.read_csv(file_path)\n",
    "            \n",
    "            loaded_file = filename\n",
    "            print(f\"✅ Cargado exitosamente: {filename}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error cargando {filename}: {e}\")\n",
    "\n",
    "# Fallback a datos limpios si no se encuentra data procesada\n",
    "if df_processed is None:\n",
    "    print(\"⚠️ No se encontraron datos procesados, usando datos limpios como fallback\")\n",
    "    df_processed = df_clean.copy()\n",
    "    loaded_file = \"cleaned_data (fallback)\"\n",
    "\n",
    "print(f\"📊 Datos cargados desde: {loaded_file}\")\n",
    "print(f\"   Dimensiones: {df_processed.shape[0]:,} filas x {df_processed.shape[1]} columnas\")\n",
    "\n",
    "# Verificar integridad de datos procesados\n",
    "print(f\"\\n🔍 Verificando datos procesados:\")\n",
    "print(f\"   Target '{TARGET}' presente: {TARGET in df_processed.columns}\")\n",
    "print(f\"   Valores nulos totales: {df_processed.isnull().sum().sum():,}\")\n",
    "\n",
    "# Limpiar datos si es necesario\n",
    "if TARGET in df_processed.columns:\n",
    "    initial_rows = len(df_processed)\n",
    "    df_processed = df_processed.dropna(subset=[TARGET])\n",
    "    if initial_rows != len(df_processed):\n",
    "        print(f\"   Limpieza: {initial_rows} → {len(df_processed)} filas\")\n",
    "\n",
    "# Actualizar configuraciones para datos procesados\n",
    "categorical_cols = df_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Preparar features para modelado (excluir target y columnas problemáticas)\n",
    "available_features = [col for col in df_processed.columns \n",
    "                     if col not in (DROP_COLS + [TARGET])]\n",
    "\n",
    "print(f\"   Features disponibles: {len(available_features)}\")\n",
    "print(f\"   Categóricas: {len(categorical_cols)}, Numéricas: {len(numerical_cols)}\")\n",
    "\n",
    "print(f\"\\n✅ Preparación de datos procesados completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4dc357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌊 Preparando datos para frameworks múltiples (H2O + Sklearn)...\n",
      "⚠️ H2O no disponible, trabajando solo con pandas/sklearn\n",
      "\n",
      "📊 Preparando datos para Sklearn...\n",
      "✅ Datos Sklearn preparados:\n",
      "   Dimensiones: 1,506,591 filas x 14 features\n",
      "   Variable objetivo: purchase_price\n",
      "\n",
      "🎯 Configuración final de modelado:\n",
      "   target_column: purchase_price\n",
      "   feature_columns: 14 elementos\n",
      "   categorical_columns: 7 elementos\n",
      "   numerical_columns: 7 elementos\n",
      "   h2o_available: False\n",
      "   total_samples: 1506591\n",
      "   total_features: 14\n",
      "\n",
      "✅ Preparación multiframework completada\n",
      "🚀 Listo para modelado con H2O: False\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# CONVERSIÓN A H2O FRAME Y PREPARACIÓN MULTIFRAMEWORK\n",
    "# ==================================================\n",
    "\n",
    "print(\"🌊 Preparando datos para frameworks múltiples (H2O + Sklearn)...\")\n",
    "\n",
    "# Convertir a H2O Frame si H2O está disponible\n",
    "if H2O_AVAILABLE:\n",
    "    print(\"🔄 Convirtiendo datos procesados a H2O Frame...\")\n",
    "    \n",
    "    try:\n",
    "        # Crear copia para H2O (evitar modificar original)\n",
    "        df_h2o_prep = df_processed.copy()\n",
    "        \n",
    "        # Limpiar datos para H2O (remover filas con nulls en target)\n",
    "        if TARGET in df_h2o_prep.columns:\n",
    "            initial_rows = len(df_h2o_prep)\n",
    "            df_h2o_prep = df_h2o_prep.dropna(subset=[TARGET])\n",
    "            cleaned_rows = len(df_h2o_prep)\n",
    "            if initial_rows != cleaned_rows:\n",
    "                print(f\"   Limpieza H2O: {initial_rows} → {cleaned_rows} filas (-{initial_rows-cleaned_rows})\")\n",
    "        \n",
    "        # Convertir DataFrame a H2O Frame\n",
    "        h2o_frame = h2o.H2OFrame(df_h2o_prep)\n",
    "        h2o_frame.set_names(list(df_h2o_prep.columns))\n",
    "        \n",
    "        # Configurar tipos de datos en H2O\n",
    "        if TARGET in df_h2o_prep.columns:\n",
    "            h2o_frame[TARGET] = h2o_frame[TARGET].asnumeric()\n",
    "        \n",
    "        # Convertir columnas categóricas en H2O\n",
    "        for col in categorical_cols:\n",
    "            if col in h2o_frame.columns:\n",
    "                try:\n",
    "                    h2o_frame[col] = h2o_frame[col].asfactor()\n",
    "                except:\n",
    "                    print(f\"   ⚠️ No se pudo convertir {col} a factor\")\n",
    "        \n",
    "        # Convertir columnas numéricas explícitamente\n",
    "        for col in numerical_cols:\n",
    "            if col in h2o_frame.columns and col != TARGET:\n",
    "                try:\n",
    "                    h2o_frame[col] = h2o_frame[col].asnumeric()\n",
    "                except:\n",
    "                    print(f\"   ⚠️ No se pudo convertir {col} a numérico\")\n",
    "        \n",
    "        print(f\"✅ H2O Frame creado exitosamente:\")\n",
    "        print(f\"   Dimensiones: {h2o_frame.nrows:,} filas x {h2o_frame.ncols} columnas\")\n",
    "        print(f\"   Memoria H2O Frame: {h2o_frame.byte_size / (1024**2):.1f} MB\")\n",
    "        \n",
    "        # Mostrar información del frame H2O\n",
    "        if TARGET in df_h2o_prep.columns:\n",
    "            print(f\"\\n📊 Información H2O Frame - Variable objetivo:\")\n",
    "            target_summary = h2o_frame[TARGET].summary()\n",
    "            target_summary.show()\n",
    "        \n",
    "        # Verificar tipos en H2O\n",
    "        print(f\"\\n🔍 Tipos de datos en H2O:\")\n",
    "        h2o_types = h2o_frame.types\n",
    "        type_counts = {}\n",
    "        for col, dtype in h2o_types.items():\n",
    "            type_counts[dtype] = type_counts.get(dtype, 0) + 1\n",
    "        \n",
    "        for dtype, count in type_counts.items():\n",
    "            print(f\"   {dtype}: {count} columnas\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creando H2O Frame: {e}\")\n",
    "        h2o_frame = None\n",
    "        H2O_AVAILABLE = False\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️ H2O no disponible, trabajando solo con pandas/sklearn\")\n",
    "    h2o_frame = None\n",
    "\n",
    "# Preparar datos para sklearn (siempre disponible como backup)\n",
    "print(f\"\\n📊 Preparando datos para Sklearn...\")\n",
    "\n",
    "# Verificar y limpiar datos para sklearn\n",
    "df_sklearn = df_processed.copy()\n",
    "\n",
    "# Remover filas con nulls en target\n",
    "if TARGET in df_sklearn.columns:\n",
    "    initial_rows = len(df_sklearn)\n",
    "    df_sklearn = df_sklearn.dropna(subset=[TARGET])\n",
    "    if initial_rows != len(df_sklearn):\n",
    "        print(f\"   Limpieza Sklearn: {initial_rows} → {len(df_sklearn)} filas\")\n",
    "\n",
    "# Filtrar features disponibles para sklearn\n",
    "available_features_sklearn = [f for f in available_features if f in df_sklearn.columns]\n",
    "\n",
    "# Verificar que tenemos datos válidos\n",
    "if not available_features_sklearn:\n",
    "    print(\"❌ Error: No hay features válidas para sklearn\")\n",
    "    print(f\"   Features solicitadas: {available_features[:10]}...\")\n",
    "    print(f\"   Columnas disponibles: {list(df_sklearn.columns)}\")\n",
    "    raise ValueError(\"No hay features válidas para modelado\")\n",
    "\n",
    "if TARGET not in df_sklearn.columns:\n",
    "    print(f\"❌ Error: Variable objetivo '{TARGET}' no está en los datos\")\n",
    "    raise ValueError(f\"Variable objetivo '{TARGET}' no encontrada\")\n",
    "\n",
    "print(f\"✅ Datos Sklearn preparados:\")\n",
    "print(f\"   Dimensiones: {len(df_sklearn):,} filas x {len(available_features_sklearn)} features\")\n",
    "print(f\"   Variable objetivo: {TARGET}\")\n",
    "\n",
    "# Configuración final para modelado\n",
    "modeling_config = {\n",
    "    'target_column': TARGET,\n",
    "    'feature_columns': available_features_sklearn,\n",
    "    'categorical_columns': [col for col in categorical_cols if col in available_features_sklearn],\n",
    "    'numerical_columns': [col for col in numerical_cols if col in available_features_sklearn],\n",
    "    'h2o_available': H2O_AVAILABLE and h2o_frame is not None,\n",
    "    'total_samples': len(df_sklearn),\n",
    "    'total_features': len(available_features_sklearn)\n",
    "}\n",
    "\n",
    "print(f\"\\n🎯 Configuración final de modelado:\")\n",
    "for key, value in modeling_config.items():\n",
    "    if isinstance(value, list) and len(value) > 5:\n",
    "        print(f\"   {key}: {len(value)} elementos\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\n✅ Preparación multiframework completada\")\n",
    "print(f\"🚀 Listo para modelado con H2O: {modeling_config['h2o_available']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77151b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# DIVISIÓN H2O FRAME Y PREPARACIÓN DISTRIBUIDA\n",
    "# ==================================================\n",
    "\n",
    "if H2O_AVAILABLE and h2o_frame is not None:\n",
    "    print(\"🌊 Preparando división H2O con datos procesados...\")\n",
    "    \n",
    "    try:\n",
    "        # Configurar división H2O usando las mismas features\n",
    "        h2o_features = [f for f in final_features if f in h2o_frame.columns]\n",
    "        \n",
    "        print(f\"   Features H2O: {len(h2o_features)}\")\n",
    "        print(f\"   Target H2O: {TARGET}\")\n",
    "        \n",
    "        # Verificar que el target esté disponible\n",
    "        if TARGET not in h2o_frame.columns:\n",
    "            print(f\"❌ Target '{TARGET}' no encontrado en H2O Frame\")\n",
    "            print(f\"   Columnas disponibles: {h2o_frame.columns}\")\n",
    "            raise ValueError(f\"Target no encontrado en H2O\")\n",
    "        \n",
    "        # División train/validation/test para H2O (80/10/10)\n",
    "        # Mantener consistencia con división sklearn cuando sea posible\n",
    "        train_ratio = 0.8\n",
    "        valid_ratio = 0.1\n",
    "        test_ratio = 0.1\n",
    "        \n",
    "        # División temporal si hay columnas de fecha en H2O\n",
    "        temporal_h2o = False\n",
    "        if temporal_columns:\n",
    "            try:\n",
    "                date_col = temporal_columns[0]\n",
    "                if date_col in h2o_frame.columns:\n",
    "                    print(f\"   Usando división temporal H2O con {date_col}\")\n",
    "                    \n",
    "                    # Ordenar por fecha para H2O\n",
    "                    h2o_frame_sorted = h2o_frame.sort(date_col)\n",
    "                    \n",
    "                    # Calcular puntos de corte\n",
    "                    total_rows = h2o_frame_sorted.nrows\n",
    "                    train_end = int(total_rows * train_ratio)\n",
    "                    valid_end = int(total_rows * (train_ratio + valid_ratio))\n",
    "                    \n",
    "                    # Crear splits temporales\n",
    "                    train_h2o = h2o_frame_sorted[:train_end, :]\n",
    "                    valid_h2o = h2o_frame_sorted[train_end:valid_end, :]\n",
    "                    test_h2o = h2o_frame_sorted[valid_end:, :]\n",
    "                    \n",
    "                    temporal_h2o = True\n",
    "                    print(f\"   ✅ División temporal H2O exitosa\")\n",
    "                    print(f\"   Train H2O: {train_h2o.nrows:,} filas\")\n",
    "                    print(f\"   Valid H2O: {valid_h2o.nrows:,} filas\") \n",
    "                    print(f\"   Test H2O: {test_h2o.nrows:,} filas\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Error división temporal H2O: {e}\")\n",
    "                temporal_h2o = False\n",
    "        \n",
    "        # División aleatoria como fallback para H2O\n",
    "        if not temporal_h2o:\n",
    "            print(f\"   Usando división aleatoria H2O\")\n",
    "            \n",
    "            # División aleatoria balanceada\n",
    "            train_h2o, valid_h2o, test_h2o = h2o_frame.split_frame(\n",
    "                ratios=[train_ratio, valid_ratio], \n",
    "                seed=42\n",
    "            )\n",
    "            \n",
    "            print(f\"   ✅ División aleatoria H2O exitosa\")\n",
    "            print(f\"   Train H2O: {train_h2o.nrows:,} filas\")\n",
    "            print(f\"   Valid H2O: {valid_h2o.nrows:,} filas\")\n",
    "            print(f\"   Test H2O: {test_h2o.nrows:,} filas\")\n",
    "        \n",
    "        # Configurar predictores y target para H2O\n",
    "        h2o_predictors = h2o_features\n",
    "        h2o_target = TARGET\n",
    "        \n",
    "        # Validar que tenemos datos suficientes\n",
    "        min_samples = 100\n",
    "        if train_h2o.nrows < min_samples:\n",
    "            print(f\"⚠️ Advertencia: Train H2O tiene solo {train_h2o.nrows} muestras\")\n",
    "        \n",
    "        # Información de la división H2O\n",
    "        print(f\"\\n📊 Configuración H2O final:\")\n",
    "        print(f\"   Predictores: {len(h2o_predictors)}\")\n",
    "        print(f\"   Target: {h2o_target}\")\n",
    "        print(f\"   División temporal: {temporal_h2o}\")\n",
    "        \n",
    "        # Verificar tipos de datos en cada split\n",
    "        print(f\"\\n\udd0d Verificación de tipos H2O:\")\n",
    "        target_type_train = train_h2o[h2o_target].type\n",
    "        print(f\"   Target type: {target_type_train}\")\n",
    "        \n",
    "        # Estadísticas del target en H2O\n",
    "        print(f\"\\n📈 Estadísticas target H2O:\")\n",
    "        train_target_summary = train_h2o[h2o_target].summary()\n",
    "        print(f\"   Train target summary disponible\")\n",
    "        \n",
    "        h2o_ready = True\n",
    "        print(f\"\\n✅ H2O Frame división completada\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en división H2O: {e}\")\n",
    "        h2o_ready = False\n",
    "        train_h2o = None\n",
    "        valid_h2o = None\n",
    "        test_h2o = None\n",
    "        h2o_predictors = []\n",
    "        h2o_target = None\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️ H2O no disponible para división\")\n",
    "    h2o_ready = False\n",
    "    train_h2o = None\n",
    "    valid_h2o = None\n",
    "    test_h2o = None\n",
    "    h2o_predictors = []\n",
    "    h2o_target = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Resumen de división multiframework\n",
    "print(f\"\\n🎯 Resumen división multiframework:\")\n",
    "print(f\"   Sklearn listo: ✅ ({len(X_train):,} train, {len(X_test):,} test)\")\n",
    "print(f\"   H2O listo: {'✅' if h2o_ready else '❌'} {f'({train_h2o.nrows:,} train)' if h2o_ready else ''}\")\n",
    "print(f\"   Features finales: {len(final_features)}\")\n",
    "\n",
    "# Guardar configuración para modelos\n",
    "model_config = {\n",
    "    'sklearn_ready': True,\n",
    "    'h2o_ready': h2o_ready,\n",
    "    'features': final_features,\n",
    "    'target': TARGET,\n",
    "    'train_size': len(X_train),\n",
    "    'test_size': len(X_test),\n",
    "    'h2o_train_size': train_h2o.nrows if h2o_ready else 0\n",
    "}\n",
    "\n",
    "print(f\"\\n🚀 Configuración almacenada para modelado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 4.3 RESUMEN FINAL Y PERSISTENCIA DE RESULTADOS\n",
    "# ==================================================\n",
    "\n",
    "print(\"📋 Generando resumen final del modelado supervisado...\")\n",
    "\n",
    "# Resumen estadístico final\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 RESUMEN EJECUTIVO - MODELADO SUPERVISADO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n🎯 CONFIGURACIÓN DEL EXPERIMENTO:\")\n",
    "print(f\"  • Dataset: Precios inmobiliarios Dinamarca\")\n",
    "print(f\"  • Tamaño total: {len(df_processed):,} observaciones\")\n",
    "print(f\"  • Training set: {len(X_train):,} observaciones ({len(X_train)/len(df_processed)*100:.1f}%)\")\n",
    "print(f\"  • Test set: {len(X_test):,} observaciones ({len(X_test)/len(df_processed)*100:.1f}%)\")\n",
    "print(f\"  • Features utilizadas: {len(available_features)} variables\")\n",
    "print(f\"  • Período temporal: {df_processed['Year'].min()}-{df_processed['Year'].max()}\")\n",
    "\n",
    "print(f\"\\n🏆 RESULTADOS PRINCIPALES:\")\n",
    "best_model_metrics = results_df[results_df['model'] == best_model_name].iloc[0]\n",
    "print(f\"  • Mejor modelo: {best_model_name}\")\n",
    "print(f\"  • RMSE: {best_model_metrics['rmse']:,.0f} DKK\")\n",
    "print(f\"  • MAE: {best_model_metrics['mae']:,.0f} DKK\")\n",
    "print(f\"  • R²: {best_model_metrics['r2']:.4f}\")\n",
    "print(f\"  • MAPE: {best_model_metrics['mape']:.2f}%\")\n",
    "\n",
    "# Contexto de precios para interpretar errores\n",
    "price_stats = y_test.describe()\n",
    "print(f\"\\n💰 CONTEXTO DE PRECIOS (conjunto de test):\")\n",
    "print(f\"  • Precio promedio: {price_stats['mean']:,.0f} DKK\")\n",
    "print(f\"  • Precio mediano: {price_stats['50%']:,.0f} DKK\")\n",
    "print(f\"  • Rango de precios: {price_stats['min']:,.0f} - {price_stats['max']:,.0f} DKK\")\n",
    "print(f\"  • Error relativo RMSE: {best_model_metrics['rmse_relative']:.1f}% del rango total\")\n",
    "\n",
    "print(f\"\\n📈 MEJORAS RESPECTO A BASELINE:\")\n",
    "baseline_metrics = results_df[results_df['model'] == 'Linear_Regression'].iloc[0]\n",
    "improvement_rmse = ((baseline_metrics['rmse'] - best_model_metrics['rmse']) / baseline_metrics['rmse']) * 100\n",
    "improvement_r2 = ((best_model_metrics['r2'] - baseline_metrics['r2']) / baseline_metrics['r2']) * 100\n",
    "\n",
    "print(f\"  • Reducción RMSE: {improvement_rmse:.1f}%\")\n",
    "print(f\"  • Mejora R²: {improvement_r2:.1f}%\")\n",
    "print(f\"  • Ahorro en error promedio: {baseline_metrics['mae'] - best_model_metrics['mae']:,.0f} DKK\")\n",
    "\n",
    "# Top features del mejor modelo\n",
    "if best_model_name == 'LightGBM':\n",
    "    best_feature_imp = model_results['lgb_feature_importance'].head(5)\n",
    "elif best_model_name == 'Random_Forest':\n",
    "    best_feature_imp = model_results['rf_feature_importance'].head(5)\n",
    "\n",
    "if best_model_name in ['LightGBM', 'Random_Forest']:\n",
    "    print(f\"\\n🎯 TOP 5 FEATURES MÁS IMPORTANTES ({best_model_name}):\")\n",
    "    for i, (_, row) in enumerate(best_feature_imp.iterrows(), 1):\n",
    "        print(f\"  {i}. {row['feature']}\")\n",
    "\n",
    "# Estadísticas del proceso de modelado\n",
    "print(f\"\\n⚙️ ESTADÍSTICAS DEL PROCESO:\")\n",
    "print(f\"  • Modelos evaluados: {len(results_df)} algoritmos\")\n",
    "print(f\"  • Tiempo de división temporal validado: ✓\")\n",
    "print(f\"  • Análisis de residuos completado: ✓\")\n",
    "print(f\"  • Feature importance analizado: ✓\")\n",
    "print(f\"  • Criterios de evaluación: 4 métricas principales\")\n",
    "\n",
    "# Guardar resumen final\n",
    "final_summary = {\n",
    "    'experiment_config': {\n",
    "        'total_samples': len(df_processed),\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'n_features': len(available_features),\n",
    "        'time_period': f\"{df_processed['Year'].min()}-{df_processed['Year'].max()}\"\n",
    "    },\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'metrics': best_model_metrics.to_dict()\n",
    "    },\n",
    "    'price_context': price_stats.to_dict(),\n",
    "    'improvements': {\n",
    "        'rmse_improvement_pct': improvement_rmse,\n",
    "        'r2_improvement_pct': improvement_r2,\n",
    "        'mae_savings_dkk': baseline_metrics['mae'] - best_model_metrics['mae']\n",
    "    },\n",
    "    'all_models_summary': results_df.to_dict('records')\n",
    "}\n",
    "\n",
    "model_results['final_summary'] = final_summary\n",
    "\n",
    "# Preparar datos para exportación\n",
    "export_data = {\n",
    "    'model_performance': results_df,\n",
    "    'best_model_name': best_model_name,\n",
    "    'feature_names': available_features,\n",
    "    'train_test_split': {\n",
    "        'train_size': len(X_train),\n",
    "        'test_size': len(X_test),\n",
    "        'split_date': None  # Se podría añadir fecha de corte si aplica\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\n💾 DATOS PREPARADOS PARA EXPORTACIÓN:\")\n",
    "print(f\"  • Métricas de rendimiento: ✓\")\n",
    "print(f\"  • Modelo recomendado: {best_model_name}\")\n",
    "print(f\"  • Features importantes: ✓\")\n",
    "print(f\"  • Configuración train/test: ✓\")\n",
    "\n",
    "print(f\"\\n✅ Modelado supervisado completado exitosamente\")\n",
    "print(f\"🎯 Recomendación: Proceder con {best_model_name} para implementación en producción\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FIN DEL ANÁLISIS DE MODELADO SUPERVISADO\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d1c76",
   "metadata": {},
   "source": [
    "## 4. Comparación de métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3f187",
   "metadata": {},
   "source": [
    "## 5. Interpretación con SHAP y/o LIME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFBigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
