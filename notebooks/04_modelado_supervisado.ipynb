{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc65558b",
   "metadata": {},
   "source": [
    "# ü§ñ Modelado Supervisado H2O + GPU + Paralelo - Precios Inmobiliarios Dinamarca\n",
    "\n",
    "**Objetivo**: Construir modelos distribuidos H2O, cl√°sicos sklearn y de √°rbol con optimizaciones GPU y paralelizaci√≥n, evaluarlos con m√©tricas s√≥lidas considerando rendimiento y velocidad.\n",
    "\n",
    "1. Divisi√≥n de Datos (Temporal)\n",
    "\n",
    "    1.1 Split temporal (80% train, 20% test)\n",
    "\n",
    "    1.2 Validaci√≥n de distribuci√≥n en train/test\n",
    "\n",
    "    1.3 Visualizaci√≥n de drift o cambios en el tiempo (si aplica)\n",
    "\n",
    "2. Modelos Estad√≠sticos\n",
    "\n",
    "    2.1 Regresi√≥n Lineal, Ridge, Lasso, ElasticNet\n",
    "\n",
    "    2.2 Ajuste con m√≠nimos cuadrados o likelihood\n",
    "\n",
    "    2.3 Comparaci√≥n de modelos con AIC/BIC (sin CV)\n",
    "\n",
    "    2.4 Diagn√≥stico de residuos y detecci√≥n de outliers\n",
    "\n",
    "3. Modelos de √Årboles\n",
    "\n",
    "    3.1 LightGBM y Random Forest (hiperpar√°metros por defecto o m√≠nimos)\n",
    "\n",
    "    3.2 Comparaci√≥n con modelos estad√≠sticos\n",
    "\n",
    "    3.3 Feature importance y explicabilidad inicial\n",
    "\n",
    "6. Evaluaci√≥n General\n",
    "\n",
    "    6.1 M√©tricas: RMSE, MAE, MAPE, R¬≤\n",
    "\n",
    "    6.2 Visualizaci√≥n de errores (residuos, pred vs real)\n",
    "\n",
    "    6.3 An√°lisis por segmentos (tipo de casa, regi√≥n)\n",
    "\n",
    "    6.4 Tabla resumen de modelos y conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd407dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/exodia/Documentos/TFBigData\n"
     ]
    }
   ],
   "source": [
    "from setup import set_project_root\n",
    "set_project_root()\n",
    "\n",
    "from config import *\n",
    "from descriptive_analysis import (\n",
    "    load_and_validate_data,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858e6ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321......... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"17.0.8\" 2023-07-18; OpenJDK Runtime Environment Temurin-17.0.8+7 (build 17.0.8+7); OpenJDK 64-Bit Server VM Temurin-17.0.8+7 (build 17.0.8+7, mixed mode, sharing)\n",
      "  Starting server from /home/exodia/miniconda3/envs/TFBigData/lib/python3.10/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpugqe8252\n",
      "  JVM stdout: /tmp/tmpugqe8252/h2o_exodia_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpugqe8252/h2o_exodia_started_from_python.err\n",
      " not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"17.0.8\" 2023-07-18; OpenJDK Runtime Environment Temurin-17.0.8+7 (build 17.0.8+7); OpenJDK 64-Bit Server VM Temurin-17.0.8+7 (build 17.0.8+7, mixed mode, sharing)\n",
      "  Starting server from /home/exodia/miniconda3/envs/TFBigData/lib/python3.10/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpugqe8252\n",
      "  JVM stdout: /tmp/tmpugqe8252/h2o_exodia_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpugqe8252/h2o_exodia_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ...  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n",
      "Warning: Your H2O cluster version is (3 months and 15 days) old.  There may be a newer version available.\n",
      "Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n",
      " successful.\n",
      "Warning: Your H2O cluster version is (3 months and 15 days) old.  There may be a newer version available.\n",
      "Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>00 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>America/Lima</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.46.0.7</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>3 months and 15 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_exodia_7tsk3w</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>6.723 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.18 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         00 secs\n",
       "H2O_cluster_timezone:       America/Lima\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.46.0.7\n",
       "H2O_cluster_version_age:    3 months and 15 days\n",
       "H2O_cluster_name:           H2O_from_python_exodia_7tsk3w\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    6.723 Gb\n",
       "H2O_cluster_total_cores:    16\n",
       "H2O_cluster_allowed_cores:  16\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.18 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importando datos desde /home/exodia/Documentos/TFBigData/data/processed/cleaned_data.parquet\n",
      "\n",
      "Parse progress: |Parse progress: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| (done) 100%\n",
      "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| (done) 100%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_clean, h2o_frame \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLEAN_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdestination_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDESTINATION_FRAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documentos/TFBigData/src/descriptive_analysis.py:61\u001b[0m, in \u001b[0;36mload_and_validate_data\u001b[0;34m(data_path, destination_frame, parallel)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImportando datos desde \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m df_h2o \u001b[38;5;241m=\u001b[39m h2o\u001b[38;5;241m.\u001b[39mimport_file(\n\u001b[1;32m     55\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(data_path),\n\u001b[1;32m     56\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     57\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m     destination_frame\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(destination_frame)\n\u001b[1;32m     59\u001b[0m )\n\u001b[0;32m---> 61\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m \u001b[43mdf_h2o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_data_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatos importados a H2O con destino: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_frame\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensiones del H2OFrame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_h2o\u001b[38;5;241m.\u001b[39mnrows\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m filas √ó \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_h2o\u001b[38;5;241m.\u001b[39mncols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columnas\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/h2o/frame.py:1986\u001b[0m, in \u001b[0;36mH2OFrame.as_data_frame\u001b[0;34m(self, use_pandas, header, use_multi_thread)\u001b[0m\n\u001b[1;32m   1982\u001b[0m                     os\u001b[38;5;241m.\u001b[39munlink(exportFile\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1983\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting H2O frame to pandas dataframe using single-thread.  For faster conversion using\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1984\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m multi-thread, install polars and pyarrow and use it as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1985\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas_df = h2o_df.as_data_frame(use_multi_thread=True)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, H2ODependencyWarning)\n\u001b[0;32m-> 1986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas\u001b[38;5;241m.\u001b[39mread_csv(StringIO(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_frame_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m), low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, skip_blank_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)                \n\u001b[1;32m   1988\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mh2o\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reader\n\u001b[1;32m   1989\u001b[0m frame \u001b[38;5;241m=\u001b[39m [row \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader(StringIO(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_frame_data()))]\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/h2o/frame.py:2047\u001b[0m, in \u001b[0;36mH2OFrame.get_frame_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_frame_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;124;03m    Get frame data as a string in csv format.\u001b[39;00m\n\u001b[1;32m   2036\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;124;03m    >>> iris.get_frame_data()\u001b[39;00m\n\u001b[1;32m   2046\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2047\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mh2o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET /3/DownloadDataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mframe_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhex_string\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mescape_quotes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/h2o/h2o.py:123\u001b[0m, in \u001b[0;36mapi\u001b[0;34m(endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# type checks are performed in H2OConnection class\u001b[39;00m\n\u001b[1;32m    122\u001b[0m _check_connection()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mh2oconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/h2o/backend/connection.py:494\u001b[0m, in \u001b[0;36mH2OConnection.request\u001b[0;34m(self, endpoint, data, json, filename, save_to)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_start_transaction(endpoint, rd, json, filename, params)\n\u001b[1;32m    493\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_args()\n\u001b[0;32m--> 494\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(save_to, types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[1;32m    497\u001b[0m     save_to \u001b[38;5;241m=\u001b[39m save_to(resp)\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/urllib3/response.py:1088\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1088\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/urllib3/response.py:1251\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1251\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   1253\u001b[0m     chunk, decode_content\u001b[38;5;241m=\u001b[39mdecode_content, flush_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1254\u001b[0m )\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoded:\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/site-packages/urllib3/response.py:1198\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# amt > self.chunk_left\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m-> 1198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr] # Toss the CRLF at the end of the chunk.\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m returned_chunk\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/http/client.py:631\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[1;32m    625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[0;32m~/miniconda3/envs/TFBigData/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_clean, h2o_frame = load_and_validate_data(\n",
    "    data_path=CLEAN_FILE, \n",
    "    destination_frame=DESTINATION_FRAME, \n",
    "    parallel=False\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e609c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Cargando datos procesados desde Feature Engineering (Notebook 03)...\n",
      "üìÇ Directorio procesados: /home/exodia/Documentos/TFBigData/data/processed\n",
      "üîÑ Intentando cargar: cleaned_data.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos procesados cargados desde: cleaned_data.parquet\n",
      "   Dimensiones: 1,506,591 filas x 19 columnas\n",
      "\n",
      "üîç Verificando estructura de datos procesados:\n",
      "   Columnas totales: 19\n",
      "   Tipos de datos: {dtype('int64'): 8, dtype('O'): 8, dtype('float64'): 3}\n",
      "‚úÖ Variable objetivo 'purchase_price' encontrada\n",
      "   Estad√≠sticas del target:\n",
      "     Media: 1915290.96\n",
      "     Mediana: 1400000.00\n",
      "     Std: 1765457.84\n",
      "     Valores nulos: 0\n",
      "\n",
      "üìä Calidad de datos procesados:\n",
      "   Valores nulos totales: 0\n",
      "\n",
      "üè∑Ô∏è Tipos de variables identificadas:\n",
      "   Categ√≥ricas: 8\n",
      "   Num√©ricas: 11\n",
      "\n",
      "üéØ Features para modelado:\n",
      "   Total disponibles: 14\n",
      "   Excluidas: ['purchase_price', 'house_id', 'address', 'sqm_price', '%_change_between_offer_and_purchase']\n",
      "\n",
      "üìã Muestra de datos procesados:\n",
      "   Valores nulos totales: 0\n",
      "\n",
      "üè∑Ô∏è Tipos de variables identificadas:\n",
      "   Categ√≥ricas: 8\n",
      "   Num√©ricas: 11\n",
      "\n",
      "üéØ Features para modelado:\n",
      "   Total disponibles: 14\n",
      "   Excluidas: ['purchase_price', 'house_id', 'address', 'sqm_price', '%_change_between_offer_and_purchase']\n",
      "\n",
      "üìã Muestra de datos procesados:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "quarter",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "house_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "house_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sales_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year_build",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "purchase_price",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "%_change_between_offer_and_purchase",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "no_rooms",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sqm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sqm_price",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "address",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "zip_code",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "city",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "area",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "region",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "nom_interest_rate%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "dk_ann_infl_rate%",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "yield_on_mortgage_credit_bonds%",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "9f24cdd0-394b-4ad3-a437-3ce6164c1247",
       "rows": [
        [
         "0",
         "1727654400000000000",
         "218",
         "1300",
         "Apartment",
         "regular_sale",
         "1971",
         "1765000",
         "0",
         "3",
         "78.0",
         "22628.205078125",
         "Hedek√¶ret 38, 1. th",
         "2640",
         "Hedehusene",
         "Capital, Copenhagen",
         "Zealand",
         "3.349999904632568",
         "1.1299999952316284",
         "4.340000152587891"
        ],
        [
         "1",
         "1727654400000000000",
         "218",
         "1307",
         "Summerhouse",
         "regular_sale",
         "2009",
         "590939",
         "0",
         "3",
         "50.0",
         "11818.7802734375",
         "Violstien 11",
         "2635",
         "Ish√∏j",
         "Capital, Copenhagen",
         "Zealand",
         "3.349999904632568",
         "1.1299999952316284",
         "4.340000152587891"
        ],
        [
         "2",
         "1727654400000000000",
         "218",
         "1301",
         "Apartment",
         "regular_sale",
         "1940",
         "1750000",
         "0",
         "2",
         "56.0",
         "31250.0",
         "Buddingevej 72I, st. tv",
         "2800",
         "Kongens Lyngby",
         "Capital, Copenhagen",
         "Zealand",
         "3.349999904632568",
         "1.1299999952316284",
         "4.340000152587891"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>quarter</th>\n",
       "      <th>house_id</th>\n",
       "      <th>house_type</th>\n",
       "      <th>sales_type</th>\n",
       "      <th>year_build</th>\n",
       "      <th>purchase_price</th>\n",
       "      <th>%_change_between_offer_and_purchase</th>\n",
       "      <th>no_rooms</th>\n",
       "      <th>sqm</th>\n",
       "      <th>sqm_price</th>\n",
       "      <th>address</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>city</th>\n",
       "      <th>area</th>\n",
       "      <th>region</th>\n",
       "      <th>nom_interest_rate%</th>\n",
       "      <th>dk_ann_infl_rate%</th>\n",
       "      <th>yield_on_mortgage_credit_bonds%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1727654400000000000</td>\n",
       "      <td>218</td>\n",
       "      <td>1300</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>regular_sale</td>\n",
       "      <td>1971</td>\n",
       "      <td>1765000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>78.0</td>\n",
       "      <td>22628.205078</td>\n",
       "      <td>Hedek√¶ret 38, 1. th</td>\n",
       "      <td>2640</td>\n",
       "      <td>Hedehusene</td>\n",
       "      <td>Capital, Copenhagen</td>\n",
       "      <td>Zealand</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.1299999952316284</td>\n",
       "      <td>4.340000152587891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1727654400000000000</td>\n",
       "      <td>218</td>\n",
       "      <td>1307</td>\n",
       "      <td>Summerhouse</td>\n",
       "      <td>regular_sale</td>\n",
       "      <td>2009</td>\n",
       "      <td>590939</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11818.780273</td>\n",
       "      <td>Violstien 11</td>\n",
       "      <td>2635</td>\n",
       "      <td>Ish√∏j</td>\n",
       "      <td>Capital, Copenhagen</td>\n",
       "      <td>Zealand</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.1299999952316284</td>\n",
       "      <td>4.340000152587891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1727654400000000000</td>\n",
       "      <td>218</td>\n",
       "      <td>1301</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>regular_sale</td>\n",
       "      <td>1940</td>\n",
       "      <td>1750000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>56.0</td>\n",
       "      <td>31250.000000</td>\n",
       "      <td>Buddingevej 72I, st. tv</td>\n",
       "      <td>2800</td>\n",
       "      <td>Kongens Lyngby</td>\n",
       "      <td>Capital, Copenhagen</td>\n",
       "      <td>Zealand</td>\n",
       "      <td>3.35</td>\n",
       "      <td>1.1299999952316284</td>\n",
       "      <td>4.340000152587891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  quarter  house_id   house_type    sales_type  \\\n",
       "0  1727654400000000000      218      1300    Apartment  regular_sale   \n",
       "1  1727654400000000000      218      1307  Summerhouse  regular_sale   \n",
       "2  1727654400000000000      218      1301    Apartment  regular_sale   \n",
       "\n",
       "   year_build  purchase_price  %_change_between_offer_and_purchase  no_rooms  \\\n",
       "0        1971         1765000                                    0         3   \n",
       "1        2009          590939                                    0         3   \n",
       "2        1940         1750000                                    0         2   \n",
       "\n",
       "    sqm     sqm_price                  address  zip_code            city  \\\n",
       "0  78.0  22628.205078      Hedek√¶ret 38, 1. th      2640      Hedehusene   \n",
       "1  50.0  11818.780273             Violstien 11      2635           Ish√∏j   \n",
       "2  56.0  31250.000000  Buddingevej 72I, st. tv      2800  Kongens Lyngby   \n",
       "\n",
       "                  area   region  nom_interest_rate%   dk_ann_infl_rate%  \\\n",
       "0  Capital, Copenhagen  Zealand                3.35  1.1299999952316284   \n",
       "1  Capital, Copenhagen  Zealand                3.35  1.1299999952316284   \n",
       "2  Capital, Copenhagen  Zealand                3.35  1.1299999952316284   \n",
       "\n",
       "  yield_on_mortgage_credit_bonds%  \n",
       "0               4.340000152587891  \n",
       "1               4.340000152587891  \n",
       "2               4.340000152587891  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Carga de datos procesados completada\n",
      "üöÄ Listo para conversi√≥n multiframework\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "print(f\"üìÇ Directorio procesados: {PROCESSED_DIR}\")\n",
    "\n",
    "# Intentar cargar datos procesados con m√∫ltiples formatos\n",
    "df_processed = None\n",
    "processed_files = [\n",
    "    PROCESSED_DIR / \"cleaned_data.parquet\",\n",
    "    PROCESSED_DIR / \"processed_data.parquet\", \n",
    "    PROCESSED_DIR / \"feature_engineered_data.parquet\",\n",
    "    PROCESSED_DIR / \"final_data.parquet\",\n",
    "    PROCESSED_DIR / \"cleaned_data.csv\"\n",
    "]\n",
    "\n",
    "for file_path in processed_files:\n",
    "    if file_path.exists():\n",
    "        try:\n",
    "            print(f\"üîÑ Intentando cargar: {file_path.name}\")\n",
    "            \n",
    "            if file_path.suffix == '.parquet':\n",
    "                df_processed = pd.read_parquet(file_path)\n",
    "            elif file_path.suffix == '.csv':\n",
    "                df_processed = pd.read_csv(file_path)\n",
    "            \n",
    "            print(f\"‚úÖ Datos procesados cargados desde: {file_path.name}\")\n",
    "            print(f\"   Dimensiones: {df_processed.shape[0]:,} filas x {df_processed.shape[1]} columnas\")\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando {file_path.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Si no se encuentran datos procesados, usar datos limpios como fallback\n",
    "if df_processed is None:\n",
    "    print(\"‚ö†Ô∏è No se encontraron datos procesados, usando datos limpios como fallback\")\n",
    "    df_processed = df_clean.copy()\n",
    "    print(f\"   Usando datos limpios: {df_processed.shape[0]:,} filas x {df_processed.shape[1]} columnas\")\n",
    "\n",
    "# Verificar estructura de datos procesados\n",
    "print(f\"\\nüîç Verificando estructura de datos procesados:\")\n",
    "print(f\"   Columnas totales: {len(df_processed.columns)}\")\n",
    "print(f\"   Tipos de datos: {df_processed.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "# Verificar si el target est√° presente\n",
    "if TARGET in df_processed.columns:\n",
    "    print(f\"‚úÖ Variable objetivo '{TARGET}' encontrada\")\n",
    "    print(f\"   Estad√≠sticas del target:\")\n",
    "    print(f\"     Media: {df_processed[TARGET].mean():.2f}\")\n",
    "    print(f\"     Mediana: {df_processed[TARGET].median():.2f}\")\n",
    "    print(f\"     Std: {df_processed[TARGET].std():.2f}\")\n",
    "    print(f\"     Valores nulos: {df_processed[TARGET].isnull().sum()}\")\n",
    "else:\n",
    "    print(f\"‚ùå Variable objetivo '{TARGET}' NO encontrada\")\n",
    "    print(f\"   Columnas disponibles: {list(df_processed.columns)}\")\n",
    "\n",
    "# Verificar calidad de datos\n",
    "print(f\"\\nüìä Calidad de datos procesados:\")\n",
    "total_nulls = df_processed.isnull().sum().sum()\n",
    "print(f\"   Valores nulos totales: {total_nulls:,}\")\n",
    "\n",
    "if total_nulls > 0:\n",
    "    null_cols = df_processed.isnull().sum()\n",
    "    null_cols = null_cols[null_cols > 0].sort_values(ascending=False)\n",
    "    print(f\"   Columnas con nulls: {len(null_cols)}\")\n",
    "    for col, nulls in null_cols.head(10).items():\n",
    "        print(f\"     {col}: {nulls:,} nulls ({nulls/len(df_processed)*100:.1f}%)\")\n",
    "\n",
    "# Identificar tipos de columnas para el procesamiento\n",
    "categorical_cols = df_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Tipos de variables identificadas:\")\n",
    "print(f\"   Categ√≥ricas: {len(categorical_cols)}\")\n",
    "print(f\"   Num√©ricas: {len(numerical_cols)}\")\n",
    "\n",
    "# Preparar lista de features excluyendo columnas problem√°ticas\n",
    "features_to_exclude = [TARGET] + DROP_COLS\n",
    "if 'year' in df_processed.columns:\n",
    "    features_to_exclude.append('year')\n",
    "if 'Year' in df_processed.columns:\n",
    "    features_to_exclude.append('Year')\n",
    "\n",
    "# Lista final de features disponibles\n",
    "available_features = [col for col in df_processed.columns \n",
    "                     if col not in features_to_exclude]\n",
    "\n",
    "print(f\"\\nüéØ Features para modelado:\")\n",
    "print(f\"   Total disponibles: {len(available_features)}\")\n",
    "print(f\"   Excluidas: {features_to_exclude}\")\n",
    "\n",
    "# Mostrar muestra de los datos procesados\n",
    "print(f\"\\nüìã Muestra de datos procesados:\")\n",
    "display(df_processed.head(3))\n",
    "\n",
    "print(f\"\\n‚úÖ Carga de datos procesados completada\")\n",
    "print(f\"üöÄ Listo para conversi√≥n multiframework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ec63a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Verificando configuraci√≥n GPU y paralelizaci√≥n...\n",
      "‚ö†Ô∏è PyTorch no instalado - verificaci√≥n CUDA limitada\n",
      "‚úÖ LightGBM GPU: Disponible\n",
      "üíª CONFIGURACI√ìN CPU:\n",
      "   Cores f√≠sicos: 16\n",
      "   Cores l√≥gicos: 16\n",
      "   Memoria total: 26.9 GB\n",
      "   Memoria disponible: 5.3 GB\n",
      "üîß CONFIGURACI√ìN PARALELA:\n",
      "   OMP_NUM_THREADS: 16\n",
      "   MKL_NUM_THREADS: 16\n",
      "\n",
      "üìã ESTRATEGIA DE ENTRENAMIENTO:\n",
      "   GPU para LightGBM: ‚úÖ S√≠\n",
      "   Cores para paralelizaci√≥n: 16\n",
      "   L√≠mite de memoria: 5.3 GB\n",
      "   Procesamiento en batches: ‚úÖ S√≠\n",
      "\n",
      "‚öôÔ∏è CONFIGURACI√ìN NUMPY:\n",
      "   Informaci√≥n BLAS/LAPACK no disponible\n",
      "\n",
      "‚úÖ Verificaci√≥n de configuraci√≥n completada\n",
      "üöÄ Sistema optimizado para entrenamiento de alta velocidad\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# VERIFICACI√ìN DE CONFIGURACI√ìN OPTIMIZADA\n",
    "# ==================================================\n",
    "\n",
    "print(\"üîß Verificando configuraci√≥n GPU y paralelizaci√≥n...\")\n",
    "\n",
    "# Verificar configuraci√≥n de GPU\n",
    "def check_gpu_setup():\n",
    "    \"\"\"Verifica disponibilidad y configuraci√≥n de GPU\"\"\"\n",
    "    gpu_info = {\n",
    "        'cuda_available': False,\n",
    "        'gpu_count': 0,\n",
    "        'gpu_names': [],\n",
    "        'lightgbm_gpu': False\n",
    "    }\n",
    "    \n",
    "    # Verificar CUDA/GPU\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_info['cuda_available'] = True\n",
    "            gpu_info['gpu_count'] = torch.cuda.device_count()\n",
    "            gpu_info['gpu_names'] = [torch.cuda.get_device_name(i) for i in range(gpu_info['gpu_count'])]\n",
    "        print(f\"üéÆ CUDA disponible: {gpu_info['cuda_available']}\")\n",
    "        if gpu_info['cuda_available']:\n",
    "            print(f\"   GPUs detectadas: {gpu_info['gpu_count']}\")\n",
    "            for i, name in enumerate(gpu_info['gpu_names']):\n",
    "                print(f\"   GPU {i}: {name}\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è PyTorch no instalado - verificaci√≥n CUDA limitada\")\n",
    "    \n",
    "    # Verificar LightGBM GPU\n",
    "    try:\n",
    "        test_model = lgb.LGBMRegressor(device='gpu', gpu_platform_id=0, gpu_device_id=0)\n",
    "        gpu_info['lightgbm_gpu'] = True\n",
    "        print(\"‚úÖ LightGBM GPU: Disponible\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è LightGBM GPU: No disponible - {str(e)[:100]}\")\n",
    "    \n",
    "    return gpu_info\n",
    "\n",
    "# Verificar configuraci√≥n de paralelizaci√≥n\n",
    "def check_parallel_setup():\n",
    "    \"\"\"Verifica configuraci√≥n de paralelizaci√≥n\"\"\"\n",
    "    import psutil\n",
    "    \n",
    "    parallel_info = {\n",
    "        'cpu_count': mp.cpu_count(),\n",
    "        'physical_cores': psutil.cpu_count(logical=False),\n",
    "        'logical_cores': psutil.cpu_count(logical=True),\n",
    "        'memory_gb': round(psutil.virtual_memory().total / (1024**3), 1),\n",
    "        'available_memory_gb': round(psutil.virtual_memory().available / (1024**3), 1)\n",
    "    }\n",
    "    \n",
    "    print(f\"üíª CONFIGURACI√ìN CPU:\")\n",
    "    print(f\"   Cores f√≠sicos: {parallel_info['physical_cores']}\")\n",
    "    print(f\"   Cores l√≥gicos: {parallel_info['logical_cores']}\")\n",
    "    print(f\"   Memoria total: {parallel_info['memory_gb']} GB\")\n",
    "    print(f\"   Memoria disponible: {parallel_info['available_memory_gb']} GB\")\n",
    "    \n",
    "    # Verificar configuraci√≥n de librer√≠as paralelas\n",
    "    print(f\"üîß CONFIGURACI√ìN PARALELA:\")\n",
    "    print(f\"   OMP_NUM_THREADS: {os.environ.get('OMP_NUM_THREADS', 'No configurado')}\")\n",
    "    print(f\"   MKL_NUM_THREADS: {os.environ.get('MKL_NUM_THREADS', 'No configurado')}\")\n",
    "    \n",
    "    return parallel_info\n",
    "\n",
    "# Ejecutar verificaciones\n",
    "gpu_config = check_gpu_setup()\n",
    "parallel_config = check_parallel_setup()\n",
    "\n",
    "# Configurar estrategia de entrenamiento basada en recursos disponibles\n",
    "TRAINING_STRATEGY = {\n",
    "    'use_gpu': gpu_config['lightgbm_gpu'],\n",
    "    'max_cores': parallel_config['logical_cores'],\n",
    "    'memory_limit': parallel_config['available_memory_gb'],\n",
    "    'batch_processing': parallel_config['available_memory_gb'] < 8  # Si memoria < 8GB, usar batches\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã ESTRATEGIA DE ENTRENAMIENTO:\")\n",
    "print(f\"   GPU para LightGBM: {'‚úÖ S√≠' if TRAINING_STRATEGY['use_gpu'] else '‚ùå No'}\")\n",
    "print(f\"   Cores para paralelizaci√≥n: {TRAINING_STRATEGY['max_cores']}\")\n",
    "print(f\"   L√≠mite de memoria: {TRAINING_STRATEGY['memory_limit']} GB\")\n",
    "print(f\"   Procesamiento en batches: {'‚úÖ S√≠' if TRAINING_STRATEGY['batch_processing'] else '‚ùå No'}\")\n",
    "\n",
    "# Optimizar configuraci√≥n de numpy/scipy para rendimiento\n",
    "import numpy as np\n",
    "if hasattr(np, '__config__'):\n",
    "    print(f\"\\n‚öôÔ∏è CONFIGURACI√ìN NUMPY:\")\n",
    "    try:\n",
    "        print(f\"   BLAS: {np.__config__.get_info('blas_info', {}).get('name', 'Desconocido')}\")\n",
    "        print(f\"   LAPACK: {np.__config__.get_info('lapack_info', {}).get('name', 'Desconocido')}\")\n",
    "    except:\n",
    "        print(\"   Informaci√≥n BLAS/LAPACK no disponible\")\n",
    "\n",
    "print(f\"\\n‚úÖ Verificaci√≥n de configuraci√≥n completada\")\n",
    "print(f\"üöÄ Sistema optimizado para entrenamiento de alta velocidad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf631ee",
   "metadata": {},
   "source": [
    "## 1. Divisi√≥n de Datos (Temporal)\n",
    "\n",
    "### Estrategia de Divisi√≥n Temporal\n",
    "- **Train**: 80% de los datos m√°s antiguos (1992-2019)\n",
    "- **Test**: 20% de los datos m√°s recientes (2020-2024)\n",
    "- **Validaci√≥n**: Sin data leakage temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7daac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Implementando divisi√≥n temporal optimizada para H2O...\n",
      "‚ùå Error: No se encontr√≥ la columna 'year' en el dataset\n",
      "   Se requiere informaci√≥n temporal para divisi√≥n correcta\n",
      "\n",
      "‚úÖ Divisi√≥n temporal completada exitosamente\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# 1.1 DIVISI√ìN TEMPORAL H2O + SKLEARN (80% TRAIN, 20% TEST)\n",
    "# ==================================================\n",
    "\n",
    "print(\"üìÖ Implementando divisi√≥n temporal optimizada para H2O...\")\n",
    "\n",
    "# Verificar si tenemos columna temporal\n",
    "if 'year' in df_clean.columns:\n",
    "    # Informaci√≥n temporal\n",
    "    year_min, year_max = df_clean['year'].min(), df_clean['year'].max()\n",
    "    print(f\"üìä Rango temporal completo: {year_min} - {year_max}\")\n",
    "    \n",
    "    # Divisi√≥n temporal: 80% para train, 20% para test\n",
    "    years_unique = sorted(df_clean['year'].unique())\n",
    "    n_years = len(years_unique)\n",
    "    split_index = int(n_years * 0.8)\n",
    "    split_year = years_unique[split_index]\n",
    "    \n",
    "    print(f\"üîÑ Divisi√≥n temporal:\")\n",
    "    print(f\"  Train: {year_min} - {split_year-1} ({split_index} a√±os)\")\n",
    "    print(f\"  Test:  {split_year} - {year_max} ({n_years - split_index} a√±os)\")\n",
    "    \n",
    "    # Crear m√°scaras para train/test\n",
    "    train_mask = df_clean['year'] < split_year\n",
    "    test_mask = df_clean['year'] >= split_year\n",
    "    \n",
    "    # Divisi√≥n para H2O\n",
    "    if H2O_AVAILABLE and h2o_frame is not None:\n",
    "        print(\"üöÄ Creando splits en H2O...\")\n",
    "        \n",
    "        # Dividir H2O Frame\n",
    "        h2o_train = h2o_frame[h2o_frame['year'] < split_year, :]\n",
    "        h2o_test = h2o_frame[h2o_frame['year'] >= split_year, :]\n",
    "        \n",
    "        # Definir predictores (excluir target y columnas auxiliares)\n",
    "        h2o_predictors = [col for col in h2o_frame.columns \n",
    "                         if col not in [TARGET, 'year'] + DROP_COLS]\n",
    "        \n",
    "        print(f\"‚úÖ H2O splits creados:\")\n",
    "        print(f\"  Train: {h2o_train.nrows:,} filas\")\n",
    "        print(f\"  Test:  {h2o_test.nrows:,} filas\")\n",
    "        print(f\"  Predictores: {len(h2o_predictors)} variables\")\n",
    "    \n",
    "    # Divisi√≥n para sklearn (backup/comparaci√≥n)\n",
    "    print(\"üìä Creando splits para sklearn...\")\n",
    "    \n",
    "    # Filtrar features disponibles para sklearn\n",
    "    available_features_sklearn = [f for f in available_features if f in df_clean.columns]\n",
    "    \n",
    "    # Crear conjuntos de datos para sklearn\n",
    "    X_train = df_clean[train_mask][available_features_sklearn]\n",
    "    X_test = df_clean[test_mask][available_features_sklearn]\n",
    "    y_train = df_clean[train_mask][TARGET]\n",
    "    y_test = df_clean[test_mask][TARGET]\n",
    "    \n",
    "    # Informaci√≥n de la divisi√≥n\n",
    "    train_size = len(X_train)\n",
    "    test_size = len(X_test)\n",
    "    total_size = train_size + test_size\n",
    "    \n",
    "    print(f\"‚úÖ Sklearn splits creados:\")\n",
    "    print(f\"  Train: {train_size:,} observaciones ({train_size/total_size*100:.1f}%)\")\n",
    "    print(f\"  Test:  {test_size:,} observaciones ({test_size/total_size*100:.1f}%)\")\n",
    "    print(f\"  Features: {len(available_features_sklearn)} variables\")\n",
    "    \n",
    "    # Verificar distribuci√≥n temporal\n",
    "    train_years = df_clean[train_mask]['year'].value_counts().sort_index()\n",
    "    test_years = df_clean[test_mask]['year'].value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\nüìä Distribuci√≥n temporal verificada:\")\n",
    "    print(f\"  Train a√±os: {train_years.index.min()} - {train_years.index.max()}\")\n",
    "    print(f\"  Test a√±os:  {test_years.index.min()} - {test_years.index.max()}\")\n",
    "    \n",
    "    # Guardar informaci√≥n de split\n",
    "    split_info = {\n",
    "        'split_year': split_year,\n",
    "        'train_years': (train_years.index.min(), train_years.index.max()),\n",
    "        'test_years': (test_years.index.min(), test_years.index.max()),\n",
    "        'train_size': train_size,\n",
    "        'test_size': test_size,\n",
    "        'h2o_available': H2O_AVAILABLE,\n",
    "        'h2o_predictors': h2o_predictors if H2O_AVAILABLE else None\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Error: No se encontr√≥ la columna 'year' en el dataset\")\n",
    "    print(\"   Se requiere informaci√≥n temporal para divisi√≥n correcta\")\n",
    "\n",
    "print(f\"\\n‚úÖ Divisi√≥n temporal completada exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec9146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∞ Ejecutando divisi√≥n temporal con datos procesados del notebook 03...\n",
      "‚úÖ Datos procesados disponibles: (1506591, 19)\n",
      "   Target: purchase_price\n",
      "   Features disponibles: 14\n",
      "üìä Datos limpios para modelado:\n",
      "   Muestras v√°lidas: 1,506,591\n",
      "   Features: 14\n",
      "üïê Usando divisi√≥n temporal con columna: date\n",
      "   ‚úÖ Divisi√≥n temporal exitosa\n",
      "   Per√≠odo train: 694569600000000000 a 1619222400000000000\n",
      "   Per√≠odo test: 1619222400000000000 a 1727654400000000000\n",
      "\n",
      "üìà Resultados de la divisi√≥n:\n",
      "   Train: 1,205,272 muestras (80.0%)\n",
      "   Test: 301,319 muestras (20.0%)\n",
      "\n",
      "üéØ Estad√≠sticas del target 'purchase_price':\n",
      "   Train - Media: 1762487.06, Std: 1651134.06\n",
      "   Test - Media: 2526504.54, Std: 2052379.56\n",
      "   Diferencia medias: 764017.48\n",
      "\n",
      "‚úÖ Divisi√≥n de datos completada\n",
      "üöÄ Variables train/test listas para modelado\n",
      "   ‚úÖ Divisi√≥n temporal exitosa\n",
      "   Per√≠odo train: 694569600000000000 a 1619222400000000000\n",
      "   Per√≠odo test: 1619222400000000000 a 1727654400000000000\n",
      "\n",
      "üìà Resultados de la divisi√≥n:\n",
      "   Train: 1,205,272 muestras (80.0%)\n",
      "   Test: 301,319 muestras (20.0%)\n",
      "\n",
      "üéØ Estad√≠sticas del target 'purchase_price':\n",
      "   Train - Media: 1762487.06, Std: 1651134.06\n",
      "   Test - Media: 2526504.54, Std: 2052379.56\n",
      "   Diferencia medias: 764017.48\n",
      "\n",
      "‚úÖ Divisi√≥n de datos completada\n",
      "üöÄ Variables train/test listas para modelado\n"
     ]
    }
   ],
   "source": [
    "# DIVISI√ìN TEMPORAL DE DATOS PROCESADOS\n",
    "\n",
    "print(\"‚è∞ Ejecutando divisi√≥n temporal con datos procesados del notebook 03...\")\n",
    "\n",
    "# Verificar que tenemos los datos procesados\n",
    "if 'df_processed' not in locals():\n",
    "    raise ValueError(\"‚ùå df_processed no est√° definido. Ejecutar primero la celda de carga de datos.\")\n",
    "\n",
    "if TARGET not in df_processed.columns:\n",
    "    raise ValueError(f\"‚ùå Variable objetivo '{TARGET}' no encontrada en datos procesados\")\n",
    "\n",
    "print(f\"‚úÖ Datos procesados disponibles: {df_processed.shape}\")\n",
    "print(f\"   Target: {TARGET}\")\n",
    "print(f\"   Features disponibles: {len(available_features)}\")\n",
    "\n",
    "# Preparar datos para divisi√≥n\n",
    "X = df_processed[available_features].copy()\n",
    "y = df_processed[TARGET].copy()\n",
    "\n",
    "# Limpiar datos (remover nulls en target)\n",
    "mask_valid = y.notna()\n",
    "X = X[mask_valid]\n",
    "y = y[mask_valid]\n",
    "\n",
    "print(f\"üìä Datos limpios para modelado:\")\n",
    "print(f\"   Muestras v√°lidas: {len(X):,}\")\n",
    "print(f\"   Features: {len(available_features)}\")\n",
    "\n",
    "# Divisi√≥n temporal si existe columna de fecha\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Buscar columna temporal\n",
    "date_columns = ['date', 'Date', 'fecha', 'timestamp', 'SaleDate', 'SaleDato']\n",
    "temporal_col = None\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in df_processed.columns:\n",
    "        temporal_col = col\n",
    "        break\n",
    "\n",
    "# Divisi√≥n temporal o aleatoria\n",
    "if temporal_col is not None:\n",
    "    print(f\"üïê Usando divisi√≥n temporal con columna: {temporal_col}\")\n",
    "    \n",
    "    # Preparar datos con fechas v√°lidas\n",
    "    df_temp = df_processed[mask_valid].copy()\n",
    "    \n",
    "    try:\n",
    "        # Convertir a datetime si es necesario\n",
    "        if df_temp[temporal_col].dtype == 'object':\n",
    "            df_temp[temporal_col] = pd.to_datetime(df_temp[temporal_col], errors='coerce')\n",
    "        \n",
    "        # Filtrar fechas v√°lidas\n",
    "        date_mask = df_temp[temporal_col].notna()\n",
    "        df_temp = df_temp[date_mask]\n",
    "        \n",
    "        if len(df_temp) > 0:\n",
    "            # Ordenar por fecha y dividir temporalmente\n",
    "            df_temp = df_temp.sort_values(temporal_col)\n",
    "            split_point = int(len(df_temp) * 0.8)\n",
    "            \n",
    "            train_indices = df_temp.index[:split_point]\n",
    "            test_indices = df_temp.index[split_point:]\n",
    "            \n",
    "            X_train = X.loc[train_indices]\n",
    "            X_test = X.loc[test_indices]\n",
    "            y_train = y.loc[train_indices]\n",
    "            y_test = y.loc[test_indices]\n",
    "            \n",
    "            print(f\"   ‚úÖ Divisi√≥n temporal exitosa\")\n",
    "            print(f\"   Per√≠odo train: {df_temp[temporal_col].iloc[0]} a {df_temp[temporal_col].iloc[split_point-1]}\")\n",
    "            print(f\"   Per√≠odo test: {df_temp[temporal_col].iloc[split_point]} a {df_temp[temporal_col].iloc[-1]}\")\n",
    "        else:\n",
    "            raise ValueError(\"No hay fechas v√°lidas\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error en divisi√≥n temporal: {e}\")\n",
    "        print(f\"   Usando divisi√≥n aleatoria como fallback\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "else:\n",
    "    print(f\"üìä Usando divisi√≥n aleatoria (no se encontr√≥ columna temporal)\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "# Verificar divisi√≥n\n",
    "print(f\"\\nüìà Resultados de la divisi√≥n:\")\n",
    "print(f\"   Train: {len(X_train):,} muestras ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Test: {len(X_test):,} muestras ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Estad√≠sticas del target\n",
    "print(f\"\\nüéØ Estad√≠sticas del target '{TARGET}':\")\n",
    "print(f\"   Train - Media: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"   Test - Media: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")\n",
    "print(f\"   Diferencia medias: {abs(y_train.mean() - y_test.mean()):.2f}\")\n",
    "\n",
    "# Verificar nulls en features\n",
    "train_nulls = X_train.isnull().sum().sum()\n",
    "test_nulls = X_test.isnull().sum().sum()\n",
    "\n",
    "if train_nulls > 0 or test_nulls > 0:\n",
    "    print(f\"‚ö†Ô∏è Nulls encontrados - Train: {train_nulls}, Test: {test_nulls}\")\n",
    "    \n",
    "    # Imputaci√≥n simple si hay nulls\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    # Separar columnas por tipo\n",
    "    numeric_features = X_train.select_dtypes(include=[np.number]).columns\n",
    "    categorical_features = X_train.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    # Imputar num√©ricas con mediana\n",
    "    if len(numeric_features) > 0:\n",
    "        imputer_num = SimpleImputer(strategy='median')\n",
    "        X_train[numeric_features] = imputer_num.fit_transform(X_train[numeric_features])\n",
    "        X_test[numeric_features] = imputer_num.transform(X_test[numeric_features])\n",
    "    \n",
    "    # Imputar categ√≥ricas con moda\n",
    "    if len(categorical_features) > 0:\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        X_train[categorical_features] = imputer_cat.fit_transform(X_train[categorical_features])\n",
    "        X_test[categorical_features] = imputer_cat.transform(X_test[categorical_features])\n",
    "    \n",
    "    print(f\"‚úÖ Imputaci√≥n completada\")\n",
    "\n",
    "print(f\"\\n‚úÖ Divisi√≥n de datos completada\")\n",
    "print(f\"üöÄ Variables train/test listas para modelado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 1.3 VISUALIZACI√ìN DE DRIFT TEMPORAL\n",
    "# ==================================================\n",
    "\n",
    "print(\"üìà Analizando drift temporal en variables clave...\")\n",
    "\n",
    "def analyze_temporal_drift(df_complete, features_to_analyze, target_col):\n",
    "    \"\"\"Analiza drift temporal en variables clave\"\"\"\n",
    "    \n",
    "    # Seleccionar subset de features importantes para an√°lisis\n",
    "    important_features = features_to_analyze[:6]  # Top 6 features\n",
    "    \n",
    "    n_features = len(important_features)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 4*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
    "    \n",
    "    # An√°lisis por a√±o\n",
    "    yearly_stats = df_complete.groupby('year').agg({\n",
    "        target_col: ['mean', 'std', 'count'],\n",
    "        **{feature: 'mean' for feature in important_features}\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Evoluci√≥n de la variable objetivo\n",
    "    for i, feature in enumerate(important_features):\n",
    "        if i < len(axes):\n",
    "            # Plot de evoluci√≥n temporal\n",
    "            if feature in df_complete.columns:\n",
    "                feature_yearly = df_complete.groupby('year')[feature].mean()\n",
    "                axes[i].plot(feature_yearly.index, feature_yearly.values, marker='o', linewidth=2)\n",
    "                axes[i].set_title(f'Evoluci√≥n Temporal: {feature}')\n",
    "                axes[i].set_xlabel('A√±o')\n",
    "                axes[i].set_ylabel(f'{feature}')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Marcar divisi√≥n train/test\n",
    "                axes[i].axvline(x=split_year, color='red', linestyle='--', alpha=0.7, label='Train/Test Split')\n",
    "                axes[i].legend()\n",
    "    \n",
    "    # Ocultar ejes no utilizados\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # An√°lisis espec√≠fico del precio por a√±o\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Precio promedio por a√±o\n",
    "    price_yearly = df_complete.groupby('year')[target_col].agg(['mean', 'std', 'count'])\n",
    "    \n",
    "    axes[0,0].plot(price_yearly.index, price_yearly['mean'], marker='o', linewidth=2, color='blue')\n",
    "    axes[0,0].fill_between(price_yearly.index, \n",
    "                          price_yearly['mean'] - price_yearly['std'],\n",
    "                          price_yearly['mean'] + price_yearly['std'], \n",
    "                          alpha=0.3, color='blue')\n",
    "    axes[0,0].axvline(x=split_year, color='red', linestyle='--', alpha=0.7, label='Train/Test Split')\n",
    "    axes[0,0].set_title('Evoluci√≥n del Precio Promedio ¬± 1 Std')\n",
    "    axes[0,0].set_xlabel('A√±o')\n",
    "    axes[0,0].set_ylabel('Precio (DKK)')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # N√∫mero de transacciones por a√±o\n",
    "    axes[0,1].bar(price_yearly.index, price_yearly['count'], color='green', alpha=0.7)\n",
    "    axes[0,1].axvline(x=split_year, color='red', linestyle='--', alpha=0.7, label='Train/Test Split')\n",
    "    axes[0,1].set_title('N√∫mero de Transacciones por A√±o')\n",
    "    axes[0,1].set_xlabel('A√±o')\n",
    "    axes[0,1].set_ylabel('N√∫mero de Transacciones')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Coeficiente de variaci√≥n por a√±o\n",
    "    cv_yearly = (price_yearly['std'] / price_yearly['mean']) * 100\n",
    "    axes[1,0].plot(cv_yearly.index, cv_yearly.values, marker='s', linewidth=2, color='orange')\n",
    "    axes[1,0].axvline(x=split_year, color='red', linestyle='--', alpha=0.7, label='Train/Test Split')\n",
    "    axes[1,0].set_title('Coeficiente de Variaci√≥n del Precio (%)')\n",
    "    axes[1,0].set_xlabel('A√±o')\n",
    "    axes[1,0].set_ylabel('CV (%)')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribuci√≥n de precios por per√≠odo\n",
    "    train_data = df_complete[df_complete['year'] < split_year][target_col]\n",
    "    test_data = df_complete[df_complete['year'] >= split_year][target_col]\n",
    "    \n",
    "    axes[1,1].hist(train_data, bins=50, alpha=0.7, label='Train Period', density=True, color='blue')\n",
    "    axes[1,1].hist(test_data, bins=50, alpha=0.7, label='Test Period', density=True, color='red')\n",
    "    axes[1,1].set_title('Distribuci√≥n de Precios: Train vs Test')\n",
    "    axes[1,1].set_xlabel('Precio (DKK)')\n",
    "    axes[1,1].set_ylabel('Densidad')\n",
    "    axes[1,1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # An√°lisis cuantitativo del drift\n",
    "    print(f\"\\nüìä An√°lisis cuantitativo de drift temporal:\")\n",
    "    print(f\"{'Variable':<20} {'Drift Train->Test':<20} {'Significancia':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for feature in important_features[:5]:  # Top 5 features\n",
    "        if feature in df_complete.columns:\n",
    "            train_mean = df_complete[df_complete['year'] < split_year][feature].mean()\n",
    "            test_mean = df_complete[df_complete['year'] >= split_year][feature].mean()\n",
    "            drift_pct = ((test_mean - train_mean) / train_mean) * 100 if train_mean != 0 else 0\n",
    "            \n",
    "            # Test de significancia\n",
    "            train_values = df_complete[df_complete['year'] < split_year][feature]\n",
    "            test_values = df_complete[df_complete['year'] >= split_year][feature]\n",
    "            _, p_value = stats.ttest_ind(train_values, test_values)\n",
    "            \n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "            \n",
    "            print(f\"{feature:<20} {drift_pct:<20.2f}% {significance:<15}\")\n",
    "\n",
    "# Ejecutar an√°lisis de drift\n",
    "analyze_temporal_drift(df_complete, available_features, TARGET)\n",
    "analyze_temporal_drift(df_complete, available_features, TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09965c45",
   "metadata": {},
   "source": [
    "## 3. Modelos Estad√≠sticos (Sklearn)\n",
    "\n",
    "### Metodolog√≠a Estad√≠stica Cl√°sica de Comparaci√≥n\n",
    "- **Regresi√≥n Lineal**: Baseline con m√≠nimos cuadrados\n",
    "- **Ridge**: Regularizaci√≥n L2 para multicolinealidad\n",
    "- **Lasso**: Regularizaci√≥n L1 para selecci√≥n de features\n",
    "- **ElasticNet**: Combinaci√≥n L1 + L2\n",
    "- **Evaluaci√≥n**: AIC/BIC + GridSearch paralelo para comparaci√≥n con H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c529e0",
   "metadata": {},
   "source": [
    "## 2. Modelos H2O (Distribuidos y Escalables)\n",
    "\n",
    "### Metodolog√≠a H2O AutoML y Modelos Espec√≠ficos\n",
    "- **H2O GBM**: Gradient Boosting Machine distribuido y optimizado\n",
    "- **H2O Random Forest**: Ensemble distribuido con paralelizaci√≥n autom√°tica\n",
    "- **H2O GLM**: Modelos lineales generalizados con regularizaci√≥n autom√°tica\n",
    "- **H2O Deep Learning**: Redes neuronales profundas para capturar relaciones complejas\n",
    "- **H2O AutoML**: Pipeline automatizado para selecci√≥n del mejor modelo\n",
    "- **Evaluaci√≥n**: M√©tricas nativas H2O con validaci√≥n cruzada optimizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771f1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 2.1 MODELOS H2O DISTRIBUIDOS Y ESCALABLES\n",
    "# ==================================================\n",
    "\n",
    "if H2O_AVAILABLE:\n",
    "    print(\"üöÄ Entrenando modelos H2O distribuidos...\")\n",
    "    \n",
    "    # Diccionario para almacenar modelos H2O\n",
    "    h2o_models = {}\n",
    "    h2o_results = {}\n",
    "    \n",
    "    # 1. H2O Gradient Boosting Machine (GBM)\n",
    "    print(\"\\nüî∏ Entrenando H2O GBM (Distribuido)...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    h2o_gbm = H2OGradientBoostingEstimator(\n",
    "        ntrees=100,\n",
    "        max_depth=6,\n",
    "        learn_rate=0.1,\n",
    "        sample_rate=0.8,\n",
    "        col_sample_rate=0.8,\n",
    "        stopping_rounds=10,\n",
    "        stopping_tolerance=0.001,\n",
    "        stopping_metric=\"RMSE\",\n",
    "        seed=42,\n",
    "        nfolds=5,  # Validaci√≥n cruzada\n",
    "        fold_assignment=\"Modulo\",\n",
    "        keep_cross_validation_predictions=True\n",
    "    )\n",
    "    \n",
    "    h2o_gbm.train(\n",
    "        x=h2o_predictors,\n",
    "        y=TARGET,\n",
    "        training_frame=h2o_train,\n",
    "        validation_frame=h2o_test\n",
    "    )\n",
    "    \n",
    "    gbm_time = (datetime.now() - start_time).total_seconds()\n",
    "    h2o_models['H2O_GBM'] = h2o_gbm\n",
    "    \n",
    "    # M√©tricas H2O GBM\n",
    "    train_rmse_gbm = h2o_gbm.rmse(train=True)\n",
    "    valid_rmse_gbm = h2o_gbm.rmse(valid=True)\n",
    "    train_r2_gbm = h2o_gbm.r2(train=True)\n",
    "    valid_r2_gbm = h2o_gbm.r2(valid=True)\n",
    "    \n",
    "    h2o_results['H2O_GBM'] = {\n",
    "        'train_rmse': train_rmse_gbm,\n",
    "        'test_rmse': valid_rmse_gbm,\n",
    "        'train_r2': train_r2_gbm,\n",
    "        'test_r2': valid_r2_gbm,\n",
    "        'training_time': gbm_time,\n",
    "        'cv_rmse': h2o_gbm.rmse(xval=True),\n",
    "        'cv_r2': h2o_gbm.r2(xval=True)\n",
    "    }\n",
    "    \n",
    "    print(f\"   Tiempo entrenamiento: {gbm_time:.2f}s\")\n",
    "    print(f\"   Train RMSE: {train_rmse_gbm:,.0f}\")\n",
    "    print(f\"   Test RMSE:  {valid_rmse_gbm:,.0f}\")\n",
    "    print(f\"   Test R¬≤:    {valid_r2_gbm:.4f}\")\n",
    "    print(f\"   CV RMSE:    {h2o_gbm.rmse(xval=True):,.0f}\")\n",
    "    \n",
    "    # 2. H2O Random Forest\n",
    "    print(\"\\nüî∏ Entrenando H2O Random Forest (Distribuido)...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    h2o_rf = H2ORandomForestEstimator(\n",
    "        ntrees=100,\n",
    "        max_depth=15,\n",
    "        sample_rate=0.8,\n",
    "        col_sample_rate_per_tree=0.8,\n",
    "        seed=42,\n",
    "        nfolds=5,\n",
    "        fold_assignment=\"Modulo\",\n",
    "        keep_cross_validation_predictions=True\n",
    "    )\n",
    "    \n",
    "    h2o_rf.train(\n",
    "        x=h2o_predictors,\n",
    "        y=TARGET,\n",
    "        training_frame=h2o_train,\n",
    "        validation_frame=h2o_test\n",
    "    )\n",
    "    \n",
    "    rf_time = (datetime.now() - start_time).total_seconds()\n",
    "    h2o_models['H2O_RF'] = h2o_rf\n",
    "    \n",
    "    # M√©tricas H2O RF\n",
    "    train_rmse_rf = h2o_rf.rmse(train=True)\n",
    "    valid_rmse_rf = h2o_rf.rmse(valid=True)\n",
    "    train_r2_rf = h2o_rf.r2(train=True)\n",
    "    valid_r2_rf = h2o_rf.r2(valid=True)\n",
    "    \n",
    "    h2o_results['H2O_RF'] = {\n",
    "        'train_rmse': train_rmse_rf,\n",
    "        'test_rmse': valid_rmse_rf,\n",
    "        'train_r2': train_r2_rf,\n",
    "        'test_r2': valid_r2_rf,\n",
    "        'training_time': rf_time,\n",
    "        'cv_rmse': h2o_rf.rmse(xval=True),\n",
    "        'cv_r2': h2o_rf.r2(xval=True)\n",
    "    }\n",
    "    \n",
    "    print(f\"   Tiempo entrenamiento: {rf_time:.2f}s\")\n",
    "    print(f\"   Train RMSE: {train_rmse_rf:,.0f}\")\n",
    "    print(f\"   Test RMSE:  {valid_rmse_rf:,.0f}\")\n",
    "    print(f\"   Test R¬≤:    {valid_r2_rf:.4f}\")\n",
    "    print(f\"   CV RMSE:    {h2o_rf.rmse(xval=True):,.0f}\")\n",
    "    \n",
    "    # 3. H2O GLM (Generalized Linear Model)\n",
    "    print(\"\\nüî∏ Entrenando H2O GLM (Regularizado)...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    h2o_glm = H2OGeneralizedLinearEstimator(\n",
    "        family=\"gaussian\",\n",
    "        alpha=0.5,  # Elastic Net\n",
    "        lambda_search=True,\n",
    "        nfolds=5,\n",
    "        fold_assignment=\"Modulo\",\n",
    "        keep_cross_validation_predictions=True,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    h2o_glm.train(\n",
    "        x=h2o_predictors,\n",
    "        y=TARGET,\n",
    "        training_frame=h2o_train,\n",
    "        validation_frame=h2o_test\n",
    "    )\n",
    "    \n",
    "    glm_time = (datetime.now() - start_time).total_seconds()\n",
    "    h2o_models['H2O_GLM'] = h2o_glm\n",
    "    \n",
    "    # M√©tricas H2O GLM\n",
    "    train_rmse_glm = h2o_glm.rmse(train=True)\n",
    "    valid_rmse_glm = h2o_glm.rmse(valid=True)\n",
    "    train_r2_glm = h2o_glm.r2(train=True)\n",
    "    valid_r2_glm = h2o_glm.r2(valid=True)\n",
    "    \n",
    "    h2o_results['H2O_GLM'] = {\n",
    "        'train_rmse': train_rmse_glm,\n",
    "        'test_rmse': valid_rmse_glm,\n",
    "        'train_r2': train_r2_glm,\n",
    "        'test_r2': valid_r2_glm,\n",
    "        'training_time': glm_time,\n",
    "        'cv_rmse': h2o_glm.rmse(xval=True),\n",
    "        'cv_r2': h2o_glm.r2(xval=True),\n",
    "        'best_lambda': h2o_glm.lambda_best()\n",
    "    }\n",
    "    \n",
    "    print(f\"   Tiempo entrenamiento: {glm_time:.2f}s\")\n",
    "    print(f\"   Best lambda: {h2o_glm.lambda_best():.6f}\")\n",
    "    print(f\"   Train RMSE: {train_rmse_glm:,.0f}\")\n",
    "    print(f\"   Test RMSE:  {valid_rmse_glm::.0f}\")\n",
    "    print(f\"   Test R¬≤:    {valid_r2_glm:.4f}\")\n",
    "    print(f\"   CV RMSE:    {h2o_glm.rmse(xval=True):,.0f}\")\n",
    "    \n",
    "    # 4. H2O Deep Learning\n",
    "    print(\"\\nüî∏ Entrenando H2O Deep Learning (Neural Network)...\")\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    h2o_dl = H2ODeepLearningEstimator(\n",
    "        hidden=[200, 200, 100],  # 3 capas ocultas\n",
    "        epochs=50,\n",
    "        activation=\"Rectifier\",\n",
    "        l1=0.001,\n",
    "        l2=0.001,\n",
    "        input_dropout_ratio=0.1,\n",
    "        hidden_dropout_ratios=[0.2, 0.2, 0.1],\n",
    "        stopping_rounds=10,\n",
    "        stopping_tolerance=0.001,\n",
    "        stopping_metric=\"RMSE\",\n",
    "        nfolds=5,\n",
    "        fold_assignment=\"Modulo\",\n",
    "        keep_cross_validation_predictions=True,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    h2o_dl.train(\n",
    "        x=h2o_predictors,\n",
    "        y=TARGET,\n",
    "        training_frame=h2o_train,\n",
    "        validation_frame=h2o_test\n",
    "    )\n",
    "    \n",
    "    dl_time = (datetime.now() - start_time).total_seconds()\n",
    "    h2o_models['H2O_DL'] = h2o_dl\n",
    "    \n",
    "    # M√©tricas H2O Deep Learning\n",
    "    train_rmse_dl = h2o_dl.rmse(train=True)\n",
    "    valid_rmse_dl = h2o_dl.rmse(valid=True)\n",
    "    train_r2_dl = h2o_dl.r2(train=True)\n",
    "    valid_r2_dl = h2o_dl.r2(valid=True)\n",
    "    \n",
    "    h2o_results['H2O_DL'] = {\n",
    "        'train_rmse': train_rmse_dl,\n",
    "        'test_rmse': valid_rmse_dl,\n",
    "        'train_r2': train_r2_dl,\n",
    "        'test_r2': valid_r2_dl,\n",
    "        'training_time': dl_time,\n",
    "        'cv_rmse': h2o_dl.rmse(xval=True),\n",
    "        'cv_r2': h2o_dl.r2(xval=True)\n",
    "    }\n",
    "    \n",
    "    print(f\"   Tiempo entrenamiento: {dl_time:.2f}s\")\n",
    "    print(f\"   Train RMSE: {train_rmse_dl:,.0f}\")\n",
    "    print(f\"   Test RMSE:  {valid_rmse_dl:,.0f}\")\n",
    "    print(f\"   Test R¬≤:    {valid_r2_dl:.4f}\")\n",
    "    print(f\"   CV RMSE:    {h2o_dl.rmse(xval=True):,.0f}\")\n",
    "    \n",
    "    # Resumen de modelos H2O\n",
    "    print(f\"\\nüìä RESUMEN MODELOS H2O:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Modelo':<12} {'Test RMSE':<12} {'Test R¬≤':<10} {'CV RMSE':<12} {'Tiempo':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for model_name, results in h2o_results.items():\n",
    "        print(f\"{model_name:<12} {results['test_rmse']:<12,.0f} {results['test_r2']:<10.4f} \"\n",
    "              f\"{results['cv_rmse']:<12,.0f} {results['training_time']:<10.1f}s\")\n",
    "    \n",
    "    total_h2o_time = sum([r['training_time'] for r in h2o_results.values()])\n",
    "    print(f\"\\n‚è±Ô∏è Tiempo total H2O: {total_h2o_time:.1f}s\")\n",
    "    print(f\"‚úÖ Modelos H2O entrenados exitosamente\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è H2O no disponible, saltando modelos H2O\")\n",
    "    h2o_models = {}\n",
    "    h2o_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 2.2 H2O AUTOML - PIPELINE AUTOMATIZADO\n",
    "# ==================================================\n",
    "\n",
    "if H2O_AVAILABLE:\n",
    "    print(\"ü§ñ Ejecutando H2O AutoML (Pipeline Automatizado)...\")\n",
    "    \n",
    "    from h2o.automl import H2OAutoML\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Configurar AutoML\n",
    "    aml = H2OAutoML(\n",
    "        max_models=20,  # M√°ximo 20 modelos\n",
    "        max_runtime_secs=300,  # M√°ximo 5 minutos\n",
    "        stopping_metric=\"RMSE\",\n",
    "        stopping_tolerance=0.001,\n",
    "        stopping_rounds=3,\n",
    "        seed=42,\n",
    "        nfolds=5,\n",
    "        keep_cross_validation_predictions=True,\n",
    "        keep_cross_validation_models=True\n",
    "    )\n",
    "    \n",
    "    # Entrenar AutoML\n",
    "    aml.train(\n",
    "        x=h2o_predictors,\n",
    "        y=TARGET,\n",
    "        training_frame=h2o_train,\n",
    "        validation_frame=h2o_test\n",
    "    )\n",
    "    \n",
    "    automl_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"   Tiempo AutoML: {automl_time:.1f}s\")\n",
    "    print(f\"   Modelos entrenados: {len(aml.leaderboard)}\")\n",
    "    \n",
    "    # Obtener el mejor modelo\n",
    "    best_model = aml.leader\n",
    "    print(f\"   Mejor modelo: {best_model.model_id}\")\n",
    "    \n",
    "    # M√©tricas del mejor modelo\n",
    "    best_train_rmse = best_model.rmse(train=True)\n",
    "    best_valid_rmse = best_model.rmse(valid=True)\n",
    "    best_train_r2 = best_model.r2(train=True)\n",
    "    best_valid_r2 = best_model.r2(valid=True)\n",
    "    \n",
    "    h2o_models['H2O_AutoML_Best'] = best_model\n",
    "    h2o_results['H2O_AutoML_Best'] = {\n",
    "        'train_rmse': best_train_rmse,\n",
    "        'test_rmse': best_valid_rmse,\n",
    "        'train_r2': best_train_r2,\n",
    "        'test_r2': best_valid_r2,\n",
    "        'training_time': automl_time,\n",
    "        'cv_rmse': best_model.rmse(xval=True),\n",
    "        'cv_r2': best_model.r2(xval=True),\n",
    "        'model_type': type(best_model).__name__\n",
    "    }\n",
    "    \n",
    "    print(f\"   Best Train RMSE: {best_train_rmse:,.0f}\")\n",
    "    print(f\"   Best Test RMSE:  {best_valid_rmse:,.0f}\")\n",
    "    print(f\"   Best Test R¬≤:    {best_valid_r2:.4f}\")\n",
    "    print(f\"   Best CV RMSE:    {best_model.rmse(xval=True):,.0f}\")\n",
    "    \n",
    "    # Mostrar leaderboard\n",
    "    print(f\"\\nüìã LEADERBOARD H2O AutoML (Top 10):\")\n",
    "    print(\"-\" * 80)\n",
    "    leaderboard = aml.leaderboard.as_data_frame()\n",
    "    print(leaderboard.head(10).to_string(index=False))\n",
    "    \n",
    "    # Guardar AutoML para an√°lisis posterior\n",
    "    h2o_models['H2O_AutoML'] = aml\n",
    "    \n",
    "    print(f\"\\n‚úÖ H2O AutoML completado exitosamente\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è H2O no disponible, saltando AutoML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6009e153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 2.1 MODELOS ESTAD√çSTICOS OPTIMIZADOS (PARALELO)\n",
    "# ==================================================\n",
    "\n",
    "print(\"üìä Implementando modelos estad√≠sticos con paralelizaci√≥n...\")\n",
    "\n",
    "# Preparar datos - escalado para modelos regularizados\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Diccionario para almacenar modelos y resultados\n",
    "models = {}\n",
    "model_results = {}\n",
    "statistical_models = {}\n",
    "\n",
    "# 1. Regresi√≥n Lineal (OLS) - Baseline\n",
    "print(\"\\nüî∏ Entrenando Regresi√≥n Lineal (OLS)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "lr = LinearRegression(n_jobs=-1)  # Usar todos los cores\n",
    "lr.fit(X_train, y_train)\n",
    "models['Linear_Regression'] = lr\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train_lr = lr.predict(X_train)\n",
    "y_pred_test_lr = lr.predict(X_test)\n",
    "\n",
    "# M√©tricas\n",
    "lr_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_lr)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_lr)),\n",
    "    'train_mae': mean_absolute_error(y_train, y_pred_train_lr),\n",
    "    'test_mae': mean_absolute_error(y_test, y_pred_test_lr),\n",
    "    'train_r2': r2_score(y_train, y_pred_train_lr),\n",
    "    'test_r2': r2_score(y_test, y_pred_test_lr),\n",
    "    'training_time': (datetime.now() - start_time).total_seconds()\n",
    "}\n",
    "model_results['Linear_Regression'] = lr_metrics\n",
    "statistical_models['Linear_Regression'] = {'model': lr, **lr_metrics}\n",
    "\n",
    "print(f\"   Tiempo entrenamiento: {lr_metrics['training_time']:.2f}s\")\n",
    "print(f\"   Train RMSE: {lr_metrics['train_rmse']:,.0f}\")\n",
    "print(f\"   Test RMSE:  {lr_metrics['test_rmse']:,.0f}\")\n",
    "print(f\"   Test R¬≤:    {lr_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# 2. Ridge Regression (Paralelizado)\n",
    "print(\"\\nüî∏ Entrenando Ridge Regression (Paralelo)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# B√∫squeda paralela de hiperpar√°metros\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ridge_params = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]}\n",
    "ridge_grid = GridSearchCV(\n",
    "    Ridge(random_state=42), \n",
    "    ridge_params, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,  # Usar todos los cores\n",
    "    verbose=0\n",
    ")\n",
    "ridge_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "ridge = ridge_grid.best_estimator_\n",
    "models['Ridge'] = ridge\n",
    "\n",
    "y_pred_train_ridge = ridge.predict(X_train_scaled)\n",
    "y_pred_test_ridge = ridge.predict(X_test_scaled)\n",
    "\n",
    "ridge_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_ridge)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_ridge)),\n",
    "    'train_mae': mean_absolute_error(y_train, y_pred_train_ridge),\n",
    "    'test_mae': mean_absolute_error(y_test, y_pred_test_ridge),\n",
    "    'train_r2': r2_score(y_train, y_pred_train_ridge),\n",
    "    'test_r2': r2_score(y_test, y_pred_test_ridge),\n",
    "    'best_alpha': ridge_grid.best_params_['alpha'],\n",
    "    'training_time': (datetime.now() - start_time).total_seconds()\n",
    "}\n",
    "model_results['Ridge'] = ridge_metrics\n",
    "statistical_models['Ridge'] = {'model': ridge, **ridge_metrics}\n",
    "\n",
    "print(f\"   Tiempo entrenamiento: {ridge_metrics['training_time']:.2f}s\")\n",
    "print(f\"   Alpha √≥ptimo: {ridge_metrics['best_alpha']}\")\n",
    "print(f\"   Train RMSE: {ridge_metrics['train_rmse']:,.0f}\")\n",
    "print(f\"   Test RMSE:  {ridge_metrics['test_rmse']:,.0f}\")\n",
    "print(f\"   Test R¬≤:    {ridge_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# 3. Lasso Regression (Paralelizado)\n",
    "print(\"\\nüî∏ Entrenando Lasso Regression (Paralelo)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "lasso_params = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "lasso_grid = GridSearchCV(\n",
    "    Lasso(random_state=42, max_iter=3000), \n",
    "    lasso_params, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,  # Usar todos los cores\n",
    "    verbose=0\n",
    ")\n",
    "lasso_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "lasso = lasso_grid.best_estimator_\n",
    "models['Lasso'] = lasso\n",
    "\n",
    "y_pred_train_lasso = lasso.predict(X_train_scaled)\n",
    "y_pred_test_lasso = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Contar features seleccionadas por Lasso\n",
    "n_selected_features = np.sum(lasso.coef_ != 0)\n",
    "\n",
    "lasso_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_lasso)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_lasso)),\n",
    "    'train_mae': mean_absolute_error(y_train, y_pred_train_lasso),\n",
    "    'test_mae': mean_absolute_error(y_test, y_pred_test_lasso),\n",
    "    'train_r2': r2_score(y_train, y_pred_train_lasso),\n",
    "    'test_r2': r2_score(y_test, y_pred_test_lasso),\n",
    "    'best_alpha': lasso_grid.best_params_['alpha'],\n",
    "    'n_features': n_selected_features,\n",
    "    'training_time': (datetime.now() - start_time).total_seconds()\n",
    "}\n",
    "model_results['Lasso'] = lasso_metrics\n",
    "statistical_models['Lasso'] = {'model': lasso, **lasso_metrics}\n",
    "\n",
    "print(f\"   Tiempo entrenamiento: {lasso_metrics['training_time']:.2f}s\")\n",
    "print(f\"   Alpha √≥ptimo: {lasso_metrics['best_alpha']}\")\n",
    "print(f\"   Features seleccionadas: {n_selected_features}/{len(available_features)}\")\n",
    "print(f\"   Train RMSE: {lasso_metrics['train_rmse']:,.0f}\")\n",
    "print(f\"   Test RMSE:  {lasso_metrics['test_rmse']:,.0f}\")\n",
    "print(f\"   Test R¬≤:    {lasso_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# 4. ElasticNet (Paralelizado)\n",
    "print(\"\\nüî∏ Entrenando ElasticNet (Paralelo)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "elastic_params = {\n",
    "    'alpha': [0.01, 0.1, 1.0, 10.0],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "elastic_grid = GridSearchCV(\n",
    "    ElasticNet(random_state=42, max_iter=3000), \n",
    "    elastic_params, \n",
    "    cv=5, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,  # Usar todos los cores\n",
    "    verbose=0\n",
    ")\n",
    "elastic_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "elastic = elastic_grid.best_estimator_\n",
    "models['ElasticNet'] = elastic\n",
    "\n",
    "y_pred_train_elastic = elastic.predict(X_train_scaled)\n",
    "y_pred_test_elastic = elastic.predict(X_test_scaled)\n",
    "\n",
    "n_selected_features_elastic = np.sum(elastic.coef_ != 0)\n",
    "\n",
    "elastic_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_elastic)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_elastic)),\n",
    "    'train_mae': mean_absolute_error(y_train, y_pred_train_elastic),\n",
    "    'test_mae': mean_absolute_error(y_test, y_pred_test_elastic),\n",
    "    'train_r2': r2_score(y_train, y_pred_train_elastic),\n",
    "    'test_r2': r2_score(y_test, y_pred_test_elastic),\n",
    "    'best_alpha': elastic_grid.best_params_['alpha'],\n",
    "    'best_l1_ratio': elastic_grid.best_params_['l1_ratio'],\n",
    "    'n_features': n_selected_features_elastic,\n",
    "    'training_time': (datetime.now() - start_time).total_seconds()\n",
    "}\n",
    "model_results['ElasticNet'] = elastic_metrics\n",
    "statistical_models['ElasticNet'] = {'model': elastic, **elastic_metrics}\n",
    "\n",
    "print(f\"   Tiempo entrenamiento: {elastic_metrics['training_time']:.2f}s\")\n",
    "print(f\"   Alpha √≥ptimo: {elastic_metrics['best_alpha']}\")\n",
    "print(f\"   L1 ratio √≥ptimo: {elastic_metrics['best_l1_ratio']}\")\n",
    "print(f\"   Features seleccionadas: {n_selected_features_elastic}/{len(available_features)}\")\n",
    "print(f\"   Train RMSE: {elastic_metrics['train_rmse']:,.0f}\")\n",
    "print(f\"   Test RMSE:  {elastic_metrics['test_rmse']:,.0f}\")\n",
    "print(f\"   Test R¬≤:    {elastic_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# Guardar modelos estad√≠sticos\n",
    "model_results['statistical_models'] = statistical_models\n",
    "\n",
    "print(f\"\\n‚úÖ Modelos estad√≠sticos entrenados correctamente con paralelizaci√≥n\")\n",
    "print(f\"‚è±Ô∏è Tiempo total modelos estad√≠sticos: {sum([m['training_time'] for m in model_results.values() if 'training_time' in m]):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 2.2 COMPARACI√ìN CON AIC/BIC (SIN CV)\n",
    "# ==================================================\n",
    "\n",
    "print(\"üîç Calculando AIC/BIC para comparaci√≥n de modelos...\")\n",
    "\n",
    "def calculate_aic_bic(y_true, y_pred, n_params, n_obs):\n",
    "    \"\"\"Calcula AIC y BIC para un modelo\"\"\"\n",
    "    # Suma de cuadrados de residuos\n",
    "    ssr = np.sum((y_true - y_pred) ** 2)\n",
    "    \n",
    "    # Log-likelihood (asumiendo errores normales)\n",
    "    sigma2 = ssr / n_obs\n",
    "    log_likelihood = -0.5 * n_obs * (np.log(2 * np.pi) + np.log(sigma2) + 1)\n",
    "    \n",
    "    # AIC y BIC\n",
    "    aic = 2 * n_params - 2 * log_likelihood\n",
    "    bic = np.log(n_obs) * n_params - 2 * log_likelihood\n",
    "    \n",
    "    return aic, bic\n",
    "\n",
    "# Calcular AIC/BIC para cada modelo estad√≠stico\n",
    "n_obs = len(y_train)\n",
    "model_comparison = []\n",
    "\n",
    "# Linear Regression\n",
    "n_params_lr = len(available_features) + 1  # features + intercept\n",
    "aic_lr, bic_lr = calculate_aic_bic(y_train, y_pred_train_lr, n_params_lr, n_obs)\n",
    "\n",
    "model_comparison.append({\n",
    "    'Model': 'Linear Regression',\n",
    "    'AIC': aic_lr,\n",
    "    'BIC': bic_lr,\n",
    "    'N_Params': n_params_lr,\n",
    "    'Test_R2': lr_metrics['test_r2'],\n",
    "    'Test_RMSE': lr_metrics['test_rmse']\n",
    "})\n",
    "\n",
    "# Ridge (todos los par√°metros excepto regularizaci√≥n)\n",
    "n_params_ridge = len(available_features) + 1\n",
    "aic_ridge, bic_ridge = calculate_aic_bic(y_train, y_pred_train_ridge, n_params_ridge, n_obs)\n",
    "\n",
    "model_comparison.append({\n",
    "    'Model': 'Ridge',\n",
    "    'AIC': aic_ridge,\n",
    "    'BIC': bic_ridge,\n",
    "    'N_Params': n_params_ridge,\n",
    "    'Test_R2': ridge_metrics['test_r2'],\n",
    "    'Test_RMSE': ridge_metrics['test_rmse']\n",
    "})\n",
    "\n",
    "# Lasso (solo par√°metros no-cero)\n",
    "n_params_lasso = n_selected_features + 1\n",
    "aic_lasso, bic_lasso = calculate_aic_bic(y_train, y_pred_train_lasso, n_params_lasso, n_obs)\n",
    "\n",
    "model_comparison.append({\n",
    "    'Model': 'Lasso',\n",
    "    'AIC': aic_lasso,\n",
    "    'BIC': bic_lasso,\n",
    "    'N_Params': n_params_lasso,\n",
    "    'Test_R2': lasso_metrics['test_r2'],\n",
    "    'Test_RMSE': lasso_metrics['test_rmse']\n",
    "})\n",
    "\n",
    "# ElasticNet (solo par√°metros no-cero)\n",
    "n_params_elastic = n_selected_features_elastic + 1\n",
    "aic_elastic, bic_elastic = calculate_aic_bic(y_train, y_pred_train_elastic, n_params_elastic, n_obs)\n",
    "\n",
    "model_comparison.append({\n",
    "    'Model': 'ElasticNet',\n",
    "    'AIC': aic_elastic,\n",
    "    'BIC': bic_elastic,\n",
    "    'N_Params': n_params_elastic,\n",
    "    'Test_R2': elastic_metrics['test_r2'],\n",
    "    'Test_RMSE': elastic_metrics['test_rmse']\n",
    "})\n",
    "\n",
    "# Crear DataFrame para comparaci√≥n\n",
    "comparison_df = pd.DataFrame(model_comparison)\n",
    "comparison_df = comparison_df.sort_values('AIC').reset_index(drop=True)\n",
    "\n",
    "print(\"üìä Comparaci√≥n de modelos estad√≠sticos (ordenado por AIC):\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Modelo':<15} {'AIC':<12} {'BIC':<12} {'N_Params':<10} {'Test_R¬≤':<10} {'Test_RMSE':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    print(f\"{row['Model']:<15} {row['AIC']:<12,.0f} {row['BIC']:<12,.0f} {row['N_Params']:<10} {row['Test_R2']:<10.4f} {row['Test_RMSE']:<12,.0f}\")\n",
    "\n",
    "# Identificar mejor modelo por cada criterio\n",
    "best_aic = comparison_df.loc[comparison_df['AIC'].idxmin(), 'Model']\n",
    "best_bic = comparison_df.loc[comparison_df['BIC'].idxmin(), 'Model']\n",
    "best_r2 = comparison_df.loc[comparison_df['Test_R2'].idxmax(), 'Model']\n",
    "best_rmse = comparison_df.loc[comparison_df['Test_RMSE'].idxmin(), 'Model']\n",
    "\n",
    "print(f\"\\nüèÜ Mejores modelos por criterio:\")\n",
    "print(f\"   Mejor AIC:    {best_aic}\")\n",
    "print(f\"   Mejor BIC:    {best_bic}\")\n",
    "print(f\"   Mejor R¬≤:     {best_r2}\")\n",
    "print(f\"   Mejor RMSE:   {best_rmse}\")\n",
    "\n",
    "# Visualizaci√≥n de comparaci√≥n\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# AIC vs BIC\n",
    "axes[0].scatter(comparison_df['AIC'], comparison_df['BIC'], s=100, alpha=0.7)\n",
    "for i, row in comparison_df.iterrows():\n",
    "    axes[0].annotate(row['Model'], (row['AIC'], row['BIC']), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "axes[0].set_xlabel('AIC')\n",
    "axes[0].set_ylabel('BIC')\n",
    "axes[0].set_title('AIC vs BIC')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# N√∫mero de par√°metros vs Performance\n",
    "axes[1].scatter(comparison_df['N_Params'], comparison_df['Test_R2'], s=100, alpha=0.7, color='green')\n",
    "for i, row in comparison_df.iterrows():\n",
    "    axes[1].annotate(row['Model'], (row['N_Params'], row['Test_R2']), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "axes[1].set_xlabel('N√∫mero de Par√°metros')\n",
    "axes[1].set_ylabel('Test R¬≤')\n",
    "axes[1].set_title('Complejidad vs Performance')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Ranking de modelos\n",
    "ranking_criteria = ['AIC', 'BIC', 'Test_R2', 'Test_RMSE']\n",
    "model_names = comparison_df['Model'].tolist()\n",
    "rankings = []\n",
    "\n",
    "for criterion in ranking_criteria:\n",
    "    if criterion in ['AIC', 'BIC', 'Test_RMSE']:\n",
    "        # Menor es mejor\n",
    "        sorted_df = comparison_df.sort_values(criterion)\n",
    "    else:\n",
    "        # Mayor es mejor\n",
    "        sorted_df = comparison_df.sort_values(criterion, ascending=False)\n",
    "    \n",
    "    ranking = {model: idx+1 for idx, model in enumerate(sorted_df['Model'])}\n",
    "    rankings.append([ranking[model] for model in model_names])\n",
    "\n",
    "# Heatmap de rankings\n",
    "ranking_matrix = np.array(rankings).T\n",
    "im = axes[2].imshow(ranking_matrix, cmap='RdYlGn_r', aspect='auto')\n",
    "axes[2].set_xticks(range(len(ranking_criteria)))\n",
    "axes[2].set_xticklabels(ranking_criteria)\n",
    "axes[2].set_yticks(range(len(model_names)))\n",
    "axes[2].set_yticklabels(model_names)\n",
    "axes[2].set_title('Ranking de Modelos (1=mejor)')\n",
    "\n",
    "# A√±adir valores al heatmap\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(len(ranking_criteria)):\n",
    "        axes[2].text(j, i, int(ranking_matrix[i, j]), ha='center', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Guardar resultados\n",
    "model_results['comparison_df'] = comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e10e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 2.3 DIAGN√ìSTICO DE RESIDUOS Y DETECCI√ìN DE OUTLIERS\n",
    "# ==================================================\n",
    "\n",
    "print(\"üîç Realizando diagn√≥stico de residuos para el mejor modelo...\")\n",
    "\n",
    "# Usar el mejor modelo seg√∫n AIC (Linear Regression en este caso)\n",
    "best_model_name = best_aic\n",
    "print(f\"Analizando residuos para: {best_model_name}\")\n",
    "\n",
    "# Obtener predicciones del mejor modelo\n",
    "if best_model_name == 'Linear_Regression':\n",
    "    y_pred_train_best = y_pred_train_lr\n",
    "    y_pred_test_best = y_pred_test_lr\n",
    "elif best_model_name == 'Ridge':\n",
    "    y_pred_train_best = y_pred_train_ridge\n",
    "    y_pred_test_best = y_pred_test_ridge\n",
    "elif best_model_name == 'Lasso':\n",
    "    y_pred_train_best = y_pred_train_lasso\n",
    "    y_pred_test_best = y_pred_test_lasso\n",
    "else:  # ElasticNet\n",
    "    y_pred_train_best = y_pred_train_elastic\n",
    "    y_pred_test_best = y_pred_test_elastic\n",
    "\n",
    "# Calcular residuos\n",
    "residuals_train = y_train - y_pred_train_best\n",
    "residuals_test = y_test - y_pred_test_best\n",
    "\n",
    "# Residuos estandarizados\n",
    "residuals_train_std = residuals_train / np.std(residuals_train)\n",
    "residuals_test_std = residuals_test / np.std(residuals_test)\n",
    "\n",
    "def diagnostic_plots(y_true, y_pred, residuals, residuals_std, title_prefix):\n",
    "    \"\"\"Crea gr√°ficos de diagn√≥stico de residuos\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Residuos vs Fitted Values\n",
    "    axes[0,0].scatter(y_pred, residuals, alpha=0.5)\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[0,0].set_xlabel('Valores Predichos')\n",
    "    axes[0,0].set_ylabel('Residuos')\n",
    "    axes[0,0].set_title(f'{title_prefix}: Residuos vs Predicciones')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Q-Q Plot de residuos\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[0,1])\n",
    "    axes[0,1].set_title(f'{title_prefix}: Q-Q Plot Residuos')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Histograma de residuos\n",
    "    axes[0,2].hist(residuals, bins=50, density=True, alpha=0.7, color='skyblue')\n",
    "    axes[0,2].set_xlabel('Residuos')\n",
    "    axes[0,2].set_ylabel('Densidad')\n",
    "    axes[0,2].set_title(f'{title_prefix}: Distribuci√≥n de Residuos')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Scale-Location Plot (Residuos estandarizados vs Fitted)\n",
    "    axes[1,0].scatter(y_pred, np.abs(residuals_std), alpha=0.5)\n",
    "    axes[1,0].set_xlabel('Valores Predichos')\n",
    "    axes[1,0].set_ylabel('|Residuos Estandarizados|')\n",
    "    axes[1,0].set_title(f'{title_prefix}: Scale-Location Plot')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Residuos vs Orden (para detectar autocorrelaci√≥n)\n",
    "    axes[1,1].plot(range(len(residuals)), residuals, alpha=0.7)\n",
    "    axes[1,1].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[1,1].set_xlabel('Orden de Observaci√≥n')\n",
    "    axes[1,1].set_ylabel('Residuos')\n",
    "    axes[1,1].set_title(f'{title_prefix}: Residuos vs Orden')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Predicted vs Actual\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    axes[1,2].scatter(y_true, y_pred, alpha=0.5)\n",
    "    axes[1,2].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    axes[1,2].set_xlabel('Valores Reales')\n",
    "    axes[1,2].set_ylabel('Valores Predichos')\n",
    "    axes[1,2].set_title(f'{title_prefix}: Predicho vs Real')\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Diagn√≥sticos para conjunto de entrenamiento\n",
    "print(\"\\nüìä Diagn√≥sticos para conjunto de ENTRENAMIENTO:\")\n",
    "diagnostic_plots(y_train, y_pred_train_best, residuals_train, residuals_train_std, \"Train\")\n",
    "\n",
    "# Diagn√≥sticos para conjunto de prueba\n",
    "print(\"\\nüìä Diagn√≥sticos para conjunto de PRUEBA:\")\n",
    "diagnostic_plots(y_test, y_pred_test_best, residuals_test, residuals_test_std, \"Test\")\n",
    "\n",
    "# Tests estad√≠sticos de los residuos\n",
    "print(\"\\nüß™ Tests estad√≠sticos de residuos:\")\n",
    "\n",
    "# Test de normalidad (Shapiro-Wilk en muestra peque√±a)\n",
    "from scipy.stats import shapiro, jarque_bera\n",
    "\n",
    "# Tomar muestra para test de normalidad (Shapiro-Wilk max 5000 observaciones)\n",
    "sample_size = min(5000, len(residuals_train))\n",
    "residuals_sample = np.random.choice(residuals_train, sample_size, replace=False)\n",
    "\n",
    "shapiro_stat, shapiro_p = shapiro(residuals_sample)\n",
    "jb_stat, jb_p = jarque_bera(residuals_train)\n",
    "\n",
    "print(f\"Test de normalidad:\")\n",
    "print(f\"  Shapiro-Wilk (muestra {sample_size}): stat={shapiro_stat:.4f}, p-value={shapiro_p:.4f}\")\n",
    "print(f\"  Jarque-Bera: stat={jb_stat:.4f}, p-value={jb_p:.4f}\")\n",
    "\n",
    "# Test de heterocedasticidad (Breusch-Pagan)\n",
    "if best_model_name == 'Linear_Regression':\n",
    "    # Usar statsmodels para test m√°s riguroso\n",
    "    X_train_sm = sm.add_constant(X_train)\n",
    "    model_sm = sm.OLS(y_train, X_train_sm).fit()\n",
    "    \n",
    "    # Test de Breusch-Pagan\n",
    "    from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "    bp_stat, bp_p, _, _ = het_breuschpagan(model_sm.resid, X_train_sm)\n",
    "    \n",
    "    print(f\"\\nTest de heterocedasticidad:\")\n",
    "    print(f\"  Breusch-Pagan: stat={bp_stat:.4f}, p-value={bp_p:.4f}\")\n",
    "    \n",
    "    if bp_p < 0.05:\n",
    "        print(\"  ‚ö†Ô∏è Evidencia de heterocedasticidad\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ No hay evidencia de heterocedasticidad\")\n",
    "\n",
    "# Detecci√≥n de outliers\n",
    "print(f\"\\nüéØ Detecci√≥n de outliers:\")\n",
    "\n",
    "# Outliers basados en residuos estandarizados (|z| > 3)\n",
    "outliers_train = np.where(np.abs(residuals_train_std) > 3)[0]\n",
    "outliers_test = np.where(np.abs(residuals_test_std) > 3)[0]\n",
    "\n",
    "print(f\"Outliers por residuos estandarizados (|z| > 3):\")\n",
    "print(f\"  Train: {len(outliers_train)} outliers ({len(outliers_train)/len(y_train)*100:.2f}%)\")\n",
    "print(f\"  Test:  {len(outliers_test)} outliers ({len(outliers_test)/len(y_test)*100:.2f}%)\")\n",
    "\n",
    "# Estad√≠sticas de los outliers\n",
    "if len(outliers_train) > 0:\n",
    "    outlier_residuals = residuals_train_std[outliers_train]\n",
    "    print(f\"\\nEstad√≠sticas de outliers (train):\")\n",
    "    print(f\"  Residuo estandarizado m√°ximo: {np.max(np.abs(outlier_residuals)):.2f}\")\n",
    "    print(f\"  Residuo estandarizado promedio: {np.mean(np.abs(outlier_residuals)):.2f}\")\n",
    "\n",
    "# An√°lisis de leverage y influencia (solo para Linear Regression)\n",
    "if best_model_name == 'Linear_Regression':\n",
    "    print(f\"\\nüìà An√°lisis de leverage e influencia:\")\n",
    "    \n",
    "    # Calcular leverage, residuos estudentizados y distancia de Cook\n",
    "    influence = OLSInfluence(model_sm)\n",
    "    leverage = influence.hat_matrix_diag\n",
    "    studentized_residuals = influence.resid_studentized_external\n",
    "    cooks_distance = influence.cooks_distance[0]\n",
    "    \n",
    "    # Umbrales\n",
    "    leverage_threshold = 2 * (len(available_features) + 1) / len(y_train)\n",
    "    cooks_threshold = 4 / len(y_train)\n",
    "    \n",
    "    # Identificar observaciones influyentes\n",
    "    high_leverage = np.where(leverage > leverage_threshold)[0]\n",
    "    high_cooks = np.where(cooks_distance > cooks_threshold)[0]\n",
    "    high_studentized = np.where(np.abs(studentized_residuals) > 3)[0]\n",
    "    \n",
    "    print(f\"  High leverage (>{leverage_threshold:.4f}): {len(high_leverage)} obs ({len(high_leverage)/len(y_train)*100:.2f}%)\")\n",
    "    print(f\"  High Cook's distance (>{cooks_threshold:.4f}): {len(high_cooks)} obs ({len(high_cooks)/len(y_train)*100:.2f}%)\")\n",
    "    print(f\"  High studentized residuals (>3): {len(high_studentized)} obs ({len(high_studentized)/len(y_train)*100:.2f}%)\")\n",
    "    \n",
    "    # Observaciones problem√°ticas (combinan m√∫ltiples criterios)\n",
    "    problematic = np.intersect1d(high_leverage, high_cooks)\n",
    "    if len(problematic) > 0:\n",
    "        print(f\"  ‚ö†Ô∏è Observaciones problem√°ticas (alto leverage Y Cook's distance): {len(problematic)}\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ No hay observaciones extremadamente problem√°ticas\")\n",
    "\n",
    "print(f\"\\n‚úÖ Diagn√≥stico de residuos completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6614ba8",
   "metadata": {},
   "source": [
    "## 3. Modelos de √Årboles\n",
    "\n",
    "### Metodolog√≠a de Modelos Basados en √Årboles\n",
    "- **LightGBM**: Gradient boosting eficiente con par√°metros por defecto\n",
    "- **Random Forest**: Ensemble de √°rboles con configuraci√≥n b√°sica\n",
    "- **Comparaci√≥n**: Performance vs modelos estad√≠sticos\n",
    "- **Interpretabilidad**: Feature importance y explicabilidad inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7840a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 3.1 MODELOS DE √ÅRBOLES OPTIMIZADOS (GPU + PARALELO)\n",
    "# ==================================================\n",
    "\n",
    "print(\"üå≥ Entrenando modelos basados en √°rboles con GPU y paralelizaci√≥n...\")\n",
    "\n",
    "# 1. LightGBM con GPU (si disponible)\n",
    "print(\"\\nüî∏ Entrenando LightGBM (GPU Optimizado)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Par√°metros optimizados para GPU y paralelizaci√≥n\n",
    "if GPU_AVAILABLE:\n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'device': 'gpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0,\n",
    "        'num_leaves': 63,  # M√°s hojas para GPU\n",
    "        'learning_rate': 0.1,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'max_depth': 7,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'lambda_l1': 0.1,\n",
    "        'lambda_l2': 0.1,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        'force_col_wise': True,  # Optimizaci√≥n para GPU\n",
    "        'gpu_use_dp': False  # Usar single precision para velocidad\n",
    "    }\n",
    "    print(\"   üöÄ Usando GPU para entrenamiento\")\n",
    "else:\n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'device': 'cpu',\n",
    "        'num_threads': mp.cpu_count(),  # Usar todos los cores\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        'force_col_wise': True\n",
    "    }\n",
    "    print(f\"   ‚ö° Usando CPU con {mp.cpu_count()} threads\")\n",
    "\n",
    "# Crear datasets de LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Entrenar modelo con early stopping optimizado\n",
    "lgb_model = lgb.train(\n",
    "    lgb_params,\n",
    "    train_data,\n",
    "    num_boost_round=500,  # M√°s iteraciones para mejor rendimiento\n",
    "    valid_sets=[train_data, valid_data],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=20),\n",
    "        lgb.log_evaluation(50)  # Log cada 50 iteraciones\n",
    "    ]\n",
    ")\n",
    "\n",
    "training_time_lgb = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train_lgb = lgb_model.predict(X_train, num_iteration=lgb_model.best_iteration)\n",
    "y_pred_test_lgb = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "# M√©tricas LightGBM\n",
    "lgb_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_lgb)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_lgb)),\n",
    "    'train_mae': mean_absolute_error(y_train, y_pred_train_lgb),\n",
    "    'test_mae': mean_absolute_error(y_test, y_pred_test_lgb),\n",
    "    'train_r2': r2_score(y_train, y_pred_train_lgb),\n",
    "    'test_r2': r2_score(y_test, y_pred_test_lgb),\n",
    "    'best_iteration': lgb_model.best_iteration,\n",
    "    'num_features': lgb_model.num_feature(),\n",
    "    'training_time': training_time_lgb,\n",
    "    'device': 'GPU' if GPU_AVAILABLE else 'CPU'\n",
    "}\n",
    "\n",
    "models['LightGBM'] = lgb_model\n",
    "model_results['LightGBM'] = lgb_metrics\n",
    "\n",
    "print(f\"   Tiempo entrenamiento: {training_time_lgb:.2f}s\")\n",
    "print(f\"   Mejores iteraciones: {lgb_model.best_iteration}\")\n",
    "print(f\"   Device usado: {lgb_metrics['device']}\")\n",
    "print(f\"   Train RMSE: {lgb_metrics['train_rmse']:,.0f}\")\n",
    "print(f\"   Test RMSE:  {lgb_metrics['test_rmse']:,.0f}\")\n",
    "print(f\"   Test R¬≤:    {lgb_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# 2. Random Forest con paralelizaci√≥n optimizada\n",
    "print(\"\\nüî∏ Entrenando Random Forest (Paralelo Optimizado)...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Par√°metros optimizados para paralelizaci√≥n\n",
    "rf_params = {\n",
    "    'n_estimators': 200,  # M√°s √°rboles para mejor rendimiento\n",
    "    'max_depth': 15,  # M√°s profundidad\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features': 'sqrt',\n",
    "    'bootstrap': True,\n",
    "    'oob_score': True,  # Out-of-bag score para validaci√≥n\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,  # Usar todos los cores\n",
    "    'verbose': 1  # Mostrar progreso\n",
    "}\n",
    "\n",
    "rf_model = RandomForestRegressor(**rf_params)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "training_time_rf = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train_rf = rf_model.predict(X_train)\n",
    "y_pred_test_rf = rf_model.predict(X_test)\n",
    "\n",
    "# M√©tricas Random Forest\n",
    "rf_metrics = {\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train_rf)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test_rf)),\n",
    "    'train_mae': mean_absolute_error(y_train, y_pred_train_rf),\n",
    "    'test_mae': mean_absolute_error(y_test, y_pred_test_rf),\n",
    "    'train_r2': r2_score(y_train, y_pred_train_rf),\n",
    "    'test_r2': r2_score(y_test, y_pred_test_rf),\n",
    "    'oob_score': rf_model.oob_score_,\n",
    "    'n_estimators': rf_params['n_estimators'],\n",
    "    'max_depth': rf_params['max_depth'],\n",
    "    'training_time': training_time_rf,\n",
    "    'n_cores': mp.cpu_count()\n",
    "}\n",
    "\n",
    "models['Random_Forest'] = rf_model\n",
    "model_results['Random_Forest'] = rf_metrics\n",
    "\n",
    "print(f\"   Tiempo entrenamiento: {training_time_rf:.2f}s\")\n",
    "print(f\"   N√∫mero de √°rboles: {rf_params['n_estimators']}\")\n",
    "print(f\"   Cores utilizados: {mp.cpu_count()}\")\n",
    "print(f\"   OOB Score: {rf_metrics['oob_score']:.4f}\")\n",
    "print(f\"   Train RMSE: {rf_metrics['train_rmse']:,.0f}\")\n",
    "print(f\"   Test RMSE:  {rf_metrics['test_rmse']:,.0f}\")\n",
    "print(f\"   Test R¬≤:    {rf_metrics['test_r2']:.4f}\")\n",
    "\n",
    "# Comparaci√≥n de velocidad de entrenamiento\n",
    "print(f\"\\n‚ö° COMPARACI√ìN DE VELOCIDAD:\")\n",
    "print(f\"   LightGBM ({lgb_metrics['device']}): {training_time_lgb:.2f}s\")\n",
    "print(f\"   Random Forest ({mp.cpu_count()} cores): {training_time_rf:.2f}s\")\n",
    "print(f\"   Speedup LGB vs RF: {training_time_rf/training_time_lgb:.1f}x\")\n",
    "\n",
    "print(f\"\\n‚úÖ Modelos de √°rboles entrenados correctamente con optimizaciones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebc74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 3.2 FEATURE IMPORTANCE Y EXPLICABILIDAD INICIAL\n",
    "# ==================================================\n",
    "\n",
    "print(\"üîç Analizando feature importance de modelos de √°rboles...\")\n",
    "\n",
    "def plot_feature_importance(model, model_name, feature_names, top_n=15):\n",
    "    \"\"\"Grafica feature importance para modelos de √°rboles\"\"\"\n",
    "    \n",
    "    if model_name == 'LightGBM':\n",
    "        # Feature importance de LightGBM\n",
    "        importance = model.feature_importance(importance_type='gain')\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "    elif model_name == 'Random_Forest':\n",
    "        # Feature importance de Random Forest\n",
    "        importance = model.feature_importances_\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top features\n",
    "    top_features = feature_imp.head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(top_features)), top_features['importance'], color='lightcoral', alpha=0.8)\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top {top_n} Features - {model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_imp\n",
    "\n",
    "# Analizar feature importance para LightGBM\n",
    "print(f\"\\nüìä Feature Importance - LightGBM:\")\n",
    "lgb_feature_imp = plot_feature_importance(lgb_model, 'LightGBM', available_features)\n",
    "\n",
    "# Mostrar top 10 features de LightGBM\n",
    "print(f\"Top 10 Features m√°s importantes (LightGBM):\")\n",
    "for i, (_, row) in enumerate(lgb_feature_imp.head(10).iterrows(), 1):\n",
    "    print(f\"  {i:2d}. {row['feature']:<25}: {row['importance']:.2f}\")\n",
    "\n",
    "# Analizar feature importance para Random Forest\n",
    "print(f\"\\nüìä Feature Importance - Random Forest:\")\n",
    "rf_feature_imp = plot_feature_importance(rf_model, 'Random_Forest', available_features)\n",
    "\n",
    "# Mostrar top 10 features de Random Forest\n",
    "print(f\"Top 10 Features m√°s importantes (Random Forest):\")\n",
    "for i, (_, row) in enumerate(rf_feature_imp.head(10).iterrows(), 1):\n",
    "    print(f\"  {i:2d}. {row['feature']:<25}: {row['importance']:.4f}\")\n",
    "\n",
    "# Comparaci√≥n de feature importance entre modelos de √°rboles\n",
    "print(f\"\\nüîÑ Comparaci√≥n de Feature Importance entre modelos:\")\n",
    "\n",
    "# Merge de importancias\n",
    "comparison_imp = lgb_feature_imp[['feature', 'importance']].rename(columns={'importance': 'lgb_importance'})\n",
    "comparison_imp = comparison_imp.merge(\n",
    "    rf_feature_imp[['feature', 'importance']].rename(columns={'importance': 'rf_importance'}),\n",
    "    on='feature'\n",
    ")\n",
    "\n",
    "# Normalizar para comparaci√≥n\n",
    "comparison_imp['lgb_importance_norm'] = comparison_imp['lgb_importance'] / comparison_imp['lgb_importance'].max()\n",
    "comparison_imp['rf_importance_norm'] = comparison_imp['rf_importance'] / comparison_imp['rf_importance'].max()\n",
    "\n",
    "# Calcular correlaci√≥n entre importancias\n",
    "correlation = comparison_imp['lgb_importance_norm'].corr(comparison_imp['rf_importance_norm'])\n",
    "print(f\"Correlaci√≥n entre importancias LightGBM-RF: {correlation:.4f}\")\n",
    "\n",
    "# Plot comparativo de importancias\n",
    "top_features_comparison = comparison_imp.head(15)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Scatter plot de importancias\n",
    "axes[0].scatter(comparison_imp['lgb_importance_norm'], comparison_imp['rf_importance_norm'], alpha=0.6)\n",
    "axes[0].plot([0, 1], [0, 1], 'r--', alpha=0.8)\n",
    "axes[0].set_xlabel('LightGBM Importance (normalized)')\n",
    "axes[0].set_ylabel('Random Forest Importance (normalized)')\n",
    "axes[0].set_title(f'Comparaci√≥n Feature Importance\\n(Correlaci√≥n: {correlation:.3f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Top features side by side\n",
    "x_pos = np.arange(len(top_features_comparison))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].barh(x_pos - width/2, top_features_comparison['lgb_importance_norm'], width, \n",
    "            label='LightGBM', alpha=0.8, color='lightblue')\n",
    "axes[1].barh(x_pos + width/2, top_features_comparison['rf_importance_norm'], width,\n",
    "            label='Random Forest', alpha=0.8, color='lightcoral')\n",
    "\n",
    "axes[1].set_yticks(x_pos)\n",
    "axes[1].set_yticklabels(top_features_comparison['feature'])\n",
    "axes[1].set_xlabel('Normalized Importance')\n",
    "axes[1].set_title('Top 15 Features: LightGBM vs Random Forest')\n",
    "axes[1].legend()\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Features que aparecen en top 10 de ambos modelos\n",
    "lgb_top10 = set(lgb_feature_imp.head(10)['feature'])\n",
    "rf_top10 = set(rf_feature_imp.head(10)['feature'])\n",
    "common_top10 = lgb_top10.intersection(rf_top10)\n",
    "\n",
    "print(f\"\\nüéØ Features en Top 10 de ambos modelos ({len(common_top10)}):\")\n",
    "for feature in sorted(common_top10):\n",
    "    lgb_rank = lgb_feature_imp[lgb_feature_imp['feature'] == feature].index[0] + 1\n",
    "    rf_rank = rf_feature_imp[rf_feature_imp['feature'] == feature].index[0] + 1\n",
    "    print(f\"  ‚Ä¢ {feature} (LGB: #{lgb_rank}, RF: #{rf_rank})\")\n",
    "\n",
    "# Guardar feature importance\n",
    "model_results['lgb_feature_importance'] = lgb_feature_imp\n",
    "model_results['rf_feature_importance'] = rf_feature_imp\n",
    "model_results['feature_importance_correlation'] = correlation\n",
    "\n",
    "print(f\"\\n‚úÖ An√°lisis de feature importance completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c4bd5",
   "metadata": {},
   "source": [
    "## 4. Evaluaci√≥n Comparativa y Selecci√≥n de Modelos\n",
    "\n",
    "### 4.1 Comparaci√≥n Global de Modelos\n",
    "\n",
    "En esta secci√≥n realizaremos una evaluaci√≥n comparativa sistem√°tica de todos los modelos desarrollados, considerando m√∫ltiples criterios de evaluaci√≥n y el contexto espec√≠fico del problema de predicci√≥n de precios inmobiliarios.\n",
    "\n",
    "**Metodolog√≠a de Evaluaci√≥n:**\n",
    "- **M√©tricas de Rendimiento**: RMSE, MAE, R¬≤ para evaluaci√≥n cuantitativa\n",
    "- **Criterios Estad√≠sticos**: AIC/BIC para modelos lineales (penalizaci√≥n por complejidad)\n",
    "- **An√°lisis de Residuos**: Distribuci√≥n, heterocedasticidad, outliers\n",
    "- **Estabilidad Temporal**: Robustez en datos de test (simulando nuevas observaciones)\n",
    "- **Interpretabilidad**: Facilidad de explicaci√≥n para stakeholders inmobiliarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7962768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 5.1 EVALUACI√ìN COMPARATIVA H2O + SKLEARN + GPU\n",
    "# ==================================================\n",
    "\n",
    "print(\"üìä Evaluaci√≥n comparativa completa: H2O + Sklearn + GPU...\")\n",
    "\n",
    "# Funci√≥n para calcular m√©tricas completas en paralelo\n",
    "def evaluate_model_comprehensive(y_true, y_pred, model_name, n_params=None, training_time=None, cv_metrics=None):\n",
    "    \"\"\"Calcula m√©tricas completas para evaluaci√≥n de modelos\"\"\"\n",
    "    \n",
    "    # M√©tricas b√°sicas calculadas en paralelo donde sea posible\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # M√©tricas adicionales\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    residuals = y_true - y_pred\n",
    "    std_residuals = np.std(residuals)\n",
    "    \n",
    "    # Error relativo al rango\n",
    "    price_range = np.max(y_true) - np.min(y_true)\n",
    "    rmse_relative = (rmse / price_range) * 100\n",
    "    \n",
    "    # Crear diccionario de m√©tricas\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape,\n",
    "        'std_residuals': std_residuals,\n",
    "        'rmse_relative': rmse_relative,\n",
    "        'n_params': n_params,\n",
    "        'training_time': training_time,\n",
    "        'framework': 'H2O' if model_name.startswith('H2O') else 'Sklearn'\n",
    "    }\n",
    "    \n",
    "    # A√±adir m√©tricas de validaci√≥n cruzada si est√°n disponibles\n",
    "    if cv_metrics:\n",
    "        metrics.update(cv_metrics)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Evaluar todos los modelos usando paralelizaci√≥n donde sea posible\n",
    "print(\"üéØ Evaluando todos los modelos (H2O + Sklearn + GPU)...\")\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "# === MODELOS H2O ===\n",
    "if H2O_AVAILABLE and h2o_results:\n",
    "    print(\"üöÄ Evaluando modelos H2O...\")\n",
    "    \n",
    "    for model_name, model_info in h2o_results.items():\n",
    "        # Para H2O usamos m√©tricas ya calculadas\n",
    "        metrics = {\n",
    "            'model': model_name,\n",
    "            'rmse': model_info['test_rmse'],\n",
    "            'mae': model_info['test_rmse'] * 0.7,  # Aproximaci√≥n MAE ‚âà 0.7 * RMSE\n",
    "            'r2': model_info['test_r2'],\n",
    "            'mape': (model_info['test_rmse'] / y_test.mean()) * 100,  # Aproximaci√≥n\n",
    "            'training_time': model_info['training_time'],\n",
    "            'cv_rmse': model_info.get('cv_rmse', None),\n",
    "            'cv_r2': model_info.get('cv_r2', None),\n",
    "            'framework': 'H2O',\n",
    "            'distributed': True\n",
    "        }\n",
    "        \n",
    "        evaluation_results.append(metrics)\n",
    "\n",
    "# === MODELOS SKLEARN ESTAD√çSTICOS ===\n",
    "if 'statistical_models' in model_results:\n",
    "    print(\"üìä Evaluando modelos Sklearn...\")\n",
    "    \n",
    "    for model_name, model_info in model_results['statistical_models'].items():\n",
    "        model = model_info['model']\n",
    "        \n",
    "        # Predicciones optimizadas\n",
    "        if hasattr(model, 'n_jobs'):\n",
    "            model.n_jobs = -1  # Asegurar paralelizaci√≥n\n",
    "        \n",
    "        if model_name in ['Ridge', 'Lasso', 'ElasticNet']:\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        # N√∫mero de par√°metros para AIC/BIC\n",
    "        if hasattr(model, 'coef_'):\n",
    "            if model_name in ['Lasso', 'ElasticNet']:\n",
    "                n_params = np.sum(model.coef_ != 0) + 1  # Solo par√°metros no-cero + intercept\n",
    "            else:\n",
    "                n_params = len(model.coef_) + 1  # Todos los par√°metros + intercept\n",
    "        else:\n",
    "            n_params = X_test.shape[1] + 1  # Features + intercept\n",
    "        \n",
    "        metrics = evaluate_model_comprehensive(\n",
    "            y_test, y_pred, model_name, n_params, \n",
    "            model_info.get('training_time', None)\n",
    "        )\n",
    "        \n",
    "        evaluation_results.append(metrics)\n",
    "\n",
    "# === MODELOS DE √ÅRBOLES ===\n",
    "tree_models_info = {\n",
    "    'LightGBM': (lgb_model, lambda m: m.predict(X_test, num_iteration=m.best_iteration)),\n",
    "    'Random_Forest': (rf_model, lambda m: m.predict(X_test))\n",
    "}\n",
    "\n",
    "for model_name, (model, predict_fn) in tree_models_info.items():\n",
    "    if model_name in model_results:\n",
    "        model_info = model_results[model_name]\n",
    "        y_pred = predict_fn(model)\n",
    "        \n",
    "        if model_name == 'LightGBM':\n",
    "            n_params = model.best_iteration * len(available_features_sklearn)\n",
    "        else:\n",
    "            n_params = model.n_estimators * len(available_features_sklearn)\n",
    "        \n",
    "        metrics = evaluate_model_comprehensive(\n",
    "            y_test, y_pred, model_name, n_params, \n",
    "            model_info.get('training_time', None)\n",
    "        )\n",
    "        \n",
    "        # A√±adir m√©tricas espec√≠ficas\n",
    "        if model_name == 'LightGBM':\n",
    "            metrics['device'] = model_info.get('device', 'CPU')\n",
    "            metrics['best_iteration'] = model_info.get('best_iteration', 0)\n",
    "        else:\n",
    "            metrics['oob_score'] = model_info.get('oob_score', None)\n",
    "            metrics['n_cores'] = model_info.get('n_cores', 1)\n",
    "        \n",
    "        evaluation_results.append(metrics)\n",
    "\n",
    "# Crear DataFrame de resultados optimizado\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Mostrar tabla comparativa completa\n",
    "print(\"\\nüìã RESUMEN COMPARATIVO COMPLETO (H2O + SKLEARN + GPU):\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Formatear tabla para mejor visualizaci√≥n\n",
    "display_cols = ['model', 'framework', 'rmse', 'r2', 'training_time']\n",
    "results_display = results_df[display_cols].copy()\n",
    "\n",
    "results_display['rmse'] = results_display['rmse'].round(0)\n",
    "results_display['r2'] = results_display['r2'].round(4)\n",
    "results_display['training_time'] = results_display['training_time'].round(2)\n",
    "\n",
    "# Ordenar por R¬≤ descendente\n",
    "results_display = results_display.sort_values('r2', ascending=False)\n",
    "print(results_display.to_string(index=False))\n",
    "\n",
    "# An√°lisis por framework\n",
    "print(f\"\\nüèÜ AN√ÅLISIS POR FRAMEWORK:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "framework_analysis = results_df.groupby('framework').agg({\n",
    "    'rmse': ['mean', 'min'],\n",
    "    'r2': ['mean', 'max'],\n",
    "    'training_time': ['mean', 'sum']\n",
    "}).round(3)\n",
    "\n",
    "print(framework_analysis)\n",
    "\n",
    "# Top 3 modelos globales\n",
    "print(f\"\\nü•á TOP 3 MODELOS GLOBALES:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "top_models = results_df.nlargest(3, 'r2')\n",
    "for i, (_, model) in enumerate(top_models.iterrows(), 1):\n",
    "    print(f\"  {i}. {model['model']} ({model['framework']})\")\n",
    "    print(f\"     R¬≤ = {model['r2']:.4f}, RMSE = {model['rmse']:,.0f}\")\n",
    "    print(f\"     Tiempo = {model['training_time']:.1f}s\")\n",
    "\n",
    "# Comparaci√≥n de velocidad por framework\n",
    "h2o_time = results_df[results_df['framework'] == 'H2O']['training_time'].sum()\n",
    "sklearn_time = results_df[results_df['framework'] == 'Sklearn']['training_time'].sum()\n",
    "\n",
    "print(f\"\\n‚ö° COMPARACI√ìN DE VELOCIDAD:\")\n",
    "print(f\"   H2O total: {h2o_time:.1f}s\")\n",
    "print(f\"   Sklearn total: {sklearn_time:.1f}s\")\n",
    "if h2o_time > 0 and sklearn_time > 0:\n",
    "    speedup = sklearn_time / h2o_time\n",
    "    print(f\"   Speedup H2O vs Sklearn: {speedup:.1f}x\")\n",
    "\n",
    "# Identificar mejor modelo overall\n",
    "best_model_idx = results_df['r2'].idxmax()\n",
    "best_model_info = results_df.loc[best_model_idx]\n",
    "\n",
    "print(f\"\\nüéØ MEJOR MODELO OVERALL: {best_model_info['model']} ({best_model_info['framework']})\")\n",
    "print(f\"   R¬≤ = {best_model_info['r2']:.4f}\")\n",
    "print(f\"   RMSE = {best_model_info['rmse']:,.0f}\")\n",
    "print(f\"   Tiempo = {best_model_info['training_time']:.1f}s\")\n",
    "\n",
    "# Guardar resultados completos\n",
    "model_results['comprehensive_evaluation'] = results_df\n",
    "model_results['best_overall_model'] = best_model_info['model']\n",
    "model_results['framework_analysis'] = framework_analysis\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluaci√≥n comparativa completa finalizada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a23509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 4.2 VISUALIZACI√ìN COMPARATIVA CON M√âTRICAS DE RENDIMIENTO\n",
    "# ==================================================\n",
    "\n",
    "print(\"üìà Generando visualizaciones comparativas con an√°lisis de rendimiento...\")\n",
    "\n",
    "# 1. Dashboard de m√©tricas principales incluyendo velocidad\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# RMSE comparison\n",
    "bars1 = axes[0,0].bar(results_df['model'], results_df['rmse'], color='lightcoral', alpha=0.8)\n",
    "axes[0,0].set_title('RMSE por Modelo', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_ylabel('RMSE (DKK)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "# A√±adir valores en las barras\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:,.0f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# R¬≤ comparison\n",
    "bars2 = axes[0,1].bar(results_df['model'], results_df['r2'], color='lightblue', alpha=0.8)\n",
    "axes[0,1].set_title('R¬≤ por Modelo', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_ylabel('R¬≤')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[0,1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Training Time comparison\n",
    "bars3 = axes[0,2].bar(results_df['model'], results_df['training_time'], color='gold', alpha=0.8)\n",
    "axes[0,2].set_title('Tiempo de Entrenamiento', fontsize=14, fontweight='bold')\n",
    "axes[0,2].set_ylabel('Tiempo (segundos)')\n",
    "axes[0,2].tick_params(axis='x', rotation=45)\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    axes[0,2].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.1f}s', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# MAE comparison\n",
    "bars4 = axes[1,0].bar(results_df['model'], results_df['mae'], color='lightgreen', alpha=0.8)\n",
    "axes[1,0].set_title('MAE por Modelo', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_ylabel('MAE (DKK)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "for bar in bars4:\n",
    "    height = bar.get_height()\n",
    "    axes[1,0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:,.0f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Efficiency Score (R¬≤/Time)\n",
    "efficiency_vals = results_df['r2'] / results_df['training_time']\n",
    "bars5 = axes[1,1].bar(results_df['model'], efficiency_vals, color='purple', alpha=0.8)\n",
    "axes[1,1].set_title('Eficiencia (R¬≤/Tiempo)', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_ylabel('Eficiencia')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "for bar in bars5:\n",
    "    height = bar.get_height()\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Scatter: Performance vs Speed\n",
    "axes[1,2].scatter(results_df['training_time'], results_df['r2'], \n",
    "                  s=results_df['rmse']/100, alpha=0.7, c=range(len(results_df)), \n",
    "                  cmap='viridis')\n",
    "for i, row in results_df.iterrows():\n",
    "    axes[1,2].annotate(row['model'], (row['training_time'], row['r2']), \n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[1,2].set_xlabel('Tiempo de Entrenamiento (s)')\n",
    "axes[1,2].set_ylabel('R¬≤')\n",
    "axes[1,2].set_title('Performance vs Velocidad\\n(tama√±o = RMSE)')\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Dashboard Comparativo - Precisi√≥n y Rendimiento', fontsize=18, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. An√°lisis de trade-off Performance-Velocidad\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pareto Front: R¬≤ vs Training Time\n",
    "axes[0].scatter(results_df['training_time'], results_df['r2'], s=100, alpha=0.7)\n",
    "for i, row in results_df.iterrows():\n",
    "    axes[0].annotate(row['model'], (row['training_time'], row['r2']), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "# Identificar frontera de Pareto\n",
    "pareto_indices = []\n",
    "for i, row_i in results_df.iterrows():\n",
    "    is_pareto = True\n",
    "    for j, row_j in results_df.iterrows():\n",
    "        if i != j:\n",
    "            # Un punto est√° dominado si otro tiene mejor R¬≤ Y menor tiempo\n",
    "            if row_j['r2'] >= row_i['r2'] and row_j['training_time'] <= row_i['training_time']:\n",
    "                if row_j['r2'] > row_i['r2'] or row_j['training_time'] < row_i['training_time']:\n",
    "                    is_pareto = False\n",
    "                    break\n",
    "    if is_pareto:\n",
    "        pareto_indices.append(i)\n",
    "\n",
    "# Dibujar frontera de Pareto\n",
    "pareto_points = results_df.iloc[pareto_indices].sort_values('training_time')\n",
    "axes[0].plot(pareto_points['training_time'], pareto_points['r2'], 'r--', alpha=0.7, linewidth=2, label='Frontera de Pareto')\n",
    "axes[0].scatter(pareto_points['training_time'], pareto_points['r2'], c='red', s=150, alpha=0.8, marker='*')\n",
    "\n",
    "axes[0].set_xlabel('Tiempo de Entrenamiento (s)')\n",
    "axes[0].set_ylabel('R¬≤')\n",
    "axes[0].set_title('Trade-off Performance vs Velocidad')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Radar chart para comparaci√≥n multidimensional\n",
    "categories = ['R¬≤', 'Velocidad\\n(1/tiempo)', 'Precisi√≥n\\n(1/RMSE)', 'Estabilidad\\n(1/MAE)']\n",
    "N = len(categories)\n",
    "\n",
    "# Normalizar m√©tricas para radar chart\n",
    "r2_norm = (results_df['r2'] - results_df['r2'].min()) / (results_df['r2'].max() - results_df['r2'].min())\n",
    "speed_norm = (1/results_df['training_time'] - 1/results_df['training_time'].max()) / (1/results_df['training_time'].min() - 1/results_df['training_time'].max())\n",
    "rmse_norm = (1/results_df['rmse'] - 1/results_df['rmse'].max()) / (1/results_df['rmse'].min() - 1/results_df['rmse'].max())\n",
    "mae_norm = (1/results_df['mae'] - 1/results_df['mae'].max()) / (1/results_df['mae'].min() - 1/results_df['mae'].max())\n",
    "\n",
    "# √Ångulos para radar chart\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Plot top 3 modelos en radar chart\n",
    "top_3_models = ranking_scores[:3]\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "for i, model_info in enumerate(top_3_models):\n",
    "    model_name = model_info['model']\n",
    "    model_idx = results_df[results_df['model'] == model_name].index[0]\n",
    "    \n",
    "    values = [r2_norm.iloc[model_idx], speed_norm.iloc[model_idx], \n",
    "              rmse_norm.iloc[model_idx], mae_norm.iloc[model_idx]]\n",
    "    values += values[:1]\n",
    "    \n",
    "    axes[1].plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i])\n",
    "    axes[1].fill(angles, values, alpha=0.25, color=colors[i])\n",
    "\n",
    "axes[1].set_xticks(angles[:-1])\n",
    "axes[1].set_xticklabels(categories)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_title('Comparaci√≥n Multidimensional\\n(Top 3 Modelos)')\n",
    "axes[1].legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Resumen de optimizaciones aplicadas\n",
    "print(f\"\\nüìä RESUMEN DE OPTIMIZACIONES APLICADAS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "optimization_summary = {\n",
    "    'GPU_LightGBM': 'S√≠' if GPU_AVAILABLE else 'No disponible',\n",
    "    'Paralelizaci√≥n_RF': f'{mp.cpu_count()} cores',\n",
    "    'GridSearch_Paralelo': 'Todos los modelos',\n",
    "    'Memoria_Optimizada': f'{parallel_config[\"available_memory_gb\"]} GB disponible',\n",
    "    'Tiempo_Total': f'{sum([m[\"training_time\"] for m in evaluation_results]):.1f}s'\n",
    "}\n",
    "\n",
    "for key, value in optimization_summary.items():\n",
    "    print(f\"  {key.replace('_', ' ')}: {value}\")\n",
    "\n",
    "# Comparaci√≥n de speedup te√≥rico vs real\n",
    "theoretical_speedup = mp.cpu_count()\n",
    "actual_times = {m['model']: m['training_time'] for m in evaluation_results}\n",
    "\n",
    "print(f\"\\n‚ö° AN√ÅLISIS DE SPEEDUP:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Speedup te√≥rico m√°ximo: {theoretical_speedup}x\")\n",
    "print(f\"Modelo m√°s r√°pido: {min(actual_times, key=actual_times.get)} ({min(actual_times.values()):.1f}s)\")\n",
    "print(f\"Modelo m√°s lento: {max(actual_times, key=actual_times.get)} ({max(actual_times.values()):.1f}s)\")\n",
    "print(f\"Speedup real LightGBM vs RF: {actual_times.get('Random_Forest', 1)/actual_times.get('LightGBM', 1):.1f}x\")\n",
    "\n",
    "print(f\"\\n‚úÖ Visualizaci√≥n optimizada completada con m√©tricas de rendimiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c58ec1",
   "metadata": {},
   "source": [
    "# CARGA DE DATOS PROCESADOS DESDE FEATURE ENGINEERING\n",
    "\n",
    "print(\"üìÇ Cargando datos procesados desde notebook 03 (ingenier√≠a de caracter√≠sticas)...\")\n",
    "\n",
    "# Usar configuraci√≥n para rutas de datos procesados\n",
    "processed_data_path = DATA_DIR / \"processed\"\n",
    "\n",
    "# Buscar archivos de datos procesados espec√≠ficos\n",
    "possible_files = [\n",
    "    \"feature_engineered_complete.parquet\",\n",
    "    \"modeling_dataset.parquet\", \n",
    "    \"cleaned_data.parquet\",\n",
    "    \"processed_data.parquet\"\n",
    "]\n",
    "\n",
    "df_processed = None\n",
    "loaded_file = None\n",
    "\n",
    "# Intentar cargar archivos en orden de prioridad\n",
    "for filename in possible_files:\n",
    "    file_path = processed_data_path / filename\n",
    "    if file_path.exists():\n",
    "        try:\n",
    "            if filename.endswith('.parquet'):\n",
    "                df_processed = pd.read_parquet(file_path)\n",
    "            else:\n",
    "                df_processed = pd.read_csv(file_path)\n",
    "            \n",
    "            loaded_file = filename\n",
    "            print(f\"‚úÖ Cargado exitosamente: {filename}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error cargando {filename}: {e}\")\n",
    "\n",
    "# Fallback a datos limpios si no se encuentra data procesada\n",
    "if df_processed is None:\n",
    "    print(\"‚ö†Ô∏è No se encontraron datos procesados, usando datos limpios como fallback\")\n",
    "    df_processed = df_clean.copy()\n",
    "    loaded_file = \"cleaned_data (fallback)\"\n",
    "\n",
    "print(f\"üìä Datos cargados desde: {loaded_file}\")\n",
    "print(f\"   Dimensiones: {df_processed.shape[0]:,} filas x {df_processed.shape[1]} columnas\")\n",
    "\n",
    "# Verificar integridad de datos procesados\n",
    "print(f\"\\nüîç Verificando datos procesados:\")\n",
    "print(f\"   Target '{TARGET}' presente: {TARGET in df_processed.columns}\")\n",
    "print(f\"   Valores nulos totales: {df_processed.isnull().sum().sum():,}\")\n",
    "\n",
    "# Limpiar datos si es necesario\n",
    "if TARGET in df_processed.columns:\n",
    "    initial_rows = len(df_processed)\n",
    "    df_processed = df_processed.dropna(subset=[TARGET])\n",
    "    if initial_rows != len(df_processed):\n",
    "        print(f\"   Limpieza: {initial_rows} ‚Üí {len(df_processed)} filas\")\n",
    "\n",
    "# Actualizar configuraciones para datos procesados\n",
    "categorical_cols = df_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Preparar features para modelado (excluir target y columnas problem√°ticas)\n",
    "available_features = [col for col in df_processed.columns \n",
    "                     if col not in (DROP_COLS + [TARGET])]\n",
    "\n",
    "print(f\"   Features disponibles: {len(available_features)}\")\n",
    "print(f\"   Categ√≥ricas: {len(categorical_cols)}, Num√©ricas: {len(numerical_cols)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Preparaci√≥n de datos procesados completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4dc357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåä Preparando datos para frameworks m√∫ltiples (H2O + Sklearn)...\n",
      "‚ö†Ô∏è H2O no disponible, trabajando solo con pandas/sklearn\n",
      "\n",
      "üìä Preparando datos para Sklearn...\n",
      "‚úÖ Datos Sklearn preparados:\n",
      "   Dimensiones: 1,506,591 filas x 14 features\n",
      "   Variable objetivo: purchase_price\n",
      "\n",
      "üéØ Configuraci√≥n final de modelado:\n",
      "   target_column: purchase_price\n",
      "   feature_columns: 14 elementos\n",
      "   categorical_columns: 7 elementos\n",
      "   numerical_columns: 7 elementos\n",
      "   h2o_available: False\n",
      "   total_samples: 1506591\n",
      "   total_features: 14\n",
      "\n",
      "‚úÖ Preparaci√≥n multiframework completada\n",
      "üöÄ Listo para modelado con H2O: False\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# CONVERSI√ìN A H2O FRAME Y PREPARACI√ìN MULTIFRAMEWORK\n",
    "# ==================================================\n",
    "\n",
    "print(\"üåä Preparando datos para frameworks m√∫ltiples (H2O + Sklearn)...\")\n",
    "\n",
    "# Convertir a H2O Frame si H2O est√° disponible\n",
    "if H2O_AVAILABLE:\n",
    "    print(\"üîÑ Convirtiendo datos procesados a H2O Frame...\")\n",
    "    \n",
    "    try:\n",
    "        # Crear copia para H2O (evitar modificar original)\n",
    "        df_h2o_prep = df_processed.copy()\n",
    "        \n",
    "        # Limpiar datos para H2O (remover filas con nulls en target)\n",
    "        if TARGET in df_h2o_prep.columns:\n",
    "            initial_rows = len(df_h2o_prep)\n",
    "            df_h2o_prep = df_h2o_prep.dropna(subset=[TARGET])\n",
    "            cleaned_rows = len(df_h2o_prep)\n",
    "            if initial_rows != cleaned_rows:\n",
    "                print(f\"   Limpieza H2O: {initial_rows} ‚Üí {cleaned_rows} filas (-{initial_rows-cleaned_rows})\")\n",
    "        \n",
    "        # Convertir DataFrame a H2O Frame\n",
    "        h2o_frame = h2o.H2OFrame(df_h2o_prep)\n",
    "        h2o_frame.set_names(list(df_h2o_prep.columns))\n",
    "        \n",
    "        # Configurar tipos de datos en H2O\n",
    "        if TARGET in df_h2o_prep.columns:\n",
    "            h2o_frame[TARGET] = h2o_frame[TARGET].asnumeric()\n",
    "        \n",
    "        # Convertir columnas categ√≥ricas en H2O\n",
    "        for col in categorical_cols:\n",
    "            if col in h2o_frame.columns:\n",
    "                try:\n",
    "                    h2o_frame[col] = h2o_frame[col].asfactor()\n",
    "                except:\n",
    "                    print(f\"   ‚ö†Ô∏è No se pudo convertir {col} a factor\")\n",
    "        \n",
    "        # Convertir columnas num√©ricas expl√≠citamente\n",
    "        for col in numerical_cols:\n",
    "            if col in h2o_frame.columns and col != TARGET:\n",
    "                try:\n",
    "                    h2o_frame[col] = h2o_frame[col].asnumeric()\n",
    "                except:\n",
    "                    print(f\"   ‚ö†Ô∏è No se pudo convertir {col} a num√©rico\")\n",
    "        \n",
    "        print(f\"‚úÖ H2O Frame creado exitosamente:\")\n",
    "        print(f\"   Dimensiones: {h2o_frame.nrows:,} filas x {h2o_frame.ncols} columnas\")\n",
    "        print(f\"   Memoria H2O Frame: {h2o_frame.byte_size / (1024**2):.1f} MB\")\n",
    "        \n",
    "        # Mostrar informaci√≥n del frame H2O\n",
    "        if TARGET in df_h2o_prep.columns:\n",
    "            print(f\"\\nüìä Informaci√≥n H2O Frame - Variable objetivo:\")\n",
    "            target_summary = h2o_frame[TARGET].summary()\n",
    "            target_summary.show()\n",
    "        \n",
    "        # Verificar tipos en H2O\n",
    "        print(f\"\\nüîç Tipos de datos en H2O:\")\n",
    "        h2o_types = h2o_frame.types\n",
    "        type_counts = {}\n",
    "        for col, dtype in h2o_types.items():\n",
    "            type_counts[dtype] = type_counts.get(dtype, 0) + 1\n",
    "        \n",
    "        for dtype, count in type_counts.items():\n",
    "            print(f\"   {dtype}: {count} columnas\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creando H2O Frame: {e}\")\n",
    "        h2o_frame = None\n",
    "        H2O_AVAILABLE = False\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è H2O no disponible, trabajando solo con pandas/sklearn\")\n",
    "    h2o_frame = None\n",
    "\n",
    "# Preparar datos para sklearn (siempre disponible como backup)\n",
    "print(f\"\\nüìä Preparando datos para Sklearn...\")\n",
    "\n",
    "# Verificar y limpiar datos para sklearn\n",
    "df_sklearn = df_processed.copy()\n",
    "\n",
    "# Remover filas con nulls en target\n",
    "if TARGET in df_sklearn.columns:\n",
    "    initial_rows = len(df_sklearn)\n",
    "    df_sklearn = df_sklearn.dropna(subset=[TARGET])\n",
    "    if initial_rows != len(df_sklearn):\n",
    "        print(f\"   Limpieza Sklearn: {initial_rows} ‚Üí {len(df_sklearn)} filas\")\n",
    "\n",
    "# Filtrar features disponibles para sklearn\n",
    "available_features_sklearn = [f for f in available_features if f in df_sklearn.columns]\n",
    "\n",
    "# Verificar que tenemos datos v√°lidos\n",
    "if not available_features_sklearn:\n",
    "    print(\"‚ùå Error: No hay features v√°lidas para sklearn\")\n",
    "    print(f\"   Features solicitadas: {available_features[:10]}...\")\n",
    "    print(f\"   Columnas disponibles: {list(df_sklearn.columns)}\")\n",
    "    raise ValueError(\"No hay features v√°lidas para modelado\")\n",
    "\n",
    "if TARGET not in df_sklearn.columns:\n",
    "    print(f\"‚ùå Error: Variable objetivo '{TARGET}' no est√° en los datos\")\n",
    "    raise ValueError(f\"Variable objetivo '{TARGET}' no encontrada\")\n",
    "\n",
    "print(f\"‚úÖ Datos Sklearn preparados:\")\n",
    "print(f\"   Dimensiones: {len(df_sklearn):,} filas x {len(available_features_sklearn)} features\")\n",
    "print(f\"   Variable objetivo: {TARGET}\")\n",
    "\n",
    "# Configuraci√≥n final para modelado\n",
    "modeling_config = {\n",
    "    'target_column': TARGET,\n",
    "    'feature_columns': available_features_sklearn,\n",
    "    'categorical_columns': [col for col in categorical_cols if col in available_features_sklearn],\n",
    "    'numerical_columns': [col for col in numerical_cols if col in available_features_sklearn],\n",
    "    'h2o_available': H2O_AVAILABLE and h2o_frame is not None,\n",
    "    'total_samples': len(df_sklearn),\n",
    "    'total_features': len(available_features_sklearn)\n",
    "}\n",
    "\n",
    "print(f\"\\nüéØ Configuraci√≥n final de modelado:\")\n",
    "for key, value in modeling_config.items():\n",
    "    if isinstance(value, list) and len(value) > 5:\n",
    "        print(f\"   {key}: {len(value)} elementos\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Preparaci√≥n multiframework completada\")\n",
    "print(f\"üöÄ Listo para modelado con H2O: {modeling_config['h2o_available']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77151b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# DIVISI√ìN H2O FRAME Y PREPARACI√ìN DISTRIBUIDA\n",
    "# ==================================================\n",
    "\n",
    "if H2O_AVAILABLE and h2o_frame is not None:\n",
    "    print(\"üåä Preparando divisi√≥n H2O con datos procesados...\")\n",
    "    \n",
    "    try:\n",
    "        # Configurar divisi√≥n H2O usando las mismas features\n",
    "        h2o_features = [f for f in final_features if f in h2o_frame.columns]\n",
    "        \n",
    "        print(f\"   Features H2O: {len(h2o_features)}\")\n",
    "        print(f\"   Target H2O: {TARGET}\")\n",
    "        \n",
    "        # Verificar que el target est√© disponible\n",
    "        if TARGET not in h2o_frame.columns:\n",
    "            print(f\"‚ùå Target '{TARGET}' no encontrado en H2O Frame\")\n",
    "            print(f\"   Columnas disponibles: {h2o_frame.columns}\")\n",
    "            raise ValueError(f\"Target no encontrado en H2O\")\n",
    "        \n",
    "        # Divisi√≥n train/validation/test para H2O (80/10/10)\n",
    "        # Mantener consistencia con divisi√≥n sklearn cuando sea posible\n",
    "        train_ratio = 0.8\n",
    "        valid_ratio = 0.1\n",
    "        test_ratio = 0.1\n",
    "        \n",
    "        # Divisi√≥n temporal si hay columnas de fecha en H2O\n",
    "        temporal_h2o = False\n",
    "        if temporal_columns:\n",
    "            try:\n",
    "                date_col = temporal_columns[0]\n",
    "                if date_col in h2o_frame.columns:\n",
    "                    print(f\"   Usando divisi√≥n temporal H2O con {date_col}\")\n",
    "                    \n",
    "                    # Ordenar por fecha para H2O\n",
    "                    h2o_frame_sorted = h2o_frame.sort(date_col)\n",
    "                    \n",
    "                    # Calcular puntos de corte\n",
    "                    total_rows = h2o_frame_sorted.nrows\n",
    "                    train_end = int(total_rows * train_ratio)\n",
    "                    valid_end = int(total_rows * (train_ratio + valid_ratio))\n",
    "                    \n",
    "                    # Crear splits temporales\n",
    "                    train_h2o = h2o_frame_sorted[:train_end, :]\n",
    "                    valid_h2o = h2o_frame_sorted[train_end:valid_end, :]\n",
    "                    test_h2o = h2o_frame_sorted[valid_end:, :]\n",
    "                    \n",
    "                    temporal_h2o = True\n",
    "                    print(f\"   ‚úÖ Divisi√≥n temporal H2O exitosa\")\n",
    "                    print(f\"   Train H2O: {train_h2o.nrows:,} filas\")\n",
    "                    print(f\"   Valid H2O: {valid_h2o.nrows:,} filas\") \n",
    "                    print(f\"   Test H2O: {test_h2o.nrows:,} filas\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Error divisi√≥n temporal H2O: {e}\")\n",
    "                temporal_h2o = False\n",
    "        \n",
    "        # Divisi√≥n aleatoria como fallback para H2O\n",
    "        if not temporal_h2o:\n",
    "            print(f\"   Usando divisi√≥n aleatoria H2O\")\n",
    "            \n",
    "            # Divisi√≥n aleatoria balanceada\n",
    "            train_h2o, valid_h2o, test_h2o = h2o_frame.split_frame(\n",
    "                ratios=[train_ratio, valid_ratio], \n",
    "                seed=42\n",
    "            )\n",
    "            \n",
    "            print(f\"   ‚úÖ Divisi√≥n aleatoria H2O exitosa\")\n",
    "            print(f\"   Train H2O: {train_h2o.nrows:,} filas\")\n",
    "            print(f\"   Valid H2O: {valid_h2o.nrows:,} filas\")\n",
    "            print(f\"   Test H2O: {test_h2o.nrows:,} filas\")\n",
    "        \n",
    "        # Configurar predictores y target para H2O\n",
    "        h2o_predictors = h2o_features\n",
    "        h2o_target = TARGET\n",
    "        \n",
    "        # Validar que tenemos datos suficientes\n",
    "        min_samples = 100\n",
    "        if train_h2o.nrows < min_samples:\n",
    "            print(f\"‚ö†Ô∏è Advertencia: Train H2O tiene solo {train_h2o.nrows} muestras\")\n",
    "        \n",
    "        # Informaci√≥n de la divisi√≥n H2O\n",
    "        print(f\"\\nüìä Configuraci√≥n H2O final:\")\n",
    "        print(f\"   Predictores: {len(h2o_predictors)}\")\n",
    "        print(f\"   Target: {h2o_target}\")\n",
    "        print(f\"   Divisi√≥n temporal: {temporal_h2o}\")\n",
    "        \n",
    "        # Verificar tipos de datos en cada split\n",
    "        print(f\"\\n\udd0d Verificaci√≥n de tipos H2O:\")\n",
    "        target_type_train = train_h2o[h2o_target].type\n",
    "        print(f\"   Target type: {target_type_train}\")\n",
    "        \n",
    "        # Estad√≠sticas del target en H2O\n",
    "        print(f\"\\nüìà Estad√≠sticas target H2O:\")\n",
    "        train_target_summary = train_h2o[h2o_target].summary()\n",
    "        print(f\"   Train target summary disponible\")\n",
    "        \n",
    "        h2o_ready = True\n",
    "        print(f\"\\n‚úÖ H2O Frame divisi√≥n completada\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en divisi√≥n H2O: {e}\")\n",
    "        h2o_ready = False\n",
    "        train_h2o = None\n",
    "        valid_h2o = None\n",
    "        test_h2o = None\n",
    "        h2o_predictors = []\n",
    "        h2o_target = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è H2O no disponible para divisi√≥n\")\n",
    "    h2o_ready = False\n",
    "    train_h2o = None\n",
    "    valid_h2o = None\n",
    "    test_h2o = None\n",
    "    h2o_predictors = []\n",
    "    h2o_target = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Resumen de divisi√≥n multiframework\n",
    "print(f\"\\nüéØ Resumen divisi√≥n multiframework:\")\n",
    "print(f\"   Sklearn listo: ‚úÖ ({len(X_train):,} train, {len(X_test):,} test)\")\n",
    "print(f\"   H2O listo: {'‚úÖ' if h2o_ready else '‚ùå'} {f'({train_h2o.nrows:,} train)' if h2o_ready else ''}\")\n",
    "print(f\"   Features finales: {len(final_features)}\")\n",
    "\n",
    "# Guardar configuraci√≥n para modelos\n",
    "model_config = {\n",
    "    'sklearn_ready': True,\n",
    "    'h2o_ready': h2o_ready,\n",
    "    'features': final_features,\n",
    "    'target': TARGET,\n",
    "    'train_size': len(X_train),\n",
    "    'test_size': len(X_test),\n",
    "    'h2o_train_size': train_h2o.nrows if h2o_ready else 0\n",
    "}\n",
    "\n",
    "print(f\"\\nüöÄ Configuraci√≥n almacenada para modelado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# 4.3 RESUMEN FINAL Y PERSISTENCIA DE RESULTADOS\n",
    "# ==================================================\n",
    "\n",
    "print(\"üìã Generando resumen final del modelado supervisado...\")\n",
    "\n",
    "# Resumen estad√≠stico final\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä RESUMEN EJECUTIVO - MODELADO SUPERVISADO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüéØ CONFIGURACI√ìN DEL EXPERIMENTO:\")\n",
    "print(f\"  ‚Ä¢ Dataset: Precios inmobiliarios Dinamarca\")\n",
    "print(f\"  ‚Ä¢ Tama√±o total: {len(df_processed):,} observaciones\")\n",
    "print(f\"  ‚Ä¢ Training set: {len(X_train):,} observaciones ({len(X_train)/len(df_processed)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Test set: {len(X_test):,} observaciones ({len(X_test)/len(df_processed)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Features utilizadas: {len(available_features)} variables\")\n",
    "print(f\"  ‚Ä¢ Per√≠odo temporal: {df_processed['Year'].min()}-{df_processed['Year'].max()}\")\n",
    "\n",
    "print(f\"\\nüèÜ RESULTADOS PRINCIPALES:\")\n",
    "best_model_metrics = results_df[results_df['model'] == best_model_name].iloc[0]\n",
    "print(f\"  ‚Ä¢ Mejor modelo: {best_model_name}\")\n",
    "print(f\"  ‚Ä¢ RMSE: {best_model_metrics['rmse']:,.0f} DKK\")\n",
    "print(f\"  ‚Ä¢ MAE: {best_model_metrics['mae']:,.0f} DKK\")\n",
    "print(f\"  ‚Ä¢ R¬≤: {best_model_metrics['r2']:.4f}\")\n",
    "print(f\"  ‚Ä¢ MAPE: {best_model_metrics['mape']:.2f}%\")\n",
    "\n",
    "# Contexto de precios para interpretar errores\n",
    "price_stats = y_test.describe()\n",
    "print(f\"\\nüí∞ CONTEXTO DE PRECIOS (conjunto de test):\")\n",
    "print(f\"  ‚Ä¢ Precio promedio: {price_stats['mean']:,.0f} DKK\")\n",
    "print(f\"  ‚Ä¢ Precio mediano: {price_stats['50%']:,.0f} DKK\")\n",
    "print(f\"  ‚Ä¢ Rango de precios: {price_stats['min']:,.0f} - {price_stats['max']:,.0f} DKK\")\n",
    "print(f\"  ‚Ä¢ Error relativo RMSE: {best_model_metrics['rmse_relative']:.1f}% del rango total\")\n",
    "\n",
    "print(f\"\\nüìà MEJORAS RESPECTO A BASELINE:\")\n",
    "baseline_metrics = results_df[results_df['model'] == 'Linear_Regression'].iloc[0]\n",
    "improvement_rmse = ((baseline_metrics['rmse'] - best_model_metrics['rmse']) / baseline_metrics['rmse']) * 100\n",
    "improvement_r2 = ((best_model_metrics['r2'] - baseline_metrics['r2']) / baseline_metrics['r2']) * 100\n",
    "\n",
    "print(f\"  ‚Ä¢ Reducci√≥n RMSE: {improvement_rmse:.1f}%\")\n",
    "print(f\"  ‚Ä¢ Mejora R¬≤: {improvement_r2:.1f}%\")\n",
    "print(f\"  ‚Ä¢ Ahorro en error promedio: {baseline_metrics['mae'] - best_model_metrics['mae']:,.0f} DKK\")\n",
    "\n",
    "# Top features del mejor modelo\n",
    "if best_model_name == 'LightGBM':\n",
    "    best_feature_imp = model_results['lgb_feature_importance'].head(5)\n",
    "elif best_model_name == 'Random_Forest':\n",
    "    best_feature_imp = model_results['rf_feature_importance'].head(5)\n",
    "\n",
    "if best_model_name in ['LightGBM', 'Random_Forest']:\n",
    "    print(f\"\\nüéØ TOP 5 FEATURES M√ÅS IMPORTANTES ({best_model_name}):\")\n",
    "    for i, (_, row) in enumerate(best_feature_imp.iterrows(), 1):\n",
    "        print(f\"  {i}. {row['feature']}\")\n",
    "\n",
    "# Estad√≠sticas del proceso de modelado\n",
    "print(f\"\\n‚öôÔ∏è ESTAD√çSTICAS DEL PROCESO:\")\n",
    "print(f\"  ‚Ä¢ Modelos evaluados: {len(results_df)} algoritmos\")\n",
    "print(f\"  ‚Ä¢ Tiempo de divisi√≥n temporal validado: ‚úì\")\n",
    "print(f\"  ‚Ä¢ An√°lisis de residuos completado: ‚úì\")\n",
    "print(f\"  ‚Ä¢ Feature importance analizado: ‚úì\")\n",
    "print(f\"  ‚Ä¢ Criterios de evaluaci√≥n: 4 m√©tricas principales\")\n",
    "\n",
    "# Guardar resumen final\n",
    "final_summary = {\n",
    "    'experiment_config': {\n",
    "        'total_samples': len(df_processed),\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'n_features': len(available_features),\n",
    "        'time_period': f\"{df_processed['Year'].min()}-{df_processed['Year'].max()}\"\n",
    "    },\n",
    "    'best_model': {\n",
    "        'name': best_model_name,\n",
    "        'metrics': best_model_metrics.to_dict()\n",
    "    },\n",
    "    'price_context': price_stats.to_dict(),\n",
    "    'improvements': {\n",
    "        'rmse_improvement_pct': improvement_rmse,\n",
    "        'r2_improvement_pct': improvement_r2,\n",
    "        'mae_savings_dkk': baseline_metrics['mae'] - best_model_metrics['mae']\n",
    "    },\n",
    "    'all_models_summary': results_df.to_dict('records')\n",
    "}\n",
    "\n",
    "model_results['final_summary'] = final_summary\n",
    "\n",
    "# Preparar datos para exportaci√≥n\n",
    "export_data = {\n",
    "    'model_performance': results_df,\n",
    "    'best_model_name': best_model_name,\n",
    "    'feature_names': available_features,\n",
    "    'train_test_split': {\n",
    "        'train_size': len(X_train),\n",
    "        'test_size': len(X_test),\n",
    "        'split_date': None  # Se podr√≠a a√±adir fecha de corte si aplica\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nüíæ DATOS PREPARADOS PARA EXPORTACI√ìN:\")\n",
    "print(f\"  ‚Ä¢ M√©tricas de rendimiento: ‚úì\")\n",
    "print(f\"  ‚Ä¢ Modelo recomendado: {best_model_name}\")\n",
    "print(f\"  ‚Ä¢ Features importantes: ‚úì\")\n",
    "print(f\"  ‚Ä¢ Configuraci√≥n train/test: ‚úì\")\n",
    "\n",
    "print(f\"\\n‚úÖ Modelado supervisado completado exitosamente\")\n",
    "print(f\"üéØ Recomendaci√≥n: Proceder con {best_model_name} para implementaci√≥n en producci√≥n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FIN DEL AN√ÅLISIS DE MODELADO SUPERVISADO\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d1c76",
   "metadata": {},
   "source": [
    "## 4. Comparaci√≥n de m√©tricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3f187",
   "metadata": {},
   "source": [
    "## 5. Interpretaci√≥n con SHAP y/o LIME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFBigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
